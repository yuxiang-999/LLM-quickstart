{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc902319-2d68-4d6c-b53a-b9ba711e36d9",
   "metadata": {},
   "source": [
    "## Homework：DeepSpeed ZeRO-3 模式训练翻译模型（T5-3B）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2b9077-14c1-484e-b8d2-97d08e64edb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T13:37:20.816479Z",
     "iopub.status.busy": "2024-02-18T13:37:20.816027Z",
     "iopub.status.idle": "2024-02-18T13:37:20.956103Z",
     "shell.execute_reply": "2024-02-18T13:37:20.954700Z",
     "shell.execute_reply.started": "2024-02-18T13:37:20.816434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"fp16\": {\n",
      "        \"enabled\": \"auto\",\n",
      "        \"loss_scale\": 0,\n",
      "        \"loss_scale_window\": 1000,\n",
      "        \"initial_scale_power\": 16,\n",
      "        \"hysteresis\": 2,\n",
      "        \"min_loss_scale\": 1\n",
      "    },\n",
      "\n",
      "    \"bf16\": {\n",
      "        \"enabled\": \"auto\"\n",
      "    },\n",
      "\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\",\n",
      "        \"params\": {\n",
      "            \"lr\": \"auto\",\n",
      "            \"betas\": \"auto\",\n",
      "            \"eps\": \"auto\",\n",
      "            \"weight_decay\": \"auto\"\n",
      "        }\n",
      "    },\n",
      "\n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\",\n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": \"auto\",\n",
      "            \"warmup_max_lr\": \"auto\",\n",
      "            \"warmup_num_steps\": \"auto\"\n",
      "        }\n",
      "    },\n",
      "\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3,\n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\",\n",
      "            \"pin_memory\": true\n",
      "        },\n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\",\n",
      "            \"pin_memory\": true\n",
      "        },\n",
      "        \"overlap_comm\": true,\n",
      "        \"contiguous_gradients\": true,\n",
      "        \"sub_group_size\": 1e12,\n",
      "        \"reduce_bucket_size\": \"auto\",\n",
      "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
      "        \"stage3_param_persistence_threshold\": \"auto\",\n",
      "        \"stage3_max_live_parameters\": 1e12,\n",
      "        \"stage3_max_reuse_distance\": 1e12,\n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    },\n",
      "\n",
      "    \"gradient_accumulation_steps\": \"auto\",\n",
      "    \"gradient_clipping\": \"auto\",\n",
      "    \"steps_per_print\": 20,\n",
      "    \"train_batch_size\": \"auto\",\n",
      "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
      "    \"wall_clock_breakdown\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat ./config/homework-T5-3B-ds_config_zero3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc080b0-7164-489e-8b95-c5def471f214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T13:37:58.296505Z",
     "iopub.status.busy": "2024-02-18T13:37:58.296009Z",
     "iopub.status.idle": "2024-02-18T13:45:34.507465Z",
     "shell.execute_reply": "2024-02-18T13:45:34.505778Z",
     "shell.execute_reply.started": "2024-02-18T13:37:58.296450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-18 21:38:04,189] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-18 21:38:04,845] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-02-18 21:38:04,845] [INFO] [runner.py:568:main] cmd = /home/yuxiang/miniconda3/envs/py311_llm_deepspeed/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None translation/run_translation.py --deepspeed config/homework-T5-3B-ds_config_zero3.json --model_name_or_path t5-3b --per_device_train_batch_size 12 --per_device_eval_batch_size 12 --output_dir output_dir --overwrite_output_dir --do_train --do_eval --max_train_samples 500 --num_train_epochs 1 --dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro\n",
      "[2024-02-18 21:38:07,601] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-18 21:38:08,042] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2024-02-18 21:38:08,042] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2024-02-18 21:38:08,042] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2024-02-18 21:38:08,042] [INFO] [launch.py:163:main] dist_world_size=4\n",
      "[2024-02-18 21:38:08,042] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2024-02-18 21:38:08,043] [INFO] [launch.py:253:main] process 37613 spawned with command: ['/home/yuxiang/miniconda3/envs/py311_llm_deepspeed/bin/python', '-u', 'translation/run_translation.py', '--local_rank=0', '--deepspeed', 'config/homework-T5-3B-ds_config_zero3.json', '--model_name_or_path', 't5-3b', '--per_device_train_batch_size', '12', '--per_device_eval_batch_size', '12', '--output_dir', 'output_dir', '--overwrite_output_dir', '--do_train', '--do_eval', '--max_train_samples', '500', '--num_train_epochs', '1', '--dataset_name', 'wmt16', '--dataset_config', 'ro-en', '--source_lang', 'en', '--target_lang', 'ro']\n",
      "[2024-02-18 21:38:08,044] [INFO] [launch.py:253:main] process 37614 spawned with command: ['/home/yuxiang/miniconda3/envs/py311_llm_deepspeed/bin/python', '-u', 'translation/run_translation.py', '--local_rank=1', '--deepspeed', 'config/homework-T5-3B-ds_config_zero3.json', '--model_name_or_path', 't5-3b', '--per_device_train_batch_size', '12', '--per_device_eval_batch_size', '12', '--output_dir', 'output_dir', '--overwrite_output_dir', '--do_train', '--do_eval', '--max_train_samples', '500', '--num_train_epochs', '1', '--dataset_name', 'wmt16', '--dataset_config', 'ro-en', '--source_lang', 'en', '--target_lang', 'ro']\n",
      "[2024-02-18 21:38:08,044] [INFO] [launch.py:253:main] process 37615 spawned with command: ['/home/yuxiang/miniconda3/envs/py311_llm_deepspeed/bin/python', '-u', 'translation/run_translation.py', '--local_rank=2', '--deepspeed', 'config/homework-T5-3B-ds_config_zero3.json', '--model_name_or_path', 't5-3b', '--per_device_train_batch_size', '12', '--per_device_eval_batch_size', '12', '--output_dir', 'output_dir', '--overwrite_output_dir', '--do_train', '--do_eval', '--max_train_samples', '500', '--num_train_epochs', '1', '--dataset_name', 'wmt16', '--dataset_config', 'ro-en', '--source_lang', 'en', '--target_lang', 'ro']\n",
      "[2024-02-18 21:38:08,045] [INFO] [launch.py:253:main] process 37616 spawned with command: ['/home/yuxiang/miniconda3/envs/py311_llm_deepspeed/bin/python', '-u', 'translation/run_translation.py', '--local_rank=3', '--deepspeed', 'config/homework-T5-3B-ds_config_zero3.json', '--model_name_or_path', 't5-3b', '--per_device_train_batch_size', '12', '--per_device_eval_batch_size', '12', '--output_dir', 'output_dir', '--overwrite_output_dir', '--do_train', '--do_eval', '--max_train_samples', '500', '--num_train_epochs', '1', '--dataset_name', 'wmt16', '--dataset_config', 'ro-en', '--source_lang', 'en', '--target_lang', 'ro']\n",
      "[2024-02-18 21:38:12,650] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-18 21:38:12,726] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-18 21:38:12,749] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-18 21:38:12,749] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-18 21:38:12,885] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-02-18 21:38:12,885] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-02-18 21:38:12,931] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-02-18 21:38:12,957] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-02-18 21:38:12,958] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "02/18/2024 21:38:13 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False\n",
      "02/18/2024 21:38:13 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=config/homework-T5-3B-ds_config_zero3.json,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output_dir/runs/Feb18_21-38-11_llm-dev,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "output_dir=output_dir,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=12,\n",
      "per_device_train_batch_size=12,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output_dir,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/18/2024 21:38:13 - WARNING - __main__ - You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with `--source_prefix 'translate English to German: ' `\n",
      "02/18/2024 21:38:13 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, 16-bits training: False\n",
      "02/18/2024 21:38:13 - WARNING - __main__ - You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with `--source_prefix 'translate English to German: ' `\n",
      "02/18/2024 21:38:13 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, 16-bits training: False\n",
      "02/18/2024 21:38:13 - WARNING - __main__ - You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with `--source_prefix 'translate English to German: ' `\n",
      "02/18/2024 21:38:13 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False\n",
      "02/18/2024 21:38:13 - WARNING - __main__ - You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with `--source_prefix 'translate English to German: ' `\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_deepspeed/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for wmt16 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wmt16\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_deepspeed/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for wmt16 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wmt16\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_deepspeed/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for wmt16 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wmt16\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_deepspeed/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for wmt16 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wmt16\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Loading Dataset Infos from /home/yuxiang/.cache/huggingface/modules/datasets_modules/datasets/wmt16/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad\n",
      "02/18/2024 21:38:58 - INFO - datasets.info - Loading Dataset Infos from /home/yuxiang/.cache/huggingface/modules/datasets_modules/datasets/wmt16/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "02/18/2024 21:38:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/yuxiang/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad\n",
      "02/18/2024 21:38:58 - INFO - datasets.info - Loading Dataset info from /home/yuxiang/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad\n",
      "Found cached dataset wmt16 (/home/yuxiang/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad)\n",
      "02/18/2024 21:38:59 - INFO - datasets.builder - Found cached dataset wmt16 (/home/yuxiang/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad)\n",
      "Loading Dataset info from /home/yuxiang/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad\n",
      "02/18/2024 21:38:59 - INFO - datasets.info - Loading Dataset info from /home/yuxiang/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad\n",
      "[INFO|configuration_utils.py:728] 2024-02-18 21:38:59,526 >> loading configuration file config.json from cache at /home/yuxiang/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-18 21:38:59,534 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-3b\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 16384,\n",
      "  \"d_kv\": 128,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:610] 2024-02-18 21:39:00,017 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:728] 2024-02-18 21:39:00,507 >> loading configuration file config.json from cache at /home/yuxiang/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-18 21:39:00,509 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-3b\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 16384,\n",
      "  \"d_kv\": 128,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-02-18 21:39:01,695 >> loading file spiece.model from cache at /home/yuxiang/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-02-18 21:39:01,696 >> loading file tokenizer.json from cache at /home/yuxiang/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-02-18 21:39:01,696 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-02-18 21:39:01,696 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-02-18 21:39:01,696 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:728] 2024-02-18 21:39:01,696 >> loading configuration file config.json from cache at /home/yuxiang/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-18 21:39:01,698 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-3b\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 16384,\n",
      "  \"d_kv\": 128,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3257] 2024-02-18 21:39:01,827 >> loading weights file model.safetensors from cache at /home/yuxiang/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/model.safetensors\n",
      "[INFO|modeling_utils.py:3363] 2024-02-18 21:39:01,858 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "[INFO|configuration_utils.py:840] 2024-02-18 21:39:01,868 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[2024-02-18 21:39:10,361] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 510, num_elems = 2.88B\n",
      "[INFO|modeling_utils.py:3992] 2024-02-18 21:39:20,058 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4000] 2024-02-18 21:39:20,058 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-3b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|modeling_utils.py:3544] 2024-02-18 21:39:20,547 >> Generation config file not found, using a generation config created from the model config.\n",
      "[INFO|modeling_utils.py:1875] 2024-02-18 21:39:20,776 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32100. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "Loading cached processed dataset at /home/yuxiang/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad/cache-1e5dd422f4ebd4a8.arrow\n",
      "02/18/2024 21:39:20 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/yuxiang/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad/cache-1e5dd422f4ebd4a8.arrow\n",
      "Loading cached processed dataset at /home/yuxiang/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad/cache-ac3a6cb1749d71a0.arrow\n",
      "02/18/2024 21:39:20 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/yuxiang/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/f5dc442f4d1c2cc487cd2d5591af56c03a5f03bb98a3bb92151d015c8c9cb7ad/cache-ac3a6cb1749d71a0.arrow\n",
      "02/18/2024 21:39:34 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2024-02-18 21:39:34,891] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.3+a37e59b5, git-hash=a37e59b5, git-branch=master\n",
      "[2024-02-18 21:39:34,902] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-02-18 21:39:36,604] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2024-02-18 21:39:36,604] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-02-18 21:39:36,632] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-02-18 21:39:36,632] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-02-18 21:39:36,632] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-02-18 21:39:36,632] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 3 optimizer\n",
      "[2024-02-18 21:39:36,767] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-02-18 21:39:36,768] [INFO] [utils.py:801:see_memory_usage] MA 0.12 GB         Max_MA 0.37 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-02-18 21:39:36,768] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 20.11 GB, percent = 16.0%\n",
      "[2024-02-18 21:39:36,772] [INFO] [stage3.py:130:__init__] Reduce bucket size 1048576\n",
      "[2024-02-18 21:39:36,772] [INFO] [stage3.py:131:__init__] Prefetch bucket size 943718\n",
      "[2024-02-18 21:39:36,892] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-02-18 21:39:36,892] [INFO] [utils.py:801:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-02-18 21:39:36,893] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 20.11 GB, percent = 16.0%\n",
      "Parameter Offload: Total persistent parameters: 126976 in 124 params\n",
      "[2024-02-18 21:39:37,713] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-02-18 21:39:37,714] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.12 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-02-18 21:39:37,714] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.14 GB, percent = 16.8%\n",
      "[2024-02-18 21:39:37,848] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-02-18 21:39:37,849] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-02-18 21:39:37,849] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 21.14 GB, percent = 16.8%\n",
      "[2024-02-18 21:39:44,724] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2024-02-18 21:39:44,732] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-02-18 21:39:44,734] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 42.72 GB, percent = 34.0%\n",
      "[2024-02-18 21:39:45,172] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-02-18 21:39:45,173] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-02-18 21:39:45,173] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.39 GB, percent = 36.1%\n",
      "[2024-02-18 21:39:45,733] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions\n",
      "[2024-02-18 21:39:45,733] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-02-18 21:39:45,734] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 48.12 GB, percent = 38.3%\n",
      "[2024-02-18 21:39:46,157] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-02-18 21:39:46,157] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-02-18 21:39:46,158] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 55.79 GB, percent = 44.4%\n",
      "[2024-02-18 21:39:50,714] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-02-18 21:39:50,714] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.12 GB         Max_CA 0 GB \n",
      "[2024-02-18 21:39:50,715] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 64.7 GB, percent = 51.4%\n",
      "[2024-02-18 21:39:50,724] [INFO] [stage3.py:487:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2024-02-18 21:39:57,881] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-02-18 21:39:57,882] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.25 GB         CA 0.37 GB         Max_CA 0 GB \n",
      "[2024-02-18 21:39:57,882] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 80.86 GB, percent = 64.3%\n",
      "[2024-02-18 21:39:57,882] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
      "[2024-02-18 21:39:57,882] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2024-02-18 21:39:57,882] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f0a74efe610>\n",
      "[2024-02-18 21:39:57,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[[0.9, 0.999]]\n",
      "[2024-02-18 21:39:57,884] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0a7c343790>\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-02-18 21:39:57,885] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   loss_scale ................... 0\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   optimizer_name ............... adamw\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-02-18 21:39:57,886] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupLR\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-05, 'warmup_num_steps': 0}\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   train_batch_size ............. 48\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  12\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   world_size ................... 4\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=1048576 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=943718 param_persistence_threshold=10240 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000000 max_reuse_distance=1000000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3\n",
      "[2024-02-18 21:39:57,887] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 5e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.0\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 5e-05, \n",
      "            \"warmup_num_steps\": 0\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+12, \n",
      "        \"reduce_bucket_size\": 1.048576e+06, \n",
      "        \"stage3_prefetch_bucket_size\": 9.437184e+05, \n",
      "        \"stage3_param_persistence_threshold\": 1.024000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+12, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+12, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 48, \n",
      "    \"train_micro_batch_size_per_gpu\": 12, \n",
      "    \"wall_clock_breakdown\": false\n",
      "}\n",
      "[INFO|trainer.py:1794] 2024-02-18 21:39:57,887 >> ***** Running training *****\n",
      "[INFO|trainer.py:1795] 2024-02-18 21:39:57,888 >>   Num examples = 500\n",
      "[INFO|trainer.py:1796] 2024-02-18 21:39:57,888 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1797] 2024-02-18 21:39:57,888 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1800] 2024-02-18 21:39:57,888 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "[INFO|trainer.py:1801] 2024-02-18 21:39:57,888 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1802] 2024-02-18 21:39:57,888 >>   Total optimization steps = 11\n",
      "[INFO|trainer.py:1803] 2024-02-18 21:39:57,890 >>   Number of trainable parameters = 2,851,569,664\n",
      "100%|███████████████████████████████████████████| 11/11 [02:53<00:00, 14.21s/it][INFO|trainer.py:2040] 2024-02-18 21:42:51,416 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 173.5265, 'train_samples_per_second': 2.881, 'train_steps_per_second': 0.063, 'train_loss': 2.9672310569069604, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████████| 11/11 [02:53<00:00, 15.78s/it]\n",
      "[INFO|trainer.py:3038] 2024-02-18 21:43:05,571 >> Saving model checkpoint to output_dir\n",
      "[INFO|configuration_utils.py:473] 2024-02-18 21:43:05,602 >> Configuration saved in output_dir/config.json\n",
      "[INFO|configuration_utils.py:609] 2024-02-18 21:43:05,607 >> Configuration saved in output_dir/generation_config.json\n",
      "[INFO|modeling_utils.py:2462] 2024-02-18 21:43:24,317 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at output_dir/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2459] 2024-02-18 21:43:24,364 >> tokenizer config file saved in output_dir/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2468] 2024-02-18 21:43:24,366 >> Special tokens file saved in output_dir/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:191] 2024-02-18 21:43:24,386 >> Copy vocab file to output_dir/spiece.model\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.9672\n",
      "  train_runtime            = 0:02:53.52\n",
      "  train_samples            =        500\n",
      "  train_samples_per_second =      2.881\n",
      "  train_steps_per_second   =      0.063\n",
      "02/18/2024 21:43:24 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3347] 2024-02-18 21:43:24,659 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3349] 2024-02-18 21:43:24,660 >>   Num examples = 1999\n",
      "[INFO|trainer.py:3352] 2024-02-18 21:43:24,660 >>   Batch size = 12\n",
      "100%|███████████████████████████████████████████| 42/42 [01:51<00:00,  2.66s/it]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =     2.1404\n",
      "  eval_runtime            = 0:01:54.31\n",
      "  eval_samples            =       1999\n",
      "  eval_samples_per_second =     17.486\n",
      "  eval_steps_per_second   =      0.367\n",
      "[2024-02-18 21:45:24,172] [INFO] [launch.py:348:main] Process 37614 exits successfully.\n",
      "[2024-02-18 21:45:25,172] [INFO] [launch.py:348:main] Process 37615 exits successfully.\n",
      "[INFO|modelcard.py:450] 2024-02-18 21:45:26,053 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'wmt16 ro-en', 'type': 'wmt16', 'config': 'ro-en', 'split': 'validation', 'args': 'ro-en'}}\n",
      "[2024-02-18 21:45:26,172] [INFO] [launch.py:348:main] Process 37616 exits successfully.\n",
      "[2024-02-18 21:45:32,173] [INFO] [launch.py:348:main] Process 37613 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "# DeepSpeed ZeRO-3 模式 4 GPU 训练翻译模型（T5-3B）\n",
    "!deepspeed --num_gpus=4 translation/run_translation.py \\\n",
    "--deepspeed config/homework-T5-3B-ds_config_zero3.json \\\n",
    "--model_name_or_path t5-3b \\\n",
    "--per_device_train_batch_size 12 \\\n",
    "--per_device_eval_batch_size 12 \\\n",
    "--output_dir output_dir --overwrite_output_dir \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--max_train_samples 500 --num_train_epochs 1 \\\n",
    "--dataset_name wmt16 --dataset_config \"ro-en\" \\\n",
    "--source_lang en --target_lang ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8162b58-d960-49db-b8f8-cb2c09e96c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
