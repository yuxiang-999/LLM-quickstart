{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c6730f-5d76-450b-9788-ec883d024f57",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调训练入门\n",
    "\n",
    "本示例将介绍基于 Transformers 实现模型微调训练的主要流程，包括：\n",
    "- 数据集下载\n",
    "- 数据预处理\n",
    "- 训练超参数配置\n",
    "- 训练评估指标设置\n",
    "- 训练器基本介绍\n",
    "- 实战训练\n",
    "- 模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1e12-1921-4438-8d5d-9760a629dcfe",
   "metadata": {},
   "source": [
    "## YelpReviewFull 数据集\n",
    "\n",
    "**Hugging Face 数据集：[ YelpReviewFull ](https://huggingface.co/datasets/yelp_review_full)**\n",
    "\n",
    "### 数据集摘要\n",
    "\n",
    "Yelp评论数据集包括来自Yelp的评论。它是从Yelp Dataset Challenge 2015数据中提取的。\n",
    "\n",
    "### 支持的任务和排行榜\n",
    "文本分类、情感分类：该数据集主要用于文本分类：给定文本，预测情感。\n",
    "\n",
    "### 语言\n",
    "这些评论主要以英语编写。\n",
    "\n",
    "### 数据集结构\n",
    "\n",
    "#### 数据实例\n",
    "一个典型的数据点包括文本和相应的标签。\n",
    "\n",
    "来自YelpReviewFull测试集的示例如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    'label': 0,\n",
    "    'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'\n",
    "}\n",
    "```\n",
    "\n",
    "#### 数据字段\n",
    "\n",
    "- 'text': 评论文本使用双引号（\"）转义，任何内部双引号都通过2个双引号（\"\"）转义。换行符使用反斜杠后跟一个 \"n\" 字符转义，即 \"\\n\"。\n",
    "- 'label': 对应于评论的分数（介于1和5之间）。\n",
    "\n",
    "#### 数据拆分\n",
    "\n",
    "Yelp评论完整星级数据集是通过随机选取每个1到5星评论的130,000个训练样本和10,000个测试样本构建的。总共有650,000个训练样本和50,000个测试样本。\n",
    "\n",
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf72d6c-7ea5-4ee1-969a-c5060b9cb2d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:51:56.984231Z",
     "iopub.status.busy": "2024-01-07T17:51:56.983898Z",
     "iopub.status.idle": "2024-01-07T17:52:27.559323Z",
     "shell.execute_reply": "2024-01-07T17:52:27.558756Z",
     "shell.execute_reply.started": "2024-01-07T17:51:56.984204Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6fc806-1395-42dd-8121-a6e98a95cf01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:52:30.665233Z",
     "iopub.status.busy": "2024-01-07T17:52:30.664629Z",
     "iopub.status.idle": "2024-01-07T17:52:30.677307Z",
     "shell.execute_reply": "2024-01-07T17:52:30.676620Z",
     "shell.execute_reply.started": "2024-01-07T17:52:30.665183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94ad529-1604-48bd-8c8d-aa2f3bca6200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:52:33.451074Z",
     "iopub.status.busy": "2024-01-07T17:52:33.450501Z",
     "iopub.status.idle": "2024-01-07T17:52:33.459018Z",
     "shell.execute_reply": "2024-01-07T17:52:33.458055Z",
     "shell.execute_reply.started": "2024-01-07T17:52:33.451028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': \"Owning a driving range inside the city limits is like a license to print money.  I don't think I ask much out of a driving range.  Decent mats, clean balls and accessible hours.  Hell you need even less people now with the advent of the machine that doles out the balls.  This place has none of them.  It is april and there are no grass tees yet.  BTW they opened for the season this week although it has been golfing weather for a month.  The mats look like the carpet at my 107 year old aunt Irene's house.  Worn and thread bare.  Let's talk about the hours.  This place is equipped with lights yet they only sell buckets of balls until 730.  It is still light out.  Finally lets you have the pit to hit into.  When I arrived I wasn't sure if this was a driving range or an excavation site for a mastodon or a strip mining operation.  There is no grass on the range. Just mud.  Makes it a good tool to figure out how far you actually are hitting the ball.  Oh, they are cash only also.\\\\n\\\\nBottom line, this place sucks.  The best hope is that the owner sells it to someone that actually wants to make money and service golfers in Pittsburgh.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc45997-e391-456f-b0b9-d3193b0f6a9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:52:36.382698Z",
     "iopub.status.busy": "2024-01-07T17:52:36.382101Z",
     "iopub.status.idle": "2024-01-07T17:52:36.388744Z",
     "shell.execute_reply": "2024-01-07T17:52:36.387576Z",
     "shell.execute_reply.started": "2024-01-07T17:52:36.382649Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2ecebb-d5d1-456d-967c-842a79fdd622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:52:38.681931Z",
     "iopub.status.busy": "2024-01-07T17:52:38.681367Z",
     "iopub.status.idle": "2024-01-07T17:52:38.691768Z",
     "shell.execute_reply": "2024-01-07T17:52:38.690724Z",
     "shell.execute_reply.started": "2024-01-07T17:52:38.681886Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af560b6-7d21-499e-9b82-114be371a98a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T02:09:35.036703Z",
     "iopub.status.busy": "2024-01-04T02:09:35.036144Z",
     "iopub.status.idle": "2024-01-04T02:09:35.060968Z",
     "shell.execute_reply": "2024-01-04T02:09:35.059874Z",
     "shell.execute_reply.started": "2024-01-04T02:09:35.036651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 star</td>\n",
       "      <td>2 large cokes for almost $5 !?!?\\n\\nHell with you, czar of sirloin!  I'm going to McDonalds, Carl's Jr or QuikTrip from now on.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>OK buffet @40 bucks.\\n\\nExtremely crowded and very long lines make for a so so experience.\\n\\nI had a line pass,but if I didn't ,there would be no way I would stand in line for an hour to give them $40 bucks to eat an OK buffet.\\n\\nI've eaten at other places that are better and cheaper,but there is something about the folks bragging that they ate at the Bellagio that makes this some sort of Mecca.\\n\\nNot a must do.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>This was my first visit here.  Liked the location which is quiet on a busy street.   Probably quiet because of the time of year.  People don't usually hang around for the 120 degree temperatures here.  Loved the banana and the chocolate custard.  Tried coffee with pieces of chocolate in it which I did not like as much because it wasnt as smooth and creamy as the banana and chocolate were.  I will definitely be back for more.  Forget about ice cream!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>I love this Smashburger in the northwest valley, I love the concept, I love the modern &amp; clean look, I love the sweet potato fries, and most of all I love the burgers.  Very consistent, taste, flavorful burger.  I have had the Arizona burger which isn't too bad here the other day, but my favorite is just the regular cheese burger.  I have never had a bad burger here but I have only been to this location once since is barely opened last month.  I have been to the 20th Street &amp; Camelback location several times.  I also like the fact that they are open at 10am, which is about the time I am hungry since my day starts way before most peeps.  The service was good for me (sorry Bryan B, but I hope you give them another shot)  I never wait more than a few minutes for my meal but I usually stop in when it is not peak lunch hours either.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>After Earl and I had a bowl of hot savory ramen at Monta Chaya, we stopped by Frost Bite to get some shaved ice to balance out our taste buds.\\n\\nEarl got the medium with tiger blood and cucumber lime, topped with condensed milk.  I must say the cucumber lime really tasted like cucumber lime.  Even the smell was like the fruit/gourd itself!  Highly recommend it with condensed milk.  They not only topped it, but the added the condensed milk in the middle of the cup too.  They do not hold back, and we LOVE it.\\n\\nI had the medium with coconut, tiger blood, and egg custard, with icecream, though, I was kind of disappointed because I was looking forward to the icecream at the bottom, but there was no icecream to be found.  I was probably because initially, I wanted wedding cake flavor, but because I changed it to egg custard, the friendly cashier didn't take down my icecream order.  When I told Earl about my no icecream, he said, \\\"We will go back soon enough for you to have it with icecream.\\\"  Yes-that's how much Earl enjoys it.\\n\\nNeedless to say, we are fans and will come back for more.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Adorable Burros- Horrible Customer Service\\n\\nI waited quite awhile before I wrote this review as I was severely irritated right after the incident..\\nI contacted this business to inquire about their services on initially on 3-4-14. \\n\\nOn 5-20 I had worked out the logistics/ got the appropriate permissions from our wedding venue and told Vanessa that I would like to book two Burros. \\n\\nOne month before our wedding date I received an e-mail from Vanessa letting me know that they had me as a tentative hold and that they needed the contract finalized. Over the next couple days we worked out the specifics as far as timing would go. \\n\\nOn October 1st I received an e-mail from Vanessa saying that they had overbooked and would only have one Burro available- to try and make it right they offered us their \\\"photo option\\\". \\nOn October 5th Vanessa sent me a pay pal invoice and it and the contract were returned on the same day. \\n\\nThe day of the event came (10/18) and even though the Burros were my fianc\\u00e9's \\\"groom gift\\\" (When I initially spoke with Vanessa on the phone I let her know) he never got to see them.  \\nFrom the day after the wedding(10/19) my fianc\\u00e9 and I were traveling on our honeymoon and had very little internet access. (what internet  we did have was unsecure). \\n\\nWhen I got back to internet access I had no less than 6 e-mails with progressively rude comments from Vanessa letting me know that she had expected payment on the day of the event.  Shame on me for not getting it to her... though it would have been a lot easier to pay her IF I HAD SEEN HER. Good thing she left without us literally ever laying eyes on the burros. I have never left a bill unpaid in my entire life and considering that I paid her the deposit the day she sent the invoice she should have had a clue about my character. \\n\\nAs for the photo option- I am glad that I didn't pay for it.. We got approximately 50 photos with at least a quarter of the photos blurry/people not looking at the camera/the ground.. great stuff. Luckily a lot of guests took their own photos with their camera phones.\\n\\nWith all of that being said all of the guests loved the burros.... Good luck if you book with these people.\\n\\nEDITED TO RESPOND TO BUSINESS OWNER---\\n\\nVanessa,\\n\\nYou have got to be kidding me. You were unaware that I was unhappy with your service?? I told you that the whole reason we had the burros was for my husband to enjoy and you didn't bother to come see us (though you were asked to) and then you complained that I didn't pay you soon enough even though I let you know why...unbelievable (off the continent/ unsecured internet). You took down my photo gallery or I would link it here.  I am not sure how you counted 70 photos...(I cant help but notice all of the reviews you asked for from other customers feature a photo of them with the burro-- something we don't have). As for the six e-mails that appeared to come from you- I would be entirely happy to send you a screen shot if you would like. You were paid prior to the event.. half up front. I would be happy to forward you that e-mail if you would like as well.\\n\\nAs far as calling my wedding \\\"low budget and disorganized\\\"... you are a true class act lady. So sorry that we didn't get married at the Ritz. Lastly as far as \\\"being no where in sight\\\"- yes, as is traditional- we were taking photos. As outlined in the timeline- I requested from the burros be with us from 5- 5:15 so that we could see them/take photos.. that didn't happen. And the beer? You are correct- it wasn't where it was supposed to be--turns out delivery guys get a little lost when they deliver to a 92 acre facility (even with clear instructions).\\n\\nOver all I am just in awe that you went to the lengths to call our wedding low budget and disorganized.... true class ma'am, true class.\\n\\nThe point of my post? I was trying to give a clear understanding with timeline... the exact point though- LOVED the burros- the guests raved about them. The customer service was poor... further demonstrated by your response.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1 star</td>\n",
       "      <td>I have been to this Starbucks growing up as a kid. The baristas working here always did an amazing job. Last night I went and was disappointed with how the baristas represented Starbucks. They were talking instead of helping us. They never asked for my name to write it on the cup. They took longer than I am used to at Starbucks to make my drink, because they were talking. Then after my drink was made they didn't call out what it was or whose it was. After making a second drink they called out mine. I had asked for extra toppings but the drink was so hot all the whipped cream had melted. I went to the counter to ask for some whipped cream, and again had to wait while the barista made a drink and then washed out the blender before being helped. Then when I told him what I'd like he says, \\\"Oh that's what you were saying\\\" adds the topping and pushes the drink on the counter towards me. He didn't replace the lid or act politely. They were not busy either nor were they understaffed. I absolutely love Starbucks, and the remodel here looks fabulous! I just wish I had a different experience last night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Food is just ok. Wait staff constantly complaining at the bar about how managers are incompetent and customers can hear it. Drinks are small but salads are good. Split them, big and overpriced for 1 person. Mussels better at Upstream. Pizza ok.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Completely a fan of this place!!! Everyone is complaining about the wait but we got in and out in less than 10 minutes and we went on a wednesday afternoon!!! For sure worth the wait!\\n\\nThe lobster tails are mind blowing!! Cannolis delicious and the cupcakes YUM!!!\\n\\nI would for sure go back and I had been waiting awhile to try some of the cake boss goodies and it turned out to be everything I expected and more!\\n\\nStop with the bad reviews about the wait because on the show there is always a wait as well!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2 star</td>\n",
       "      <td>1.5 stars. WORST. SERVICE. EVER.\\n\\nour waiter seemed completely clueless. kept mixing up orders. he disappeared for awhile...we were finished eating for at least an hour before he even showed up at our table again. trying to flag him down was nearly impossible. the checks weren't even calculated right. he never came back to the table, so our whole party ended up getting up to go pay for our stuff at the bar. seriously, how is it that a different group that came in at least an hour after, ate and left before we did? \\n\\nat least the food was good and they have good happy hour specials. thats the only reason why i would even consider coming back.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df7cd0-23cd-458f-b2b5-f025c3b9fe62",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "下载数据集到本地后，使用 Tokenizer 来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n",
    "\n",
    "Datasets 的 `map` 方法，支持一次性在整个数据集上应用预处理函数。\n",
    "\n",
    "下面使用填充到最大长度的策略，处理整个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf2b342-e1dd-4ab6-ad57-28eb2513ae38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:52:50.105693Z",
     "iopub.status.busy": "2024-01-07T17:52:50.105117Z",
     "iopub.status.idle": "2024-01-07T17:53:04.080560Z",
     "shell.execute_reply": "2024-01-07T17:53:04.079834Z",
     "shell.execute_reply.started": "2024-01-07T17:52:50.105644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd652a0c3ca04d37867823129fd80de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47a415a8-cd15-4a8c-851b-9b4740ef8271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:53:06.569110Z",
     "iopub.status.busy": "2024-01-07T17:53:06.568595Z",
     "iopub.status.idle": "2024-01-07T17:53:06.587752Z",
     "shell.execute_reply": "2024-01-07T17:53:06.587110Z",
     "shell.execute_reply.started": "2024-01-07T17:53:06.569056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>This is a great affordable and centrally located hotel in the Montreal area. There is free wi-fi, and they give you the password on checkin, so there's no hassle in obtaining it. We needed a new iron since the one in the room didn't work. It arrived promptly in the morning. The staff is courtesy, and the rooms are clean and well appointed. Not a luxury hotel, but an awesome deal for your money. I would definitely stay here again.</td>\n",
       "      <td>[101, 1188, 1110, 170, 1632, 19017, 1105, 2129, 1193, 1388, 3415, 1107, 1103, 5206, 1298, 119, 1247, 1110, 1714, 192, 1182, 118, 20497, 117, 1105, 1152, 1660, 1128, 1103, 25241, 1113, 4031, 1394, 117, 1177, 1175, 112, 188, 1185, 1144, 27112, 1107, 11621, 1122, 119, 1284, 1834, 170, 1207, 3926, 1290, 1103, 1141, 1107, 1103, 1395, 1238, 112, 189, 1250, 119, 1135, 2474, 13796, 1107, 1103, 2106, 119, 1109, 2546, 1110, 15897, 117, 1105, 1103, 4045, 1132, 4044, 1105, 1218, 1923, 119, 1753, 170, 9886, 3415, 117, 1133, 1126, 14918, 2239, 1111, 1240, 1948, 119, 146, 1156, 5397, 2215, 1303, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33d153-f729-4f04-972c-a764c1cbbb8b",
   "metadata": {},
   "source": [
    "### 数据抽样\n",
    "\n",
    "使用 1000 个数据样本，在 BERT 上演示小规模训练（基于 Pytorch Trainer）\n",
    "\n",
    "`shuffle()`函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a17317d8-3c6a-467f-843d-87491f600db1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:53:10.557723Z",
     "iopub.status.busy": "2024-01-07T17:53:10.557166Z",
     "iopub.status.idle": "2024-01-07T17:53:10.604107Z",
     "shell.execute_reply": "2024-01-07T17:53:10.603150Z",
     "shell.execute_reply.started": "2024-01-07T17:53:10.557675Z"
    }
   },
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "half_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(300000))\n",
    "half_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(30000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b65d63-2d3a-4a56-bc31-6e88a29e9dec",
   "metadata": {},
   "source": [
    "## 微调训练配置\n",
    "\n",
    "### 加载 BERT 模型\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d2af4df-abd4-4a4b-94b6-b0e7375304ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:53:14.643806Z",
     "iopub.status.busy": "2024-01-07T17:53:14.643292Z",
     "iopub.status.idle": "2024-01-07T17:53:20.779280Z",
     "shell.execute_reply": "2024-01-07T17:53:20.778495Z",
     "shell.execute_reply.started": "2024-01-07T17:53:14.643763Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44014df-b52c-4c72-9e9f-54424725a473",
   "metadata": {},
   "source": [
    "### 训练超参数（TrainingArguments）\n",
    "\n",
    "完整配置参数与默认值：https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "源代码定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161\n",
    "\n",
    "**最重要配置：模型权重保存路径(output_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c01d5c-de72-4ff0-b11d-e07ac5346888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:53:20.781846Z",
     "iopub.status.busy": "2024-01-07T17:53:20.781243Z",
     "iopub.status.idle": "2024-01-07T17:53:22.627128Z",
     "shell.execute_reply": "2024-01-07T17:53:22.626413Z",
     "shell.execute_reply.started": "2024-01-07T17:53:20.781826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 01:53:21.100205: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-08 01:53:21.100261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-08 01:53:21.101482: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-08 01:53:21.108728: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-08 01:53:22.106631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"models/bert-base-cased\"\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为100\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce03480-3aaa-48ea-a0c6-a177b8d8e34f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T02:47:08.486641Z",
     "iopub.status.busy": "2024-01-03T02:47:08.486223Z",
     "iopub.status.idle": "2024-01-03T02:47:08.490465Z",
     "shell.execute_reply": "2024-01-03T02:47:08.489945Z",
     "shell.execute_reply.started": "2024-01-03T02:47:08.486605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased/test_trainer/runs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "optim_args=None,\n",
      "output_dir=models/bert-base-cased/test_trainer,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased/test_trainer,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd3365-d359-4ab4-a300-4717590cc240",
   "metadata": {},
   "source": [
    "### 训练过程中的指标评估（Evaluate)\n",
    "\n",
    "**[Hugging Face Evaluate 库](https://huggingface.co/docs/evaluate/index)** 支持使用一行代码，获得数十种不同领域（自然语言处理、计算机视觉、强化学习等）的评估方法。 当前支持 **完整评估指标：https://huggingface.co/evaluate-metric**\n",
    "\n",
    "训练器（Trainer）在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告指标。 \n",
    "\n",
    "Evaluate库提供了一个简单的准确率函数，您可以使用`evaluate.load`函数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a8ef138-5bf2-41e5-8c68-df8e11f4e98f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:53:24.058850Z",
     "iopub.status.busy": "2024-01-07T17:53:24.058060Z",
     "iopub.status.idle": "2024-01-07T17:53:37.164997Z",
     "shell.execute_reply": "2024-01-07T17:53:37.163773Z",
     "shell.execute_reply.started": "2024-01-07T17:53:24.058786Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d406c0-56d0-4a54-9c6c-e126ab7f5254",
   "metadata": {},
   "source": [
    "\n",
    "接着，调用 `compute` 函数来计算预测的准确率。\n",
    "\n",
    "在将预测传递给 compute 函数之前，我们需要将 logits 转换为预测值（**所有Transformers 模型都返回 logits**）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f46d2e59-1ebf-43d2-bc86-6b57a4d24d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:53:37.167831Z",
     "iopub.status.busy": "2024-01-07T17:53:37.167054Z",
     "iopub.status.idle": "2024-01-07T17:53:37.173432Z",
     "shell.execute_reply": "2024-01-07T17:53:37.172328Z",
     "shell.execute_reply.started": "2024-01-07T17:53:37.167787Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feba67-9ca9-4793-9a15-3eaa426df2a1",
   "metadata": {},
   "source": [
    "#### 训练过程指标监控\n",
    "\n",
    "通常，为了监控训练过程中的评估指标变化，我们可以在`TrainingArguments`指定`evaluation_strategy`参数，以便在 epoch 结束时报告评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afaaee18-4986-4e39-8ad9-b8d413ab4cd1",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-01-07T17:53:37.175066Z",
     "iopub.status.busy": "2024-01-07T17:53:37.174642Z",
     "iopub.status.idle": "2024-01-07T17:53:37.191003Z",
     "shell.execute_reply": "2024-01-07T17:53:37.189777Z",
     "shell.execute_reply.started": "2024-01-07T17:53:37.175025Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  evaluation_strategy=\"steps\",  # 可选值：no, steps, epoch\n",
    "                                  eval_steps=500,\n",
    "                                  per_device_eval_batch_size=24,\n",
    "                                  per_device_train_batch_size=24,\n",
    "                                  logging_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6981-e444-4c0f-a7cb-dd7f2ba8df12",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "### 实例化训练器（Trainer）\n",
    "\n",
    "`kernel version` 版本问题：暂不影响本示例代码运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca1d12ac-89dc-4c30-8282-f859724c0062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:53:37.193430Z",
     "iopub.status.busy": "2024-01-07T17:53:37.193013Z",
     "iopub.status.idle": "2024-01-07T17:53:37.521211Z",
     "shell.execute_reply": "2024-01-07T17:53:37.520325Z",
     "shell.execute_reply.started": "2024-01-07T17:53:37.193389Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # train_dataset=small_train_dataset,\n",
    "    # eval_dataset=small_eval_dataset,\n",
    "    train_dataset=half_train_dataset,\n",
    "    eval_dataset=half_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833e0db-1168-4a3c-8b75-bfdcef8c5157",
   "metadata": {},
   "source": [
    "## 使用 nvidia-smi 查看 GPU 使用\n",
    "\n",
    "为了实时查看GPU使用情况，可以使用 `watch` 指令实现轮询：`watch -n 1 nvidia-smi`:\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 14:37:41 2023\n",
    "\n",
    "Wed Dec 20 14:37:41 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   64C    P0              69W /  70W |   6665MiB / 15360MiB |     98%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     18395      C   /root/miniconda3/bin/python                6660MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accfe921-471d-481a-96da-c491cdebad0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T02:48:12.383084Z",
     "iopub.status.busy": "2024-01-03T02:48:12.382705Z",
     "iopub.status.idle": "2024-01-03T06:15:17.991332Z",
     "shell.execute_reply": "2024-01-03T06:15:17.990684Z",
     "shell.execute_reply.started": "2024-01-03T02:48:12.383048Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 3:27:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.962200</td>\n",
       "      <td>0.815489</td>\n",
       "      <td>0.642333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.824400</td>\n",
       "      <td>0.794675</td>\n",
       "      <td>0.651033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.796200</td>\n",
       "      <td>0.756103</td>\n",
       "      <td>0.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.771300</td>\n",
       "      <td>0.756934</td>\n",
       "      <td>0.664933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.756800</td>\n",
       "      <td>0.751870</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.739780</td>\n",
       "      <td>0.671567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.683100</td>\n",
       "      <td>0.729674</td>\n",
       "      <td>0.680867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.663900</td>\n",
       "      <td>0.738611</td>\n",
       "      <td>0.679967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>0.734863</td>\n",
       "      <td>0.678200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.653900</td>\n",
       "      <td>0.730346</td>\n",
       "      <td>0.679167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.647400</td>\n",
       "      <td>0.733596</td>\n",
       "      <td>0.680200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>0.718736</td>\n",
       "      <td>0.687233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>0.767309</td>\n",
       "      <td>0.685367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.529900</td>\n",
       "      <td>0.770527</td>\n",
       "      <td>0.683667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.530100</td>\n",
       "      <td>0.774083</td>\n",
       "      <td>0.684467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.779367</td>\n",
       "      <td>0.683633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.532200</td>\n",
       "      <td>0.773594</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.527500</td>\n",
       "      <td>0.772157</td>\n",
       "      <td>0.685033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=0.6641348388671875, metrics={'train_runtime': 12425.2809, 'train_samples_per_second': 72.433, 'train_steps_per_second': 0.755, 'total_flos': 2.368063282176e+17, 'train_loss': 0.6641348388671875, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d581099-37a4-4470-b051-1ada38554089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T07:36:52.748412Z",
     "iopub.status.busy": "2024-01-03T07:36:52.747941Z",
     "iopub.status.idle": "2024-01-03T07:36:52.779377Z",
     "shell.execute_reply": "2024-01-03T07:36:52.778344Z",
     "shell.execute_reply.started": "2024-01-03T07:36:52.748366Z"
    }
   },
   "outputs": [],
   "source": [
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))\n",
    "\n",
    "half_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(30000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffb47eab-1370-491e-8a84-6d5347a350b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T07:36:58.984366Z",
     "iopub.status.busy": "2024-01-03T07:36:58.983895Z",
     "iopub.status.idle": "2024-01-03T07:39:12.034503Z",
     "shell.execute_reply": "2024-01-03T07:39:12.033773Z",
     "shell.execute_reply.started": "2024-01-03T07:36:58.984321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 02:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7699790596961975,\n",
       " 'eval_accuracy': 0.6858666666666666,\n",
       " 'eval_runtime': 133.0383,\n",
       " 'eval_samples_per_second': 225.499,\n",
       " 'eval_steps_per_second': 2.353,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.evaluate(small_test_dataset)\n",
    "trainer.evaluate(half_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a55686-7c43-4ab8-a5cd-0e77f14c7c52",
   "metadata": {},
   "source": [
    "### 保存模型和训练状态\n",
    "\n",
    "- 使用 `trainer.save_model` 方法保存模型，后续可以通过 from_pretrained() 方法重新加载\n",
    "- 使用 `trainer.save_state` 方法保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad0cbc14-9ef7-450f-a1a3-4f92b6486f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T07:39:25.544811Z",
     "iopub.status.busy": "2024-01-03T07:39:25.544255Z",
     "iopub.status.idle": "2024-01-03T07:39:26.188239Z",
     "shell.execute_reply": "2024-01-03T07:39:26.186896Z",
     "shell.execute_reply.started": "2024-01-03T07:39:25.544767Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{model_dir}/finetuned-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "badf5868-2847-439d-a73e-42d1cca67b5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T07:39:29.233568Z",
     "iopub.status.busy": "2024-01-03T07:39:29.232974Z",
     "iopub.status.idle": "2024-01-03T07:39:29.242186Z",
     "shell.execute_reply": "2024-01-03T07:39:29.240901Z",
     "shell.execute_reply.started": "2024-01-03T07:39:29.233521Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61828934-01da-4fc3-9e75-8d754c25dfbc",
   "metadata": {},
   "source": [
    "## Homework: 使用完整的 YelpReviewFull 数据集训练，对比看 Acc 最高能到多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ee2580a-7a5a-46ae-a28b-b41e9e838eb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:53:56.366129Z",
     "iopub.status.busy": "2024-01-07T17:53:56.365494Z",
     "iopub.status.idle": "2024-01-07T17:53:56.393898Z",
     "shell.execute_reply": "2024-01-07T17:53:56.392660Z",
     "shell.execute_reply.started": "2024-01-07T17:53:56.366082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize 全量数据集\n",
    "\n",
    "full_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "full_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a571a3db-1ba6-4aa2-ac3d-4e14a46a5da6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:53:59.542994Z",
     "iopub.status.busy": "2024-01-07T17:53:59.542518Z",
     "iopub.status.idle": "2024-01-07T17:53:59.553940Z",
     "shell.execute_reply": "2024-01-07T17:53:59.552744Z",
     "shell.execute_reply.started": "2024-01-07T17:53:59.542950Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置训练超参数\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/homework_test_trainer\",\n",
    "                                  logging_dir=f\"{model_dir}/homework_test_trainer/runs\",\n",
    "                                  evaluation_strategy=\"steps\",  # 可选值：no, steps, epoch\n",
    "                                  eval_steps=3000,\n",
    "                                  per_device_eval_batch_size=24,\n",
    "                                  per_device_train_batch_size=24,\n",
    "                                  logging_steps=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06a4448c-ee6d-4e1d-8d2d-5eb96a2b9900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:54:02.085740Z",
     "iopub.status.busy": "2024-01-07T17:54:02.085267Z",
     "iopub.status.idle": "2024-01-07T17:54:02.103150Z",
     "shell.execute_reply": "2024-01-07T17:54:02.102010Z",
     "shell.execute_reply.started": "2024-01-07T17:54:02.085696Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 实例化训练器（Trainer）\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    "    eval_dataset=full_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c39d119-1ec9-4cd4-9d1e-bacfc9bd4a81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T17:54:13.910285Z",
     "iopub.status.busy": "2024-01-07T17:54:13.909790Z",
     "iopub.status.idle": "2024-01-08T06:24:14.237984Z",
     "shell.execute_reply": "2024-01-08T06:24:14.237229Z",
     "shell.execute_reply.started": "2024-01-07T17:54:13.910240Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40626' max='40626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40626/40626 12:29:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.855800</td>\n",
       "      <td>0.773281</td>\n",
       "      <td>0.657140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.771700</td>\n",
       "      <td>0.759693</td>\n",
       "      <td>0.669780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.752700</td>\n",
       "      <td>0.728147</td>\n",
       "      <td>0.680160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.734800</td>\n",
       "      <td>0.716574</td>\n",
       "      <td>0.684020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.695200</td>\n",
       "      <td>0.727479</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.660400</td>\n",
       "      <td>0.718358</td>\n",
       "      <td>0.686660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.655400</td>\n",
       "      <td>0.713631</td>\n",
       "      <td>0.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.653400</td>\n",
       "      <td>0.705623</td>\n",
       "      <td>0.691960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.645200</td>\n",
       "      <td>0.694544</td>\n",
       "      <td>0.695500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.738102</td>\n",
       "      <td>0.692760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.740480</td>\n",
       "      <td>0.692940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>0.732530</td>\n",
       "      <td>0.692740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.540100</td>\n",
       "      <td>0.738028</td>\n",
       "      <td>0.694780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40626, training_loss=0.6581256824952193, metrics={'train_runtime': 45000.0574, 'train_samples_per_second': 43.333, 'train_steps_per_second': 0.903, 'total_flos': 5.130803778048e+17, 'train_loss': 0.6581256824952193, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5beac2f-08b3-4127-8f32-9ffda0482c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T06:24:14.240022Z",
     "iopub.status.busy": "2024-01-08T06:24:14.239785Z",
     "iopub.status.idle": "2024-01-08T06:30:51.449345Z",
     "shell.execute_reply": "2024-01-08T06:30:51.448507Z",
     "shell.execute_reply.started": "2024-01-08T06:24:14.240002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1042' max='1042' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1042/1042 06:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7362170219421387,\n",
       " 'eval_accuracy': 0.69534,\n",
       " 'eval_runtime': 397.1876,\n",
       " 'eval_samples_per_second': 125.885,\n",
       " 'eval_steps_per_second': 2.623,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估模型最终准确率\n",
    "\n",
    "full_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64)\n",
    "trainer.evaluate(full_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82107968-ea20-4adb-b0d4-742306c264aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T06:30:51.450634Z",
     "iopub.status.busy": "2024-01-08T06:30:51.450433Z",
     "iopub.status.idle": "2024-01-08T06:30:51.999841Z",
     "shell.execute_reply": "2024-01-08T06:30:51.999076Z",
     "shell.execute_reply.started": "2024-01-08T06:30:51.450615Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型和训练状态\n",
    "\n",
    "trainer.save_model(f\"{model_dir}/homework-finetuned-trainer\")\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a67b4-30dd-47ac-aef6-d952fb2be3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
