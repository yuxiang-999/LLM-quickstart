{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调语言模型-问答任务\n",
    "\n",
    "我们已经学会使用 Pipeline 加载支持问答任务的预训练模型，本教程代码将展示如何微调训练一个支持问答任务的模型。\n",
    "\n",
    "**注意：微调后的模型仍然是通过提取上下文的子串来回答问题的，而不是生成新的文本。**\n",
    "\n",
    "### 模型执行问答效果示例\n",
    "\n",
    "![Widget inference representing the QA task](data/image/question_answering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:38:25.684291Z",
     "iopub.status.busy": "2024-01-08T12:38:25.683851Z",
     "iopub.status.idle": "2024-01-08T12:38:25.696138Z",
     "shell.execute_reply": "2024-01-08T12:38:25.695039Z",
     "shell.execute_reply.started": "2024-01-08T12:38:25.684246Z"
    },
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "# 根据你使用的模型和GPU资源情况，调整以下关键参数\n",
    "# squad_v2 = False\n",
    "squad_v2 = True\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "# batch_size = 16\n",
    "batch_size = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## 下载数据集\n",
    "\n",
    "在本教程中，我们将使用[斯坦福问答数据集(SQuAD）](https://rajpurkar.github.io/SQuAD-explorer/)。\n",
    "\n",
    "### SQuAD 数据集\n",
    "\n",
    "**斯坦福问答数据集(SQuAD)** 是一个阅读理解数据集，由众包工作者在一系列维基百科文章上提出问题组成。每个问题的答案都是相应阅读段落中的文本片段或范围，或者该问题可能无法回答。\n",
    "\n",
    "SQuAD2.0将SQuAD1.1中的10万个问题与由众包工作者对抗性地撰写的5万多个无法回答的问题相结合，使其看起来与可回答的问题类似。要在SQuAD2.0上表现良好，系统不仅必须在可能时回答问题，还必须确定段落中没有支持任何答案，并放弃回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:38:29.982462Z",
     "iopub.status.busy": "2024-01-08T12:38:29.982013Z",
     "iopub.status.idle": "2024-01-08T12:38:31.029835Z",
     "shell.execute_reply": "2024-01-08T12:38:31.029143Z",
     "shell.execute_reply.started": "2024-01-08T12:38:29.982417Z"
    },
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "69caab03d6264fef9fc5649bffff5e20",
      "3f74532faa86412293d90d3952f38c4a",
      "50615aa59c7247c4804ca5cbc7945bd7",
      "fe962391292a413ca55dc932c4279fa7",
      "299f4b4c07654e53a25f8192bd1d7bbd",
      "ad04ed1038154081bbb0c1444784dcc2",
      "7c667ad22b5740d5a6319f1b1e3a8097",
      "46c2b043c0f84806978784a45a4e203b",
      "80e2943be35f46eeb24c8ab13faa6578",
      "de5956b5008d4fdba807bae57509c393",
      "931db1f7a42f4b46b7ff8c2e1262b994",
      "6c1db72efff5476e842c1386fadbbdba",
      "ccd2f37647c547abb4c719b75a26f2de",
      "d30a66df5c0145e79693e09789d96b81",
      "5fa26fc336274073abbd1d550542ee33",
      "2b34de08115d49d285def9269a53f484",
      "d426be871b424affb455aeb7db5e822e",
      "160bf88485f44f5cb6eaeecba5e0901f",
      "745c0d47d672477b9bb0dae77b926364",
      "d22ab78269cd4ccfbcf70c707057c31b",
      "d298eb19eeff453cba51c2804629d3f4",
      "a7204ade36314c86907c562e0a2158b8",
      "e35d42b2d352498ca3fc8530393786b2",
      "75103f83538d44abada79b51a1cec09e",
      "f6253931d90543e9b5fd0bb2d615f73a",
      "051aa783ff9e47e28d1f9584043815f5",
      "0984b2a14115454bbb009df71c1cf36f",
      "8ab9dfce29854049912178941ef1b289",
      "c9de740e007141958545e269372780a4",
      "cbea68b25d6d4ba09b2ce0f27b1726d5",
      "5781fc45cf8d486cb06ed68853b2c644",
      "d2a92143a08a4951b55bab9bc0a6d0d3",
      "a14c3e40e5254d61ba146f6ec88eae25",
      "c4ffe6f624ce4e978a0d9b864544941a",
      "1aca01c1d8c940dfadd3e7144bb35718",
      "9fbbaae50e6743f2aa19342152398186",
      "fea27ca6c9504fc896181bc1ff5730e5",
      "940d00556cb849b3a689d56e274041c2",
      "5cdf9ed939fb42d4bf77301c80b8afca",
      "94b39ccfef0b4b08bf2fb61bb0a657c1",
      "9a55087c85b74ea08b3e952ac1d73cbe",
      "2361ab124daf47cc885ff61f2899b2af",
      "1a65887eb37747ddb75dc4a40f7285f2",
      "3c946e2260704e6c98593136bd32d921",
      "50d325cdb9844f62a9ecc98e768cb5af",
      "aa781f0cfe454e9da5b53b93e9baabd8",
      "6bb68d3887ef43809eb23feb467f9723",
      "7e29a8b952cf4f4ea42833c8bf55342f",
      "dd5997d01d8947e4b1c211433969b89b",
      "2ace4dc78e2f4f1492a181bcd63304e7",
      "bbee008c2791443d8610371d1f16b62b",
      "31b1c8a2e3334b72b45b083688c1a20c",
      "7fb7c36adc624f7dbbcb4a831c1e4f63",
      "0b7c8f1939074794b3d9221244b1344d",
      "a71908883b064e1fbdddb547a8c41743",
      "2f5223f26c8541fc87e91d2205c39995"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-01-08T12:38:35.112083Z",
     "iopub.status.busy": "2024-01-08T12:38:35.111499Z",
     "iopub.status.idle": "2024-01-08T12:39:05.109715Z",
     "shell.execute_reply": "2024-01-08T12:39:05.108358Z",
     "shell.execute_reply.started": "2024-01-08T12:38:35.112039Z"
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b"
   },
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:39:29.939934Z",
     "iopub.status.busy": "2024-01-08T12:39:29.939313Z",
     "iopub.status.idle": "2024-01-08T12:39:29.953319Z",
     "shell.execute_reply": "2024-01-08T12:39:29.952152Z",
     "shell.execute_reply.started": "2024-01-08T12:39:29.939880Z"
    },
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对比数据集\n",
    "\n",
    "相比快速入门使用的 Yelp 评论数据集，我们可以看到 SQuAD 训练和测试集都新增了用于上下文、问题以及问题答案的列：\n",
    "\n",
    "**YelpReviewFull Dataset：**\n",
    "\n",
    "```json\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 650000\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 50000\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:39:37.162988Z",
     "iopub.status.busy": "2024-01-08T12:39:37.162376Z",
     "iopub.status.idle": "2024-01-08T12:39:37.184518Z",
     "shell.execute_reply": "2024-01-08T12:39:37.183381Z",
     "shell.execute_reply.started": "2024-01-08T12:39:37.162934Z"
    },
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be85543aeaaa14008c9063',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': 'When did Beyonce start becoming popular?',\n",
       " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从上下文中组织回复内容\n",
    "\n",
    "我们可以看到答案是通过它们在文本中的起始位置（这里是第515个字符）以及它们的完整文本表示的，这是上面提到的上下文的子字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:39:44.454983Z",
     "iopub.status.busy": "2024-01-08T12:39:44.454372Z",
     "iopub.status.idle": "2024-01-08T12:39:44.467968Z",
     "shell.execute_reply": "2024-01-08T12:39:44.466955Z",
     "shell.execute_reply.started": "2024-01-08T12:39:44.454931Z"
    },
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:39:50.607652Z",
     "iopub.status.busy": "2024-01-08T12:39:50.607039Z",
     "iopub.status.idle": "2024-01-08T12:39:50.640781Z",
     "shell.execute_reply": "2024-01-08T12:39:50.639605Z",
     "shell.execute_reply.started": "2024-01-08T12:39:50.607601Z"
    },
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5726888e708984140094c921</td>\n",
       "      <td>Marvel_Comics</td>\n",
       "      <td>Marvel earned a great deal of money and recognition during the comic book boom of the early 1990s, launching the successful 2099 line of comics set in the future (Spider-Man 2099, etc.) and the creatively daring though commercially unsuccessful Razorline imprint of superhero comics created by novelist and filmmaker Clive Barker. In 1990, Marvel began selling Marvel Universe Cards with trading card maker SkyBox International. These were collectible trading cards that featured the characters and events of the Marvel Universe. The 1990s saw the rise of variant covers, cover enhancements, swimsuit issues, and company-wide crossovers that affected the overall continuity of the fictional Marvel Universe</td>\n",
       "      <td>What 1990s comic line featured futuristic, sci-fi stories?</td>\n",
       "      <td>{'text': ['2099 line'], 'answer_start': [124]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570ab7096d058f19001830ac</td>\n",
       "      <td>Houston</td>\n",
       "      <td>The primary city airport is George Bush Intercontinental Airport (IAH), the tenth-busiest in the United States for total passengers, and twenty eighth-busiest worldwide. Bush Intercontinental currently ranks fourth in the United States for non-stop domestic and international service with 182 destinations. In 2006, the United States Department of Transportation named IAH the fastest-growing of the top ten airports in the United States. The Houston Air Route Traffic Control Center stands on the George Bush Intercontinental Airport grounds.</td>\n",
       "      <td>How does Houston's George Bush Intercontinental Airport rank for number of passengers?</td>\n",
       "      <td>{'text': ['tenth-busiest'], 'answer_start': [76]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56e8f03d0b45c0140094cd82</td>\n",
       "      <td>Westminster_Abbey</td>\n",
       "      <td>Subsequently, it became one of Britain's most significant honours to be buried or commemorated in the abbey. The practice of burying national figures in the abbey began under Oliver Cromwell with the burial of Admiral Robert Blake in 1657. The practice spread to include generals, admirals, politicians, doctors and scientists such as Isaac Newton, buried on 4 April 1727, and Charles Darwin, buried 26 April 1882. Another was William Wilberforce who led the movement to abolish slavery in the United Kingdom and the Plantations, buried on 3 August 1833. Wilberforce was buried in the north transept, close to his friend, the former Prime Minister, William Pitt.[citation needed]</td>\n",
       "      <td>Who was buried in the abbey on 3 August 1833?</td>\n",
       "      <td>{'text': ['William Wilberforce'], 'answer_start': [427]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57302bd0a23a5019007fcef0</td>\n",
       "      <td>Santa_Monica,_California</td>\n",
       "      <td>Santa Monica College is a junior college originally founded in 1929. Many SMC graduates transfer to the University of California system. It occupies 35 acres (14 hectares) and enrolls 30,000 students annually. The Frederick S. Pardee RAND Graduate School, associated with the RAND Corporation, is the U.S.'s largest producer of public policy PhDs. The Art Institute of California – Los Angeles is also located in Santa Monica near the Santa Monica Airport. Universities and colleges within a 22-mile (35 km) radius from Santa Monica include Santa Monica College, Antioch University Los Angeles, Loyola Marymount University, Mount St. Mary's College, Pepperdine University, California State University, Northridge, California State University, Los Angeles, UCLA, USC, West Los Angeles College, California Institute of Technology (Caltech), Occidental College (Oxy), Los Angeles City College, Los Angeles Southwest College, Los Angeles Valley College, and Emperor's College of Traditional Oriental Medicine.</td>\n",
       "      <td>There are a plethora of colleges and universities within what radius of Santa Monica?</td>\n",
       "      <td>{'text': ['22-mile'], 'answer_start': [492]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733b463d058e614000b60c9</td>\n",
       "      <td>Tajikistan</td>\n",
       "      <td>The parliamentary elections of 2005 aroused many accusations from opposition parties and international observers that President Emomalii Rahmon corruptly manipulates the election process and unemployment. The most recent elections, in February 2010, saw the ruling PDPT lose four seats in Parliament, yet still maintain a comfortable majority. The Organization for Security and Co-operation in Europe election observers said the 2010 polling \"failed to meet many key OSCE commitments\" and that \"these elections failed on many basic democratic standards.\" The government insisted that only minor violations had occurred, which would not affect the will of the Tajik people.</td>\n",
       "      <td>What was the Tajikistan governments response?</td>\n",
       "      <td>{'text': ['The government insisted that only minor violations had occurred, which would not affect the will of the Tajik people'], 'answer_start': [555]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5731df9ce17f3d14004224db</td>\n",
       "      <td>Bras%C3%ADlia</td>\n",
       "      <td>Both low-cost and luxury housing were built by the government in the Brasília. The residential zones of the inner city are arranged into superquadras (\"superblocks\"): groups of apartment buildings along with a prescribed number and type of schools, retail stores, and open spaces. At the northern end of Lake Paranoá, separated from the inner city, is a peninsula with many fashionable homes, and a similar city exists on the southern lakeshore. Originally the city planners envisioned extensive public areas along the shores of the artificial lake, but during early development private clubs, hotels, and upscale residences and restaurants gained footholds around the water. Set well apart from the city are satellite cities, including Gama, Ceilândia, Taguatinga, Núcleo Bandeirante, Sobradinho, and Planaltina. These cities, with the exception of Gama and Sobradinho were not planned.</td>\n",
       "      <td>What mostly-unplanned cities are around Brasilia?</td>\n",
       "      <td>{'text': ['Gama, Ceilândia, Taguatinga, Núcleo Bandeirante, Sobradinho, and Planaltina'], 'answer_start': [737]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57273a5e708984140094db16</td>\n",
       "      <td>Affirmative_action_in_the_United_States</td>\n",
       "      <td>Ricci v. DeStefano was heard by the United States Supreme Court in 2009. The case concerns White and Hispanic firefighters in New Haven, Connecticut, who upon passing their test for promotions to management were denied the promotions, allegedly because of a discriminatory or at least questionable test. The test gave 17 whites and two Hispanics the possibility of immediate promotion. Although 23% of those taking the test were African American, none scored high enough to qualify. Because of the possibility the tests were biased in violation of Title VII of the Civil Rights Act, no candidates were promoted pending outcome of the controversy. In a split 5-4 vote, the Supreme Court ruled that New Haven had engaged in impermissible racial discrimination against the White and Hispanic majority.</td>\n",
       "      <td>Where was the issue of White and Hispanic firefighters heard in the case based out of?</td>\n",
       "      <td>{'text': ['New Haven, Connecticut'], 'answer_start': [126]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5730855c396df91900096175</td>\n",
       "      <td>Kievan_Rus%27</td>\n",
       "      <td>The term \"Kievan Rus'\" (Ки́евская Русь Kievskaya Rus’) was coined in the 19th century in Russian historiography to refer to the period when the centre was in Kiev. In English, the term was introduced in the early 20th century, when it was found in the 1913 English translation of Vasily Klyuchevsky's A History of Russia, to distinguish the early polity from successor states, which were also named Rus. Later, the Russian term was rendered into Belarusian and Ukrainian as Кіеўская Русь Kijeŭskaja Rus’ and Ки́ївська Русь Kyivs'ka Rus’, respectively.</td>\n",
       "      <td>When did the term Kievan Rus first appear in English?</td>\n",
       "      <td>{'text': ['1913'], 'answer_start': [252]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>572825ad4b864d190016459c</td>\n",
       "      <td>Annelid</td>\n",
       "      <td>Since annelids are soft-bodied, their fossils are rare – mostly jaws and the mineralized tubes that some of the species secreted. Although some late Ediacaran fossils may represent annelids, the oldest known fossil that is identified with confidence comes from about 518 million years ago in the early Cambrian period. Fossils of most modern mobile polychaete groups appeared by the end of the Carboniferous, about 299 million years ago. Palaeontologists disagree about whether some body fossils from the mid Ordovician, about 472 to 461 million years ago, are the remains of oligochaetes, and the earliest indisputable fossils of the group appear in the Tertiary period, which began 65 million years ago.</td>\n",
       "      <td>How old is the earliest annelid fossil?</td>\n",
       "      <td>{'text': ['518 million years ago'], 'answer_start': [267]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>572e923bdfa6aa1500f8d15b</td>\n",
       "      <td>Neptune</td>\n",
       "      <td>Most languages today, even in countries that have no direct link to Greco-Roman culture, use some variant of the name \"Neptune\" for the planet. However, in Chinese, Japanese, and Korean, the planet's name was translated as \"sea king star\" (海王星), because Neptune was the god of the sea. In Mongolian, Neptune is called Dalain Van (Далайн ван), reflecting its namesake god's role as the ruler of the sea. In modern Greek the planet is called Poseidon (Ποσειδώνας, Poseidonas), the Greek counterpart of Neptune. In Hebrew, \"Rahab\" (רהב), from a Biblical sea monster mentioned in the Book of Psalms, was selected in a vote managed by the Academy of the Hebrew Language in 2009 as the official name for the planet, even though the existing Latin term \"Neptun\" (נפטון) is commonly used. In Māori, the planet is called Tangaroa, named after the Māori god of the sea. In Nahuatl, the planet is called Tlāloccītlalli, named after the rain god Tlāloc.</td>\n",
       "      <td>What is the Chinese, Japanese, and Korean translations for Neptune?</td>\n",
       "      <td>{'text': ['sea king star'], 'answer_start': [224]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:40:43.075508Z",
     "iopub.status.busy": "2024-01-08T12:40:43.074885Z",
     "iopub.status.idle": "2024-01-08T12:40:43.610518Z",
     "shell.execute_reply": "2024-01-08T12:40:43.609597Z",
     "shell.execute_reply.started": "2024-01-08T12:40:43.075454Z"
    },
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "以下断言确保我们的 Tokenizers 使用的是 FastTokenizer（Rust 实现，速度和功能性上有一定优势）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:40:48.393298Z",
     "iopub.status.busy": "2024-01-08T12:40:48.392811Z",
     "iopub.status.idle": "2024-01-08T12:40:48.399021Z",
     "shell.execute_reply": "2024-01-08T12:40:48.397876Z",
     "shell.execute_reply.started": "2024-01-08T12:40:48.393250Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以在大模型表上查看哪种类型的模型具有可用的快速标记器，哪种类型没有。\n",
    "\n",
    "您可以直接在两个句子上调用此标记器（一个用于答案，一个用于上下文）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:40:51.987812Z",
     "iopub.status.busy": "2024-01-08T12:40:51.987380Z",
     "iopub.status.idle": "2024-01-08T12:40:52.002743Z",
     "shell.execute_reply": "2024-01-08T12:40:52.001589Z",
     "shell.execute_reply.started": "2024-01-08T12:40:51.987770Z"
    },
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer 进阶操作\n",
    "\n",
    "在问答预处理中的一个特定问题是如何处理非常长的文档。\n",
    "\n",
    "在其他任务中，当文档的长度超过模型最大句子长度时，我们通常会截断它们，但在这里，删除上下文的一部分可能会导致我们丢失正在寻找的答案。\n",
    "\n",
    "为了解决这个问题，我们允许数据集中的一个（长）示例生成多个输入特征，每个特征的长度都小于模型的最大长度（或我们设置的超参数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:40:56.842141Z",
     "iopub.status.busy": "2024-01-08T12:40:56.841669Z",
     "iopub.status.idle": "2024-01-08T12:40:56.847648Z",
     "shell.execute_reply": "2024-01-08T12:40:56.846494Z",
     "shell.execute_reply.started": "2024-01-08T12:40:56.842096Z"
    }
   },
   "outputs": [],
   "source": [
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 超出最大长度的文本数据处理\n",
    "\n",
    "下面，我们从训练集中找出一个超过最大长度（384）的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:41:03.514046Z",
     "iopub.status.busy": "2024-01-08T12:41:03.513489Z",
     "iopub.status.idle": "2024-01-08T12:41:03.688940Z",
     "shell.execute_reply": "2024-01-08T12:41:03.688142Z",
     "shell.execute_reply.started": "2024-01-08T12:41:03.513999Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:41:07.512665Z",
     "iopub.status.busy": "2024-01-08T12:41:07.512464Z",
     "iopub.status.idle": "2024-01-08T12:41:07.518239Z",
     "shell.execute_reply": "2024-01-08T12:41:07.517532Z",
     "shell.execute_reply.started": "2024-01-08T12:41:07.512647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:41:10.559346Z",
     "iopub.status.busy": "2024-01-08T12:41:10.558890Z",
     "iopub.status.idle": "2024-01-08T12:41:10.569485Z",
     "shell.execute_reply": "2024-01-08T12:41:10.568312Z",
     "shell.execute_reply.started": "2024-01-08T12:41:10.559304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"],\n",
    "              example[\"context\"],\n",
    "              max_length=max_length,\n",
    "              truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于截断的策略\n",
    "\n",
    "- 直接截断超出部分: truncation=`only_second`\n",
    "- 仅截断上下文（context），保留问题（question）：`return_overflowing_tokens=True` & 设置`stride`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T13:14:24.806325Z",
     "iopub.status.busy": "2024-01-08T13:14:24.805781Z",
     "iopub.status.idle": "2024-01-08T13:14:24.816782Z",
     "shell.execute_reply": "2024-01-08T13:14:24.815660Z",
     "shell.execute_reply.started": "2024-01-08T13:14:24.806278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_example=\n",
      "{'input_ids': [[101, 20773, 2288, 2496, 1999, 2263, 2000, 3183, 1029, 102, 2006, 2258, 1018, 1010, 2263, 1010, 20773, 2496, 6108, 1062, 1012, 2016, 7271, 3936, 2037, 3510, 1999, 1037, 2678, 18318, 4270, 2012, 1996, 5962, 2283, 2005, 2014, 2353, 2996, 2201, 1010, 1045, 2572, 1012, 1012, 1012, 14673, 9205, 1010, 1999, 7128, 1005, 1055, 8412, 2252, 2006, 2255, 2570, 1010, 2263, 1012, 1045, 2572, 1012, 1012, 1012, 14673, 9205, 2001, 2207, 2006, 2281, 2324, 1010, 2263, 1999, 1996, 2142, 2163, 1012, 1996, 2201, 6246, 13999, 20773, 1005, 1055, 11477, 13059, 14673, 9205, 1010, 10141, 2076, 1996, 2437, 1997, 2014, 2494, 2309, 1000, 4689, 1999, 2293, 1000, 1010, 4855, 4466, 2475, 1010, 2199, 4809, 1999, 2049, 2034, 2733, 1010, 24469, 10234, 1996, 4908, 3263, 1010, 1998, 3228, 20773, 2014, 2353, 5486, 2193, 1011, 2028, 2201, 1999, 1996, 2149, 1012, 1996, 2201, 2956, 1996, 2193, 1011, 2028, 2299, 1000, 2309, 6456, 1006, 2404, 1037, 3614, 2006, 2009, 1007, 1000, 1998, 1996, 2327, 1011, 2274, 2774, 1000, 2065, 1045, 2020, 1037, 2879, 1000, 1998, 1000, 17201, 1000, 1012, 10910, 1996, 24718, 1997, 3352, 2014, 6493, 1011, 2770, 2980, 2531, 2309, 1999, 2014, 2476, 1010, 1000, 17201, 1000, 1005, 1055, 3112, 1999, 1996, 2149, 3271, 20773, 18759, 2062, 2327, 1011, 2702, 3895, 2006, 1996, 2862, 2084, 2151, 2060, 2450, 2076, 1996, 8876, 1012, 2009, 2036, 2443, 1996, 3144, 1000, 4086, 5544, 1000, 1010, 1998, 3895, 1000, 25992, 1000, 1010, 1000, 13059, 1000, 1010, 1000, 3714, 1011, 18627, 2611, 1000, 1998, 1000, 2678, 3042, 1000, 1012, 1996, 2189, 2678, 2005, 1000, 2309, 6456, 1000, 2038, 2042, 11968, 7716, 6340, 1998, 10047, 15198, 2105, 1996, 2088, 1010, 27957, 1996, 1000, 2034, 2350, 3153, 13675, 10936, 2063, 1000, 1997, 1996, 4274, 2287, 2429, 2000, 1996, 4361, 2732, 1012, 1996, 2678, 2038, 2180, 2195, 2982, 1010, 2164, 2190, 2678, 2012, 1996, 2268, 8692, 2885, 2189, 2982, 1010, 1996, 2268, 4104, 11240, 2080, 2982, 1010, 1998, 1996, 2268, 6655, 2982, 1012, 2012, 1996, 2268, 8692, 2678, 2189, 2982, 1010, 1996, 2678, 2001, 4222, 2005, 3157, 2982, 1010, 4821, 3045, 2093, 2164, 2678, 1997, 1996, 2095, 1012, 2049, 4945, 2000, 2663, 1996, 2190, 2931, 2678, 4696, 1010, 2029, 2253, 2000, 2137, 2406, 3769, 3220, 4202, 9170, 1005, 1055, 1000, 2017, 7141, 2007, 2033, 1000, 1010, 2419, 2000, 29270, 2225, 22602, 1996, 5103, 1998, 20773, 102], [101, 20773, 2288, 2496, 1999, 2263, 2000, 3183, 1029, 102, 2309, 6456, 1000, 2038, 2042, 11968, 7716, 6340, 1998, 10047, 15198, 2105, 1996, 2088, 1010, 27957, 1996, 1000, 2034, 2350, 3153, 13675, 10936, 2063, 1000, 1997, 1996, 4274, 2287, 2429, 2000, 1996, 4361, 2732, 1012, 1996, 2678, 2038, 2180, 2195, 2982, 1010, 2164, 2190, 2678, 2012, 1996, 2268, 8692, 2885, 2189, 2982, 1010, 1996, 2268, 4104, 11240, 2080, 2982, 1010, 1998, 1996, 2268, 6655, 2982, 1012, 2012, 1996, 2268, 8692, 2678, 2189, 2982, 1010, 1996, 2678, 2001, 4222, 2005, 3157, 2982, 1010, 4821, 3045, 2093, 2164, 2678, 1997, 1996, 2095, 1012, 2049, 4945, 2000, 2663, 1996, 2190, 2931, 2678, 4696, 1010, 2029, 2253, 2000, 2137, 2406, 3769, 3220, 4202, 9170, 1005, 1055, 1000, 2017, 7141, 2007, 2033, 1000, 1010, 2419, 2000, 29270, 2225, 22602, 1996, 5103, 1998, 20773, 17727, 12298, 9355, 1037, 2128, 1011, 8312, 1997, 9170, 1005, 1055, 2400, 2076, 2014, 2219, 9920, 4613, 1012, 1999, 2233, 2268, 1010, 20773, 11299, 2006, 1996, 1045, 2572, 1012, 1012, 1012, 2088, 2778, 1010, 2014, 2117, 26533, 4969, 4164, 2778, 1010, 5398, 1997, 10715, 3065, 1010, 18244, 1002, 13285, 1012, 1019, 2454, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "\n",
    "print(f\"tokenized_example=\\n{tokenized_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用此策略截断后，Tokenizer 将返回多个 `input_ids` 列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:41:23.174823Z",
     "iopub.status.busy": "2024-01-08T12:41:23.174353Z",
     "iopub.status.idle": "2024-01-08T12:41:23.182862Z",
     "shell.execute_reply": "2024-01-08T12:41:23.181605Z",
     "shell.execute_reply.started": "2024-01-08T12:41:23.174778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 192]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码两个输入特征，可以看到重叠的部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T12:41:29.440752Z",
     "iopub.status.busy": "2024-01-08T12:41:29.440281Z",
     "iopub.status.idle": "2024-01-08T12:41:33.899108Z",
     "shell.execute_reply": "2024-01-08T12:41:33.898360Z",
     "shell.execute_reply.started": "2024-01-08T12:41:29.440707Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 20:41:30.099148: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-08 20:41:30.099283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-08 20:41:30.127594: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-08 20:41:30.194013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-08 20:41:31.199844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] beyonce got married in 2008 to whom? [SEP] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \"'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce [SEP]\n",
      "[CLS] beyonce got married in 2008 to whom? [SEP] single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce improvising a re - presentation of swift's award during her own acceptance speech. in march 2009, beyonce embarked on the i am... world tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $ 119. 5 million. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 offsets_mapping 获取原始的 input_ids\n",
    "\n",
    "设置 `return_offsets_mapping=True`，将使得截断分割生成的多个 input_ids 列表中的 token，通过映射保留原始文本的 input_ids。\n",
    "\n",
    "如下所示：第一个标记（[CLS]）的起始和结束字符都是（0, 0），因为它不对应问题/答案的任何部分，然后第二个标记与问题(question)的字符0到3相同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T13:49:14.009222Z",
     "iopub.status.busy": "2024-01-08T13:49:14.008689Z",
     "iopub.status.idle": "2024-01-08T13:49:14.021834Z",
     "shell.execute_reply": "2024-01-08T13:49:14.020900Z",
     "shell.execute_reply.started": "2024-01-08T13:49:14.009171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拆分后的第1段\"offset_mapping\":\n",
      "[(0, 0), (0, 7), (8, 11), (12, 19), (20, 22), (23, 27), (28, 30), (31, 35), (35, 36), (0, 0), (0, 2), (3, 8), (9, 10), (10, 11), (12, 16), (16, 17), (18, 25), (26, 33), (34, 37), (38, 39), (39, 40), (41, 44), (45, 53), (54, 62), (63, 68), (69, 77), (78, 80), (81, 82), (83, 88), (89, 93), (93, 96), (97, 99), (100, 103), (104, 113), (114, 119), (120, 123), (124, 127), (128, 133), (134, 140), (141, 146), (146, 147), (148, 149), (150, 152), (152, 153), (153, 154), (154, 155), (156, 161), (162, 168), (168, 169), (170, 172), (173, 182), (182, 183), (183, 184), (185, 189), (190, 194), (195, 197), (198, 205), (206, 208), (208, 209), (210, 214), (214, 215), (216, 217), (218, 220), (220, 221), (221, 222), (222, 223), (224, 229), (230, 236), (237, 240), (241, 249), (250, 252), (253, 261), (262, 264), (264, 265), (266, 270), (271, 273), (274, 277), (278, 284), (285, 291), (291, 292), (293, 296), (297, 302), (303, 311), (312, 322), (323, 330), (330, 331), (331, 332), (333, 338), (339, 342), (343, 348), (349, 355), (355, 356), (357, 366), (367, 373), (374, 377), (378, 384), (385, 387), (388, 391), (392, 396), (397, 403), (404, 405), (405, 410), (411, 413), (414, 418), (418, 419), (419, 420), (421, 428), (429, 431), (431, 432), (432, 433), (433, 436), (437, 443), (444, 446), (447, 450), (451, 456), (457, 461), (461, 462), (463, 471), (472, 476), (477, 480), (481, 490), (491, 494), (494, 495), (496, 499), (500, 506), (507, 514), (515, 518), (519, 524), (525, 536), (537, 543), (543, 544), (544, 547), (548, 553), (554, 556), (557, 560), (561, 563), (563, 564), (565, 568), (569, 574), (575, 583), (584, 587), (588, 594), (594, 595), (595, 598), (599, 603), (604, 605), (605, 611), (612, 618), (619, 620), (620, 623), (624, 625), (626, 630), (631, 633), (634, 636), (636, 637), (637, 638), (639, 642), (643, 646), (647, 650), (650, 651), (651, 655), (656, 661), (662, 663), (663, 665), (666, 667), (668, 672), (673, 674), (675, 678), (678, 679), (680, 683), (684, 685), (685, 689), (689, 690), (690, 691), (692, 701), (702, 705), (706, 720), (721, 723), (724, 732), (733, 736), (737, 744), (744, 745), (745, 752), (753, 756), (757, 760), (761, 767), (768, 770), (771, 774), (775, 781), (781, 782), (783, 784), (784, 788), (788, 789), (789, 790), (790, 791), (792, 799), (800, 802), (803, 806), (807, 809), (810, 816), (817, 824), (825, 831), (832, 836), (837, 840), (840, 841), (841, 844), (845, 852), (853, 855), (856, 859), (860, 864), (865, 869), (870, 873), (874, 879), (880, 885), (886, 892), (893, 896), (897, 902), (902, 903), (904, 906), (907, 911), (912, 920), (921, 924), (925, 935), (936, 937), (937, 942), (943, 949), (949, 950), (950, 951), (952, 955), (956, 963), (964, 965), (965, 969), (969, 970), (970, 971), (972, 973), (973, 976), (976, 977), (977, 978), (979, 980), (980, 986), (986, 987), (987, 994), (995, 999), (999, 1000), (1001, 1004), (1005, 1006), (1006, 1011), (1012, 1017), (1017, 1018), (1018, 1019), (1020, 1023), (1024, 1029), (1030, 1035), (1036, 1039), (1040, 1041), (1041, 1047), (1048, 1054), (1054, 1055), (1056, 1059), (1060, 1064), (1065, 1068), (1068, 1070), (1070, 1073), (1074, 1077), (1078, 1080), (1080, 1086), (1087, 1093), (1094, 1097), (1098, 1103), (1103, 1104), (1105, 1113), (1114, 1117), (1118, 1119), (1119, 1124), (1125, 1130), (1131, 1136), (1137, 1139), (1139, 1141), (1141, 1142), (1142, 1143), (1144, 1146), (1147, 1150), (1151, 1159), (1160, 1163), (1164, 1173), (1174, 1176), (1177, 1180), (1181, 1188), (1189, 1193), (1193, 1194), (1195, 1198), (1199, 1204), (1205, 1208), (1209, 1212), (1213, 1220), (1221, 1227), (1227, 1228), (1229, 1238), (1239, 1243), (1244, 1249), (1250, 1252), (1253, 1256), (1257, 1261), (1262, 1265), (1266, 1272), (1273, 1278), (1279, 1285), (1285, 1286), (1287, 1290), (1291, 1295), (1296, 1304), (1305, 1308), (1308, 1309), (1310, 1316), (1316, 1317), (1318, 1321), (1322, 1325), (1326, 1330), (1331, 1334), (1335, 1341), (1341, 1342), (1343, 1345), (1346, 1349), (1350, 1354), (1355, 1358), (1359, 1364), (1365, 1370), (1371, 1377), (1377, 1378), (1379, 1382), (1383, 1388), (1389, 1392), (1393, 1402), (1403, 1406), (1407, 1411), (1412, 1418), (1418, 1419), (1420, 1430), (1431, 1438), (1439, 1444), (1445, 1454), (1455, 1460), (1461, 1463), (1464, 1467), (1468, 1472), (1472, 1473), (1474, 1477), (1478, 1485), (1486, 1488), (1489, 1492), (1493, 1496), (1497, 1501), (1502, 1508), (1509, 1514), (1515, 1523), (1523, 1524), (1525, 1530), (1531, 1535), (1536, 1538), (1539, 1547), (1548, 1555), (1556, 1559), (1560, 1566), (1567, 1573), (1574, 1579), (1579, 1580), (1580, 1581), (1582, 1583), (1583, 1586), (1587, 1593), (1594, 1598), (1599, 1601), (1601, 1602), (1602, 1603), (1604, 1607), (1608, 1610), (1611, 1616), (1617, 1621), (1622, 1634), (1635, 1638), (1639, 1647), (1648, 1651), (1652, 1659), (0, 0)]\n",
      "拆分后的第2段\"offset_mapping\":\n",
      "[(0, 0), (0, 7), (8, 11), (12, 19), (20, 22), (23, 27), (28, 30), (31, 35), (35, 36), (0, 0), (1041, 1047), (1048, 1054), (1054, 1055), (1056, 1059), (1060, 1064), (1065, 1068), (1068, 1070), (1070, 1073), (1074, 1077), (1078, 1080), (1080, 1086), (1087, 1093), (1094, 1097), (1098, 1103), (1103, 1104), (1105, 1113), (1114, 1117), (1118, 1119), (1119, 1124), (1125, 1130), (1131, 1136), (1137, 1139), (1139, 1141), (1141, 1142), (1142, 1143), (1144, 1146), (1147, 1150), (1151, 1159), (1160, 1163), (1164, 1173), (1174, 1176), (1177, 1180), (1181, 1188), (1189, 1193), (1193, 1194), (1195, 1198), (1199, 1204), (1205, 1208), (1209, 1212), (1213, 1220), (1221, 1227), (1227, 1228), (1229, 1238), (1239, 1243), (1244, 1249), (1250, 1252), (1253, 1256), (1257, 1261), (1262, 1265), (1266, 1272), (1273, 1278), (1279, 1285), (1285, 1286), (1287, 1290), (1291, 1295), (1296, 1304), (1305, 1308), (1308, 1309), (1310, 1316), (1316, 1317), (1318, 1321), (1322, 1325), (1326, 1330), (1331, 1334), (1335, 1341), (1341, 1342), (1343, 1345), (1346, 1349), (1350, 1354), (1355, 1358), (1359, 1364), (1365, 1370), (1371, 1377), (1377, 1378), (1379, 1382), (1383, 1388), (1389, 1392), (1393, 1402), (1403, 1406), (1407, 1411), (1412, 1418), (1418, 1419), (1420, 1430), (1431, 1438), (1439, 1444), (1445, 1454), (1455, 1460), (1461, 1463), (1464, 1467), (1468, 1472), (1472, 1473), (1474, 1477), (1478, 1485), (1486, 1488), (1489, 1492), (1493, 1496), (1497, 1501), (1502, 1508), (1509, 1514), (1515, 1523), (1523, 1524), (1525, 1530), (1531, 1535), (1536, 1538), (1539, 1547), (1548, 1555), (1556, 1559), (1560, 1566), (1567, 1573), (1574, 1579), (1579, 1580), (1580, 1581), (1582, 1583), (1583, 1586), (1587, 1593), (1594, 1598), (1599, 1601), (1601, 1602), (1602, 1603), (1604, 1607), (1608, 1610), (1611, 1616), (1617, 1621), (1622, 1634), (1635, 1638), (1639, 1647), (1648, 1651), (1652, 1659), (1660, 1663), (1663, 1666), (1666, 1671), (1672, 1673), (1674, 1676), (1676, 1677), (1677, 1689), (1690, 1692), (1693, 1698), (1698, 1699), (1699, 1700), (1701, 1706), (1707, 1713), (1714, 1717), (1718, 1721), (1722, 1732), (1733, 1739), (1739, 1740), (1741, 1743), (1744, 1749), (1750, 1754), (1754, 1755), (1756, 1763), (1764, 1772), (1773, 1775), (1776, 1779), (1780, 1781), (1782, 1784), (1784, 1785), (1785, 1786), (1786, 1787), (1788, 1793), (1794, 1798), (1798, 1799), (1800, 1803), (1804, 1810), (1811, 1821), (1822, 1831), (1832, 1839), (1840, 1844), (1844, 1845), (1846, 1856), (1857, 1859), (1860, 1863), (1864, 1869), (1869, 1870), (1871, 1879), (1880, 1881), (1881, 1884), (1884, 1885), (1885, 1886), (1887, 1894), (1894, 1895), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(\"拆分后的第1段\\\"offset_mapping\\\":\")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:])\n",
    "print(\"拆分后的第2段\\\"offset_mapping\\\":\")\n",
    "print(tokenized_example[\"offset_mapping\"][1][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们可以使用这个映射来找到答案在给定特征中的起始和结束标记的位置。\n",
    "\n",
    "我们只需区分偏移的哪些部分对应于问题，哪些部分对应于上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T13:49:17.454702Z",
     "iopub.status.busy": "2024-01-08T13:49:17.454330Z",
     "iopub.status.idle": "2024-01-08T13:49:17.462248Z",
     "shell.execute_reply": "2024-01-08T13:49:17.461091Z",
     "shell.execute_reply.started": "2024-01-08T13:49:17.454666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyonce got married in 2008 to whom?\n",
      "Decode first_token_id:  beyonce\n",
      "beyonce Beyonce\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(example[\"question\"])\n",
    "print(\"Decode first_token_id: \", tokenizer.decode(first_token_id))\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "借助`tokenized_example`的`sequence_ids`方法，我们可以方便的区分token的来源编号：\n",
    "\n",
    "- 对于特殊标记：返回None，\n",
    "- 对于正文Token：返回句子编号（从0开始编号）。\n",
    "\n",
    "综上，现在我们可以很方便的在一个输入特征中找到答案的起始和结束 Token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T13:51:59.950885Z",
     "iopub.status.busy": "2024-01-08T13:51:59.950388Z",
     "iopub.status.idle": "2024-01-08T13:51:59.958504Z",
     "shell.execute_reply": "2024-01-08T13:51:59.957406Z",
     "shell.execute_reply.started": "2024-01-08T13:51:59.950841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_example = \n",
      "{'input_ids': [[101, 20773, 2288, 2496, 1999, 2263, 2000, 3183, 1029, 102, 2006, 2258, 1018, 1010, 2263, 1010, 20773, 2496, 6108, 1062, 1012, 2016, 7271, 3936, 2037, 3510, 1999, 1037, 2678, 18318, 4270, 2012, 1996, 5962, 2283, 2005, 2014, 2353, 2996, 2201, 1010, 1045, 2572, 1012, 1012, 1012, 14673, 9205, 1010, 1999, 7128, 1005, 1055, 8412, 2252, 2006, 2255, 2570, 1010, 2263, 1012, 1045, 2572, 1012, 1012, 1012, 14673, 9205, 2001, 2207, 2006, 2281, 2324, 1010, 2263, 1999, 1996, 2142, 2163, 1012, 1996, 2201, 6246, 13999, 20773, 1005, 1055, 11477, 13059, 14673, 9205, 1010, 10141, 2076, 1996, 2437, 1997, 2014, 2494, 2309, 1000, 4689, 1999, 2293, 1000, 1010, 4855, 4466, 2475, 1010, 2199, 4809, 1999, 2049, 2034, 2733, 1010, 24469, 10234, 1996, 4908, 3263, 1010, 1998, 3228, 20773, 2014, 2353, 5486, 2193, 1011, 2028, 2201, 1999, 1996, 2149, 1012, 1996, 2201, 2956, 1996, 2193, 1011, 2028, 2299, 1000, 2309, 6456, 1006, 2404, 1037, 3614, 2006, 2009, 1007, 1000, 1998, 1996, 2327, 1011, 2274, 2774, 1000, 2065, 1045, 2020, 1037, 2879, 1000, 1998, 1000, 17201, 1000, 1012, 10910, 1996, 24718, 1997, 3352, 2014, 6493, 1011, 2770, 2980, 2531, 2309, 1999, 2014, 2476, 1010, 1000, 17201, 1000, 1005, 1055, 3112, 1999, 1996, 2149, 3271, 20773, 18759, 2062, 2327, 1011, 2702, 3895, 2006, 1996, 2862, 2084, 2151, 2060, 2450, 2076, 1996, 8876, 1012, 2009, 2036, 2443, 1996, 3144, 1000, 4086, 5544, 1000, 1010, 1998, 3895, 1000, 25992, 1000, 1010, 1000, 13059, 1000, 1010, 1000, 3714, 1011, 18627, 2611, 1000, 1998, 1000, 2678, 3042, 1000, 1012, 1996, 2189, 2678, 2005, 1000, 2309, 6456, 1000, 2038, 2042, 11968, 7716, 6340, 1998, 10047, 15198, 2105, 1996, 2088, 1010, 27957, 1996, 1000, 2034, 2350, 3153, 13675, 10936, 2063, 1000, 1997, 1996, 4274, 2287, 2429, 2000, 1996, 4361, 2732, 1012, 1996, 2678, 2038, 2180, 2195, 2982, 1010, 2164, 2190, 2678, 2012, 1996, 2268, 8692, 2885, 2189, 2982, 1010, 1996, 2268, 4104, 11240, 2080, 2982, 1010, 1998, 1996, 2268, 6655, 2982, 1012, 2012, 1996, 2268, 8692, 2678, 2189, 2982, 1010, 1996, 2678, 2001, 4222, 2005, 3157, 2982, 1010, 4821, 3045, 2093, 2164, 2678, 1997, 1996, 2095, 1012, 2049, 4945, 2000, 2663, 1996, 2190, 2931, 2678, 4696, 1010, 2029, 2253, 2000, 2137, 2406, 3769, 3220, 4202, 9170, 1005, 1055, 1000, 2017, 7141, 2007, 2033, 1000, 1010, 2419, 2000, 29270, 2225, 22602, 1996, 5103, 1998, 20773, 102], [101, 20773, 2288, 2496, 1999, 2263, 2000, 3183, 1029, 102, 2309, 6456, 1000, 2038, 2042, 11968, 7716, 6340, 1998, 10047, 15198, 2105, 1996, 2088, 1010, 27957, 1996, 1000, 2034, 2350, 3153, 13675, 10936, 2063, 1000, 1997, 1996, 4274, 2287, 2429, 2000, 1996, 4361, 2732, 1012, 1996, 2678, 2038, 2180, 2195, 2982, 1010, 2164, 2190, 2678, 2012, 1996, 2268, 8692, 2885, 2189, 2982, 1010, 1996, 2268, 4104, 11240, 2080, 2982, 1010, 1998, 1996, 2268, 6655, 2982, 1012, 2012, 1996, 2268, 8692, 2678, 2189, 2982, 1010, 1996, 2678, 2001, 4222, 2005, 3157, 2982, 1010, 4821, 3045, 2093, 2164, 2678, 1997, 1996, 2095, 1012, 2049, 4945, 2000, 2663, 1996, 2190, 2931, 2678, 4696, 1010, 2029, 2253, 2000, 2137, 2406, 3769, 3220, 4202, 9170, 1005, 1055, 1000, 2017, 7141, 2007, 2033, 1000, 1010, 2419, 2000, 29270, 2225, 22602, 1996, 5103, 1998, 20773, 17727, 12298, 9355, 1037, 2128, 1011, 8312, 1997, 9170, 1005, 1055, 2400, 2076, 2014, 2219, 9920, 4613, 1012, 1999, 2233, 2268, 1010, 20773, 11299, 2006, 1996, 1045, 2572, 1012, 1012, 1012, 2088, 2778, 1010, 2014, 2117, 26533, 4969, 4164, 2778, 1010, 5398, 1997, 10715, 3065, 1010, 18244, 1002, 13285, 1012, 1019, 2454, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 7), (8, 11), (12, 19), (20, 22), (23, 27), (28, 30), (31, 35), (35, 36), (0, 0), (0, 2), (3, 8), (9, 10), (10, 11), (12, 16), (16, 17), (18, 25), (26, 33), (34, 37), (38, 39), (39, 40), (41, 44), (45, 53), (54, 62), (63, 68), (69, 77), (78, 80), (81, 82), (83, 88), (89, 93), (93, 96), (97, 99), (100, 103), (104, 113), (114, 119), (120, 123), (124, 127), (128, 133), (134, 140), (141, 146), (146, 147), (148, 149), (150, 152), (152, 153), (153, 154), (154, 155), (156, 161), (162, 168), (168, 169), (170, 172), (173, 182), (182, 183), (183, 184), (185, 189), (190, 194), (195, 197), (198, 205), (206, 208), (208, 209), (210, 214), (214, 215), (216, 217), (218, 220), (220, 221), (221, 222), (222, 223), (224, 229), (230, 236), (237, 240), (241, 249), (250, 252), (253, 261), (262, 264), (264, 265), (266, 270), (271, 273), (274, 277), (278, 284), (285, 291), (291, 292), (293, 296), (297, 302), (303, 311), (312, 322), (323, 330), (330, 331), (331, 332), (333, 338), (339, 342), (343, 348), (349, 355), (355, 356), (357, 366), (367, 373), (374, 377), (378, 384), (385, 387), (388, 391), (392, 396), (397, 403), (404, 405), (405, 410), (411, 413), (414, 418), (418, 419), (419, 420), (421, 428), (429, 431), (431, 432), (432, 433), (433, 436), (437, 443), (444, 446), (447, 450), (451, 456), (457, 461), (461, 462), (463, 471), (472, 476), (477, 480), (481, 490), (491, 494), (494, 495), (496, 499), (500, 506), (507, 514), (515, 518), (519, 524), (525, 536), (537, 543), (543, 544), (544, 547), (548, 553), (554, 556), (557, 560), (561, 563), (563, 564), (565, 568), (569, 574), (575, 583), (584, 587), (588, 594), (594, 595), (595, 598), (599, 603), (604, 605), (605, 611), (612, 618), (619, 620), (620, 623), (624, 625), (626, 630), (631, 633), (634, 636), (636, 637), (637, 638), (639, 642), (643, 646), (647, 650), (650, 651), (651, 655), (656, 661), (662, 663), (663, 665), (666, 667), (668, 672), (673, 674), (675, 678), (678, 679), (680, 683), (684, 685), (685, 689), (689, 690), (690, 691), (692, 701), (702, 705), (706, 720), (721, 723), (724, 732), (733, 736), (737, 744), (744, 745), (745, 752), (753, 756), (757, 760), (761, 767), (768, 770), (771, 774), (775, 781), (781, 782), (783, 784), (784, 788), (788, 789), (789, 790), (790, 791), (792, 799), (800, 802), (803, 806), (807, 809), (810, 816), (817, 824), (825, 831), (832, 836), (837, 840), (840, 841), (841, 844), (845, 852), (853, 855), (856, 859), (860, 864), (865, 869), (870, 873), (874, 879), (880, 885), (886, 892), (893, 896), (897, 902), (902, 903), (904, 906), (907, 911), (912, 920), (921, 924), (925, 935), (936, 937), (937, 942), (943, 949), (949, 950), (950, 951), (952, 955), (956, 963), (964, 965), (965, 969), (969, 970), (970, 971), (972, 973), (973, 976), (976, 977), (977, 978), (979, 980), (980, 986), (986, 987), (987, 994), (995, 999), (999, 1000), (1001, 1004), (1005, 1006), (1006, 1011), (1012, 1017), (1017, 1018), (1018, 1019), (1020, 1023), (1024, 1029), (1030, 1035), (1036, 1039), (1040, 1041), (1041, 1047), (1048, 1054), (1054, 1055), (1056, 1059), (1060, 1064), (1065, 1068), (1068, 1070), (1070, 1073), (1074, 1077), (1078, 1080), (1080, 1086), (1087, 1093), (1094, 1097), (1098, 1103), (1103, 1104), (1105, 1113), (1114, 1117), (1118, 1119), (1119, 1124), (1125, 1130), (1131, 1136), (1137, 1139), (1139, 1141), (1141, 1142), (1142, 1143), (1144, 1146), (1147, 1150), (1151, 1159), (1160, 1163), (1164, 1173), (1174, 1176), (1177, 1180), (1181, 1188), (1189, 1193), (1193, 1194), (1195, 1198), (1199, 1204), (1205, 1208), (1209, 1212), (1213, 1220), (1221, 1227), (1227, 1228), (1229, 1238), (1239, 1243), (1244, 1249), (1250, 1252), (1253, 1256), (1257, 1261), (1262, 1265), (1266, 1272), (1273, 1278), (1279, 1285), (1285, 1286), (1287, 1290), (1291, 1295), (1296, 1304), (1305, 1308), (1308, 1309), (1310, 1316), (1316, 1317), (1318, 1321), (1322, 1325), (1326, 1330), (1331, 1334), (1335, 1341), (1341, 1342), (1343, 1345), (1346, 1349), (1350, 1354), (1355, 1358), (1359, 1364), (1365, 1370), (1371, 1377), (1377, 1378), (1379, 1382), (1383, 1388), (1389, 1392), (1393, 1402), (1403, 1406), (1407, 1411), (1412, 1418), (1418, 1419), (1420, 1430), (1431, 1438), (1439, 1444), (1445, 1454), (1455, 1460), (1461, 1463), (1464, 1467), (1468, 1472), (1472, 1473), (1474, 1477), (1478, 1485), (1486, 1488), (1489, 1492), (1493, 1496), (1497, 1501), (1502, 1508), (1509, 1514), (1515, 1523), (1523, 1524), (1525, 1530), (1531, 1535), (1536, 1538), (1539, 1547), (1548, 1555), (1556, 1559), (1560, 1566), (1567, 1573), (1574, 1579), (1579, 1580), (1580, 1581), (1582, 1583), (1583, 1586), (1587, 1593), (1594, 1598), (1599, 1601), (1601, 1602), (1602, 1603), (1604, 1607), (1608, 1610), (1611, 1616), (1617, 1621), (1622, 1634), (1635, 1638), (1639, 1647), (1648, 1651), (1652, 1659), (0, 0)], [(0, 0), (0, 7), (8, 11), (12, 19), (20, 22), (23, 27), (28, 30), (31, 35), (35, 36), (0, 0), (1041, 1047), (1048, 1054), (1054, 1055), (1056, 1059), (1060, 1064), (1065, 1068), (1068, 1070), (1070, 1073), (1074, 1077), (1078, 1080), (1080, 1086), (1087, 1093), (1094, 1097), (1098, 1103), (1103, 1104), (1105, 1113), (1114, 1117), (1118, 1119), (1119, 1124), (1125, 1130), (1131, 1136), (1137, 1139), (1139, 1141), (1141, 1142), (1142, 1143), (1144, 1146), (1147, 1150), (1151, 1159), (1160, 1163), (1164, 1173), (1174, 1176), (1177, 1180), (1181, 1188), (1189, 1193), (1193, 1194), (1195, 1198), (1199, 1204), (1205, 1208), (1209, 1212), (1213, 1220), (1221, 1227), (1227, 1228), (1229, 1238), (1239, 1243), (1244, 1249), (1250, 1252), (1253, 1256), (1257, 1261), (1262, 1265), (1266, 1272), (1273, 1278), (1279, 1285), (1285, 1286), (1287, 1290), (1291, 1295), (1296, 1304), (1305, 1308), (1308, 1309), (1310, 1316), (1316, 1317), (1318, 1321), (1322, 1325), (1326, 1330), (1331, 1334), (1335, 1341), (1341, 1342), (1343, 1345), (1346, 1349), (1350, 1354), (1355, 1358), (1359, 1364), (1365, 1370), (1371, 1377), (1377, 1378), (1379, 1382), (1383, 1388), (1389, 1392), (1393, 1402), (1403, 1406), (1407, 1411), (1412, 1418), (1418, 1419), (1420, 1430), (1431, 1438), (1439, 1444), (1445, 1454), (1455, 1460), (1461, 1463), (1464, 1467), (1468, 1472), (1472, 1473), (1474, 1477), (1478, 1485), (1486, 1488), (1489, 1492), (1493, 1496), (1497, 1501), (1502, 1508), (1509, 1514), (1515, 1523), (1523, 1524), (1525, 1530), (1531, 1535), (1536, 1538), (1539, 1547), (1548, 1555), (1556, 1559), (1560, 1566), (1567, 1573), (1574, 1579), (1579, 1580), (1580, 1581), (1582, 1583), (1583, 1586), (1587, 1593), (1594, 1598), (1599, 1601), (1601, 1602), (1602, 1603), (1604, 1607), (1608, 1610), (1611, 1616), (1617, 1621), (1622, 1634), (1635, 1638), (1639, 1647), (1648, 1651), (1652, 1659), (1660, 1663), (1663, 1666), (1666, 1671), (1672, 1673), (1674, 1676), (1676, 1677), (1677, 1689), (1690, 1692), (1693, 1698), (1698, 1699), (1699, 1700), (1701, 1706), (1707, 1713), (1714, 1717), (1718, 1721), (1722, 1732), (1733, 1739), (1739, 1740), (1741, 1743), (1744, 1749), (1750, 1754), (1754, 1755), (1756, 1763), (1764, 1772), (1773, 1775), (1776, 1779), (1780, 1781), (1782, 1784), (1784, 1785), (1785, 1786), (1786, 1787), (1788, 1793), (1794, 1798), (1798, 1799), (1800, 1803), (1804, 1810), (1811, 1821), (1822, 1831), (1832, 1839), (1840, 1844), (1844, 1845), (1846, 1856), (1857, 1859), (1860, 1863), (1864, 1869), (1869, 1870), (1871, 1879), (1880, 1881), (1881, 1884), (1884, 1885), (1885, 1886), (1887, 1894), (1894, 1895), (0, 0)]], 'overflow_to_sample_mapping': [0, 0]}\n",
      "sequence_ids = \n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tokenized_example = \\n{tokenized_example}\")\n",
    "\n",
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(f\"sequence_ids = \\n{sequence_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T14:05:01.736722Z",
     "iopub.status.busy": "2024-01-08T14:05:01.736185Z",
     "iopub.status.idle": "2024-01-08T14:05:01.749799Z",
     "shell.execute_reply": "2024-01-08T14:05:01.748774Z",
     "shell.execute_reply.started": "2024-01-08T14:05:01.736675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers = \n",
      "{'text': ['Jay Z'], 'answer_start': [34]}\n",
      "start_char=34, end_char=39\n",
      "token_start_index=10, token_end_index=382\n",
      "start_position=18, end_position=19\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "print(f\"answers = \\n{answers}\")\n",
    "\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "print(f\"start_char={start_char}, end_char={end_char}\")\n",
    "\n",
    "# 当前span在文本中的起始标记索引。\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# 当前span在文本中的结束标记索引。\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "    \n",
    "print(f\"token_start_index={token_start_index}, token_end_index={token_end_index}\")\n",
    "\n",
    "# 检测答案是否超出span范围（如果超出范围，该特征将以CLS标记索引标记）。\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # 将token_start_index和token_end_index移动到答案的两端。\n",
    "    # 注意：如果答案是最后一个单词，我们可以移到最后一个标记之后（边界情况）。\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(f\"start_position={start_position}, end_position={end_position}\")\n",
    "else:\n",
    "    print(\"答案不在此特征中。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印检查是否准确找到了起始位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T14:05:05.597198Z",
     "iopub.status.busy": "2024-01-08T14:05:05.596759Z",
     "iopub.status.idle": "2024-01-08T14:05:05.603396Z",
     "shell.execute_reply": "2024-01-08T14:05:05.602434Z",
     "shell.execute_reply.started": "2024-01-08T14:05:05.597159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jay z\n",
      "Jay Z\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于填充的策略\n",
    "\n",
    "- 对于没有超过最大长度的文本，填充补齐长度。\n",
    "- 对于需要左侧填充的模型，交换 question 和 context 顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T14:05:09.276210Z",
     "iopub.status.busy": "2024-01-08T14:05:09.275685Z",
     "iopub.status.idle": "2024-01-08T14:05:09.282120Z",
     "shell.execute_reply": "2024-01-08T14:05:09.280911Z",
     "shell.execute_reply.started": "2024-01-08T14:05:09.276163Z"
    }
   },
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整合以上所有预处理步骤\n",
    "\n",
    "让我们将所有内容整合到一个函数中，并将其应用到训练集。\n",
    "\n",
    "针对不可回答的情况（上下文过长，答案在另一个特征中），我们为开始和结束位置都设置了cls索引。\n",
    "\n",
    "如果allow_impossible_answers标志为False，我们还可以简单地从训练集中丢弃这些示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T14:05:13.550019Z",
     "iopub.status.busy": "2024-01-08T14:05:13.549469Z",
     "iopub.status.idle": "2024-01-08T14:05:13.565791Z",
     "shell.execute_reply": "2024-01-08T14:05:13.564854Z",
     "shell.execute_reply.started": "2024-01-08T14:05:13.549978Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # 一些问题的左侧可能有很多空白字符，这对我们没有用，而且会导致上下文的截断失败\n",
    "    # （标记化的问题将占用大量空间）。因此，我们删除左侧的空白字符。\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    #print(f\"examples=\\n{examples}\")\n",
    "\n",
    "    # 使用截断和填充对我们的示例进行标记化，但保留溢出部分，使用步幅（stride）。\n",
    "    # 当上下文很长时，这会导致一个示例可能提供多个特征，其中每个特征的上下文都与前一个特征的上下文有一些重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    #print(f\"tokenized_examples=\\n{tokenized_examples}\")\n",
    "\n",
    "    # 由于一个示例可能给我们提供多个特征（如果它具有很长的上下文），我们需要一个从特征到其对应示例的映射。这个键就提供了这个映射关系。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # 偏移映射将为我们提供从令牌到原始上下文中的字符位置的映射。这将帮助我们计算开始位置和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 让我们为这些示例进行标记！\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 我们将使用CLS令牌的索引来标记不可能的答案。\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 获取与该示例对应的序列（以了解上下文和问题是什么）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 一个示例可以提供多个跨度，这是包含此文本跨度的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        #print(f\"sample_mapping=\\n{sample_mapping}\")\n",
    "        #print(f\"answers=\\n{answers}\")\n",
    "        \n",
    "        # 如果没有给出答案，则将cls_index设置为答案。\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案在文本中的开始和结束字符索引。\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # 当前跨度在文本中的开始令牌索引。\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 当前跨度在文本中的结束令牌索引。\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 检测答案是否超出跨度（在这种情况下，该特征的标签将使用CLS索引）。\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 否则，将token_start_index和token_end_index移到答案的两端。\n",
    "                # 注意：如果答案是最后一个单词（边缘情况），我们可以在最后一个偏移之后继续。\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T14:05:18.518867Z",
     "iopub.status.busy": "2024-01-08T14:05:18.518486Z",
     "iopub.status.idle": "2024-01-08T14:05:18.546615Z",
     "shell.execute_reply": "2024-01-08T14:05:18.545703Z",
     "shell.execute_reply.started": "2024-01-08T14:05:18.518832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_datasets = \n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 测试 prepare_train_features 方法\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "datasets_sample = datasets[\"train\"].select(range(3))\n",
    "\n",
    "tokenized_datasets = datasets_sample.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)\n",
    "\n",
    "print(f\"tokenized_datasets = \\n{tokenized_datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "#### datasets.map 的进阶使用\n",
    "\n",
    "使用 `datasets.map` 方法将 `prepare_train_features` 应用于所有训练、验证和测试数据：\n",
    "\n",
    "- batched: 批量处理数据。\n",
    "- remove_columns: 因为预处理更改了样本的数量，所以在应用它时需要删除旧列。\n",
    "- load_from_cache_file：是否使用datasets库的自动缓存\n",
    "\n",
    "datasets 库针对大规模数据，实现了高效缓存机制，能够自动检测传递给 map 的函数是否已更改（因此需要不使用缓存数据）。如果在调用 map 时设置 `load_from_cache_file=False`，可以强制重新应用预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T14:05:35.655587Z",
     "iopub.status.busy": "2024-01-08T14:05:35.655113Z",
     "iopub.status.idle": "2024-01-08T14:06:09.189665Z",
     "shell.execute_reply": "2024-01-08T14:06:09.188977Z",
     "shell.execute_reply.started": "2024-01-08T14:05:35.655543Z"
    },
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets=\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1177bc17053e4f39b629efb4f72b26d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_datasets=\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "        num_rows: 131754\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "        num_rows: 12134\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"datasets=\\n{datasets}\")\n",
    "\n",
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)\n",
    "\n",
    "print(f\"tokenized_datasets=\\n{tokenized_datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## 微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "现在我们的数据已经准备好用于训练，我们可以下载预训练模型并进行微调。\n",
    "\n",
    "由于我们的任务是问答，我们使用 `AutoModelForQuestionAnswering` 类。(对比 Yelp 评论打分使用的是 `AutoModelForSequenceClassification` 类）\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T14:06:20.822764Z",
     "iopub.status.busy": "2024-01-08T14:06:20.822351Z",
     "iopub.status.idle": "2024-01-08T14:06:31.082694Z",
     "shell.execute_reply": "2024-01-08T14:06:31.081991Z",
     "shell.execute_reply.started": "2024-01-08T14:06:20.822729Z"
    },
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "#### 训练超参数（TrainingArguments）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T14:08:50.833935Z",
     "iopub.status.busy": "2024-01-08T14:08:50.833053Z",
     "iopub.status.idle": "2024-01-08T14:08:50.935451Z",
     "shell.execute_reply": "2024-01-08T14:08:50.934589Z",
     "shell.execute_reply.started": "2024-01-08T14:08:50.833885Z"
    },
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "model_dir = \"models\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_dir}/{model_name}-finetuned-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collator（数据整理器）\n",
    "\n",
    "数据整理器将训练数据整理为批次数据，用于模型训练时的批次处理。本教程使用默认的 `default_data_collator`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T14:09:20.121587Z",
     "iopub.status.busy": "2024-01-08T14:09:20.121073Z",
     "iopub.status.idle": "2024-01-08T14:09:20.127199Z",
     "shell.execute_reply": "2024-01-08T14:09:20.126033Z",
     "shell.execute_reply.started": "2024-01-08T14:09:20.121539Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "### 实例化训练器（Trainer）\n",
    "\n",
    "为了减少训练时间（需要大量算力支持），我们不在本教程的训练模型过程中计算模型评估指标。\n",
    "\n",
    "而是训练完成后，再独立进行模型评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T14:10:22.524995Z",
     "iopub.status.busy": "2024-01-08T14:10:22.524526Z",
     "iopub.status.idle": "2024-01-08T14:10:22.814908Z",
     "shell.execute_reply": "2024-01-08T14:10:22.814110Z",
     "shell.execute_reply.started": "2024-01-08T14:10:22.524948Z"
    },
    "id": "imY1oC3SIrJf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU 使用情况\n",
    "\n",
    "训练数据与模型配置：\n",
    "\n",
    "- SQUAD v1.1\n",
    "- model_checkpoint = \"distilbert-base-uncased\"\n",
    "- batch_size = 64\n",
    "\n",
    "NVIDIA GPU 使用情况：\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 15:39:57 2023\n",
    "\n",
    "Wed Dec 20 15:39:57 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   67C    P0              67W /  70W |  14617MiB / 15360MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     16384      C   /root/miniconda3/bin/python               14612MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T12:03:58.802359Z",
     "iopub.status.busy": "2024-01-03T12:03:58.801856Z",
     "iopub.status.idle": "2024-01-03T12:29:54.346899Z",
     "shell.execute_reply": "2024-01-03T12:29:54.346265Z",
     "shell.execute_reply.started": "2024-01-03T12:03:58.802314Z"
    },
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1545' max='1545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1545/1545 25:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.225200</td>\n",
       "      <td>1.428931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.447900</td>\n",
       "      <td>1.321090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.308100</td>\n",
       "      <td>1.341908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1545, training_loss=1.6497430770528354, metrics={'train_runtime': 1555.2063, 'train_samples_per_second': 254.154, 'train_steps_per_second': 0.993, 'total_flos': 3.873165421863629e+16, 'train_loss': 1.6497430770528354, 'epoch': 3.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练完成后，第一时间保存模型权重文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T15:09:00.990825Z",
     "iopub.status.busy": "2024-01-08T15:09:00.990355Z",
     "iopub.status.idle": "2024-01-08T15:09:00.996415Z",
     "shell.execute_reply": "2024-01-08T15:09:00.995262Z",
     "shell.execute_reply.started": "2024-01-08T15:09:00.990782Z"
    }
   },
   "outputs": [],
   "source": [
    "trained_model_path = f\"{model_dir}/{model_name}-finetuned-squad-trained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-03T12:35:03.399985Z",
     "iopub.status.busy": "2024-01-03T12:35:03.399510Z",
     "iopub.status.idle": "2024-01-03T12:35:03.785822Z",
     "shell.execute_reply": "2024-01-03T12:35:03.784950Z",
     "shell.execute_reply.started": "2024-01-03T12:35:03.399940Z"
    }
   },
   "outputs": [],
   "source": [
    "model_to_save = trainer.save_model(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T15:39:29.439014Z",
     "iopub.status.busy": "2024-01-08T15:39:29.438502Z",
     "iopub.status.idle": "2024-01-08T15:39:30.191256Z",
     "shell.execute_reply": "2024-01-08T15:39:30.190664Z",
     "shell.execute_reply.started": "2024-01-08T15:39:29.438962Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "k=\n",
      "input_ids\n",
      "v=\n",
      "tensor([[  101,  1999,  2054,  ...,     0,     0,     0],\n",
      "        [  101,  2043,  2020,  ...,     0,     0,     0],\n",
      "        [  101,  2013,  2029,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2040, 10836,  ...,     0,     0,     0],\n",
      "        [  101,  2040,  2106,  ...,     0,     0,     0],\n",
      "        [  101,  2040,  2106,  ...,     0,     0,     0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1999,  2054,  ...,     0,     0,     0],\n",
       "         [  101,  2043,  2020,  ...,     0,     0,     0],\n",
       "         [  101,  2013,  2029,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2040, 10836,  ...,     0,     0,     0],\n",
       "         [  101,  2040,  2106,  ...,     0,     0,     0],\n",
       "         [  101,  2040,  2106,  ...,     0,     0,     0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'start_positions': tensor([ 49,  37,  72,  80, 157,   0,   0,   0,   0, 202, 119,  52,   0,   0,\n",
       "           0,   0,   0,  97,  90,   0,   0,  56,  88, 153,   0,   0,   0,   0,\n",
       "          15,   0,   0,  30,  72,   0,   0,   0,  93,   0,   0,   0,  54,   0,\n",
       "           0,   0,   0,  29,  80, 138,   0,   0,   0,   0, 139,  70,  17,   0,\n",
       "           0,   0,   0,  19,   0,   0,   0,  12,   0, 282,  42, 108,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,  88,  28,  84,   0,   0,   0,  32,\n",
       "          36,  71,   0,   0,   0,  72,  67,  89, 101,   0,   0,   0,  39,  21,\n",
       "          91,   0,   0,   0,  25,  12,  10,  43,   0,   0,   0,   0, 134,   0,\n",
       "           0,   0,  36,  94,  21,   0,   0,   0,  23,  33,  65,  94,   0,   0,\n",
       "           0,  58], device='cuda:0'),\n",
       " 'end_positions': tensor([ 49,  40,  76,  81, 158,   0,   0,   0,   0, 204, 120,  52,   0,   0,\n",
       "           0,   0,   0,  97,  91,   0,   0,  56,  90, 153,   0,   0,   0,   0,\n",
       "          16,   0,   0,  30,  72,   0,   0,   0,  94,   0,   0,   0,  58,   0,\n",
       "           0,   0,   0,  30,  81, 143,   0,   0,   0,   0, 141,  71,  18,   0,\n",
       "           0,   0,   0,  20,   0,   0,   0,  15,   0, 283,  43, 110,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,  90,  30,  86,   0,   0,   0,  33,\n",
       "          40,  72,   0,   0,   0,  75,  69,  89, 106,   0,   0,   0,  42,  22,\n",
       "          95,   0,   0,   0,  27,  13,  11,  45,   0,   0,   0,   0, 135,   0,\n",
       "           0,   0,  37,  94,  21,   0,   0,   0,  23,  37,  66,  94,   0,   0,\n",
       "           0,  62], device='cuda:0')}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新加载模型\n",
    "\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_model_path)\n",
    "\n",
    "trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(trainer.args.device)\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "\n",
    "for k, v in batch.items():\n",
    "    print(f\"k=\\n{k}\")\n",
    "    print(f\"v=\\n{v}\")\n",
    "    break\n",
    "\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**评估模型输出需要一些额外的处理：将模型的预测映射回上下文的部分。**\n",
    "\n",
    "模型直接输出的是预测答案的`起始位置`和`结束位置`的**logits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T15:40:53.176504Z",
     "iopub.status.busy": "2024-01-08T15:40:53.176020Z",
     "iopub.status.idle": "2024-01-08T15:40:54.233187Z",
     "shell.execute_reply": "2024-01-08T15:40:54.232607Z",
     "shell.execute_reply.started": "2024-01-08T15:40:53.176458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的输出是一个类似字典的对象，其中包含损失（因为我们提供了标签），以及起始和结束logits。我们不需要损失来进行预测，让我们看一下logits："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 384 是输入文本的最大长度，64 是 batch_size\n",
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T15:43:50.097428Z",
     "iopub.status.busy": "2024-01-08T15:43:50.096950Z",
     "iopub.status.idle": "2024-01-08T15:43:50.132165Z",
     "shell.execute_reply": "2024-01-08T15:43:50.131412Z",
     "shell.execute_reply.started": "2024-01-08T15:43:50.097381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 49,  37,  72,  80, 157,   0,  44,   0, 152, 202, 119,  52,  22,   0,\n",
       "           0, 120,   0,  94,   0,   0,   0,  56,  88, 152,   0,   0,   0, 124,\n",
       "          15,   0,   0,   0,  72,  32,   0,   0,  93,   0,   0,   0,  41,  14,\n",
       "           0,  55,  99,  29,  80,   0,  28,  29,   0,   0, 139,  70,  49,   0,\n",
       "          48,  62,  74,  49,   0,   0,   0,  12,  16, 281,  41, 108,   0,  15,\n",
       "           0,   0,   0,   0,   0, 106,   0, 182,  28,  84,   0,  36,  34,  32,\n",
       "          55,  72,  38,  46,   0,  65,  67,  89,   0, 123,  92,   0,  39,  21,\n",
       "          91,  31,  68,   0,   0,  11,  10,  43,  11,   0,  42,   0,   0,   0,\n",
       "          82,  65,  36,  29,  60,  29,  13, 109,  39,   0,  65,  94,  41,   0,\n",
       "           0,  58], device='cuda:0'),\n",
       " tensor([ 49,  40,  76,  81, 157,  19,  44,   0, 153, 204, 120,  53,  26,  33,\n",
       "           0, 121,   0,  98,   0,   0,   0,  56,   0, 153,  24,   0,   0, 131,\n",
       "          16,   0,  94,   0,  72,  32,   0,   0,  94,   0,  94, 139,  58,  15,\n",
       "          50,  56, 104,  30,  81,   0,  29,  30,   0,  80, 141,  71,  51,   0,\n",
       "          49,  63,  82,  52,  17,   0,   0,  15,  19, 283,  43, 110,   0,  18,\n",
       "           0,  52,   0,   0,   0, 108,   0, 184,  30,  86,   0,  38,  34,  33,\n",
       "          59,  72,  39,  48,  61,  69,  71,  89,   0, 123,  93,   0,  42,  22,\n",
       "          95,  32,  71,   0,  27,  15,  11,  45,  12,   0,  44,   0,   0,   0,\n",
       "          92,   0,  37,  31,  75,  31,  15, 113,  39,   0,  66,  94,  41,   0,\n",
       "           0,  62], device='cuda:0'))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如何从模型输出的位置 logit 组合成答案\n",
    "\n",
    "我们有每个特征和每个标记的logit。在每个特征中为每个标记预测答案最明显的方法是，将起始logits的最大索引作为起始位置，将结束logits的最大索引作为结束位置。\n",
    "\n",
    "在许多情况下这种方式效果很好，但是如果此预测给出了不可能的结果该怎么办？比如：起始位置可能大于结束位置，或者指向问题中的文本片段而不是答案。在这种情况下，我们可能希望查看第二好的预测，看它是否给出了一个可能的答案，并选择它。\n",
    "\n",
    "选择第二好的答案并不像选择最佳答案那么容易：\n",
    "- 它是起始logits中第二佳索引与结束logits中最佳索引吗？\n",
    "- 还是起始logits中最佳索引与结束logits中第二佳索引？\n",
    "- 如果第二好的答案也不可能，那么对于第三好的答案，情况会更加棘手。\n",
    "\n",
    "为了对答案进行分类，\n",
    "1. 将使用通过添加起始和结束logits获得的分数\n",
    "1. 设计一个名为`n_best_size`的超参数，限制不对所有可能的答案进行排序。\n",
    "1. 我们将选择起始和结束logits中的最佳索引，并收集这些预测的所有答案。\n",
    "1. 在检查每一个是否有效后，我们将按照其分数对它们进行排序，并保留最佳的答案。\n",
    "\n",
    "以下是我们如何在批次中的第一个特征上执行此操作的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T15:56:58.457269Z",
     "iopub.status.busy": "2024-01-08T15:56:58.456797Z",
     "iopub.status.idle": "2024-01-08T15:56:58.462942Z",
     "shell.execute_reply": "2024-01-08T15:56:58.461592Z",
     "shell.execute_reply.started": "2024-01-08T15:56:58.457224Z"
    }
   },
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T16:06:28.778433Z",
     "iopub.status.busy": "2024-01-08T16:06:28.777948Z",
     "iopub.status.idle": "2024-01-08T16:06:28.806035Z",
     "shell.execute_reply": "2024-01-08T16:06:28.805205Z",
     "shell.execute_reply.started": "2024-01-08T16:06:28.778389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_indexes=\n",
      "[49, 0, 71, 73, 44, 75, 48, 46, 10, 20, 9, 36, 47, 34, 130, 27, 55, 15, 95, 35]\n",
      "end_indexes=\n",
      "[49, 0, 75, 71, 73, 50, 44, 11, 47, 39, 55, 97, 20, 132, 65, 18, 64, 38, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# print(f\"start_logits=\\n{start_logits}\")\n",
    "# print(f\"end_logits=\\n{end_logits}\")\n",
    "\n",
    "# 获取最佳的起始和结束位置的索引：\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "print(f\"start_indexes=\\n{start_indexes}\")\n",
    "print(f\"end_indexes=\\n{end_indexes}\")\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# 遍历起始位置和结束位置的索引组合\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # 需要进一步测试以检查答案是否在上下文中\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # 我们需要找到一种方法来获取与上下文中答案对应的原始子字符串\n",
    "                }\n",
    "            )\n",
    "\n",
    "# print(f\"valid_answers=\\n{valid_answers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "然后，我们可以根据它们的得分对`valid_answers`进行排序，并仅保留最佳答案。唯一剩下的问题是如何检查给定的跨度是否在上下文中（而不是问题中），以及如何获取其中的文本。为此，我们需要向我们的验证特征添加两个内容：\n",
    "\n",
    "- 生成该特征的示例的ID（因为每个示例可以生成多个特征，如前所示）；\n",
    "- 偏移映射，它将为我们提供从标记索引到上下文中字符位置的映射。\n",
    "\n",
    "这就是为什么我们将使用以下函数稍微不同于`prepare_train_features`来重新处理验证集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T16:54:00.638174Z",
     "iopub.status.busy": "2024-01-08T16:54:00.637702Z",
     "iopub.status.idle": "2024-01-08T16:54:00.649941Z",
     "shell.execute_reply": "2024-01-08T16:54:00.648963Z",
     "shell.execute_reply.started": "2024-01-08T16:54:00.638127Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # 一些问题的左侧有很多空白，这些空白并不有用且会导致上下文截断失败（分词后的问题会占用很多空间）。\n",
    "    # 因此我们移除这些左侧空白\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和可能的填充对我们的示例进行分词，但使用步长保留溢出的令牌。这导致一个长上下文的示例可能产生\n",
    "    # 几个特征，每个特征的上下文都会稍微与前一个特征的上下文重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例在上下文很长时可能会产生几个特征，我们需要一个从特征映射到其对应示例的映射。这个键就是为了这个目的。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # 我们保留产生这个特征的示例ID，并且会存储偏移映射。\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # 获取与该示例对应的序列（以了解哪些是上下文，哪些是问题）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # 一个示例可以产生几个文本段，这里是包含该文本段的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        #print(f\"sample_index={sample_index}\")\n",
    "        #print(f\"example_id={examples['id'][sample_index]}\")\n",
    "\n",
    "        # 将不属于上下文的偏移映射设置为None，以便容易确定一个令牌位置是否属于上下文。\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T17:59:10.403555Z",
     "iopub.status.busy": "2024-01-08T17:59:10.402724Z",
     "iopub.status.idle": "2024-01-08T17:59:10.548412Z",
     "shell.execute_reply": "2024-01-08T17:59:10.547291Z",
     "shell.execute_reply.started": "2024-01-08T17:59:10.403505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d063c2a6a24c40f7b4f5c82375f13e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_datasets = \n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_predictions = \n",
      "PredictionOutput(predictions=(array([[ 3.462695 , -3.7289808, -3.540186 , ..., -7.351231 , -7.375579 ,\n",
      "        -7.412394 ],\n",
      "       [ 1.9488118, -3.2959232, -5.4759   , ..., -7.3576055, -7.3355603,\n",
      "        -7.374455 ],\n",
      "       [ 2.6795068, -4.514475 , -3.8802037, ..., -7.375135 , -7.3586087,\n",
      "        -7.406279 ]], dtype=float32), array([[ 3.7686083, -4.9501624, -4.5620584, ..., -7.3053894, -7.2711754,\n",
      "        -7.2318134],\n",
      "       [ 2.3426764, -3.911046 , -5.6655445, ..., -7.2960043, -7.323268 ,\n",
      "        -7.2802773],\n",
      "       [ 2.9239106, -5.0294123, -4.205415 , ..., -7.28047  , -7.3003607,\n",
      "        -7.2488537]], dtype=float32)), label_ids=None, metrics={'test_runtime': 0.0929, 'test_samples_per_second': 32.3, 'test_steps_per_second': 10.767})\n"
     ]
    }
   ],
   "source": [
    "# 测试 prepare_validation_features 方法\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "datasets_sample = datasets[\"validation\"].select(range(3))\n",
    "\n",
    "tokenized_datasets = datasets_sample.map(prepare_validation_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"validation\"].column_names)\n",
    "\n",
    "print(f\"tokenized_datasets = \\n{tokenized_datasets}\")\n",
    "\n",
    "raw_predictions = trainer.predict(tokenized_datasets)\n",
    "\n",
    "print(f\"raw_predictions = \\n{raw_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将`prepare_validation_features`应用到整个验证集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T18:00:02.310626Z",
     "iopub.status.busy": "2024-01-08T18:00:02.310144Z",
     "iopub.status.idle": "2024-01-08T18:00:02.330701Z",
     "shell.execute_reply": "2024-01-08T18:00:02.329619Z",
     "shell.execute_reply.started": "2024-01-08T18:00:02.310579Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T18:00:05.492704Z",
     "iopub.status.busy": "2024-01-08T18:00:05.492232Z",
     "iopub.status.idle": "2024-01-08T18:00:36.888857Z",
     "shell.execute_reply": "2024-01-08T18:00:36.888104Z",
     "shell.execute_reply.started": "2024-01-08T18:00:05.492659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Trainer`会隐藏模型不使用的列（在这里是`example_id`和`offset_mapping`，我们需要它们进行后处理），所以我们需要将它们重新设置回来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T18:00:54.294436Z",
     "iopub.status.busy": "2024-01-08T18:00:54.293933Z",
     "iopub.status.idle": "2024-01-08T18:00:54.304258Z",
     "shell.execute_reply": "2024-01-08T18:00:54.303060Z",
     "shell.execute_reply.started": "2024-01-08T18:00:54.294390Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以改进之前的测试：\n",
    "\n",
    "由于在偏移映射中，当它对应于问题的一部分时，我们将其设置为None，因此可以轻松检查答案是否完全在上下文中。我们还可以从考虑中排除非常长的答案（可以调整的超参数）。\n",
    "\n",
    "展开说下具体实现：\n",
    "- 首先从模型输出中获取起始和结束的逻辑值（logits），这些值表明答案在文本中可能开始和结束的位置。\n",
    "- 然后，它使用偏移映射（offset_mapping）来找到这些逻辑值在原始文本中的具体位置。\n",
    "- 接下来，代码遍历可能的开始和结束索引组合，排除那些不在上下文范围内或长度不合适的答案。\n",
    "- 对于有效的答案，它计算出一个分数（基于开始和结束逻辑值的和），并将答案及其分数存储起来。\n",
    "- 最后，它根据分数对答案进行排序，并返回得分最高的几个答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T17:00:23.934079Z",
     "iopub.status.busy": "2024-01-08T17:00:23.933613Z",
     "iopub.status.idle": "2024-01-08T17:00:23.939427Z",
     "shell.execute_reply": "2024-01-08T17:00:23.938255Z",
     "shell.execute_reply.started": "2024-01-08T17:00:23.934035Z"
    }
   },
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T17:08:25.150613Z",
     "iopub.status.busy": "2024-01-08T17:08:25.150150Z",
     "iopub.status.idle": "2024-01-08T17:08:25.185014Z",
     "shell.execute_reply": "2024-01-08T17:08:25.184362Z",
     "shell.execute_reply.started": "2024-01-08T17:08:25.150568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets['validation'][0]={'id': '56ddde6b9a695914005b9628', 'title': 'Normans', 'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.', 'question': 'In what country is Normandy located?', 'answers': {'text': ['France', 'France', 'France', 'France'], 'answer_start': [159, 159, 159, 159]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 11.978519, 'text': 'France'},\n",
       " {'score': 7.5229354,\n",
       "  'text': 'France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway'},\n",
       " {'score': 6.3575273,\n",
       "  'text': 'France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark'},\n",
       " {'score': 6.012344,\n",
       "  'text': 'France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland'},\n",
       " {'score': 5.835109, 'text': 'Normandy, a region in France'},\n",
       " {'score': 5.469718, 'text': 'France.'},\n",
       " {'score': 5.191332, 'text': 'in France'},\n",
       " {'score': 4.3711185, 'text': 'a region in France'},\n",
       " {'score': 3.998611,\n",
       "  'text': 'French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France'},\n",
       " {'score': 3.7937279,\n",
       "  'text': '10th and 11th centuries gave their name to Normandy, a region in France'},\n",
       " {'score': 3.7803566, 'text': 'region in France'},\n",
       " {'score': 3.5967429, 'text': 'France. They were descended from Norse'},\n",
       " {'score': 3.3240523,\n",
       "  'text': 'in the 10th and 11th centuries gave their name to Normandy, a region in France'},\n",
       " {'score': 3.303082, 'text': 'Denmark, Iceland and Norway'},\n",
       " {'score': 3.1342828,\n",
       "  'text': 'Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France'},\n",
       " {'score': 2.9468484,\n",
       "  'text': 'France. They were descended from Norse (\"Norman\" comes from \"Norseman\"'},\n",
       " {'score': 2.879967,\n",
       "  'text': 'the 10th and 11th centuries gave their name to Normandy, a region in France'},\n",
       " {'score': 2.4729538, 'text': 'Iceland and Norway'},\n",
       " {'score': 2.3172915,\n",
       "  'text': 'France. They were descended from Norse (\"Norman\" comes from \"Norseman'},\n",
       " {'score': 2.1376736, 'text': 'Denmark'}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# 第一个特征来自第一个示例。对于更一般的情况，我们需要将example_id匹配到一个示例索引\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "print(f\"datasets['validation'][0]={datasets['validation'][0]}\")\n",
    "\n",
    "# 收集最佳开始/结束逻辑的索引：\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # 我们需要细化这个测试，以检查答案是否在上下文中\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印比较模型输出和标准答案（Ground-truth）是否一致:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T17:18:30.886571Z",
     "iopub.status.busy": "2024-01-08T17:18:30.886107Z",
     "iopub.status.idle": "2024-01-08T17:18:30.895224Z",
     "shell.execute_reply": "2024-01-08T17:18:30.894064Z",
     "shell.execute_reply.started": "2024-01-08T17:18:30.886526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['France', 'France', 'France', 'France'],\n",
       " 'answer_start': [159, 159, 159, 159]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型最高概率的输出与标准答案一致**\n",
    "\n",
    "正如上面的代码所示，这在第一个特征上很容易，因为我们知道它来自第一个示例。\n",
    "\n",
    "对于其他特征，我们需要建立一个示例与其对应特征的映射关系。\n",
    "\n",
    "此外，由于一个示例可以生成多个特征，我们需要将由给定示例生成的所有特征中的所有答案汇集在一起，然后选择最佳答案。\n",
    "\n",
    "下面的代码构建了一个示例索引到其对应特征索引的映射关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T17:20:03.755144Z",
     "iopub.status.busy": "2024-01-08T17:20:03.754686Z",
     "iopub.status.idle": "2024-01-08T17:20:14.932085Z",
     "shell.execute_reply": "2024-01-08T17:20:14.931251Z",
     "shell.execute_reply.started": "2024-01-08T17:20:03.755099Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# 一个验证样本的上下文可能超过最大长度限制，在 Tokenize 操作中被拆分成多段，这里要构造出每个example_id与段列表的映射关系，例如：\n",
    "# example_id=56ddde6b9a695914005b9628, features_per_example[56ddde6b9a695914005b9628]=[3,4] (字典中每个元素是一个列表)\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当`squad_v2 = True`时，有一定概率出现不可能的答案（impossible answer)。\n",
    "\n",
    "上面的代码仅保留在上下文中的答案，我们还需要获取不可能答案的分数（其起始和结束索引对应于CLS标记的索引）。\n",
    "\n",
    "当一个示例生成多个特征时，我们必须在所有特征中的不可能答案都预测出现不可能答案时（因为一个特征可能之所以能够预测出不可能答案，是因为答案不在它可以访问的上下文部分），这就是为什么一个示例中不可能答案的分数是该示例生成的每个特征中的不可能答案的分数的最小值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T17:25:25.743890Z",
     "iopub.status.busy": "2024-01-08T17:25:25.743412Z",
     "iopub.status.idle": "2024-01-08T17:25:25.761913Z",
     "shell.execute_reply": "2024-01-08T17:25:25.761180Z",
     "shell.execute_reply.started": "2024-01-08T17:25:25.743843Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # 构建一个从示例到其对应特征的映射。\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # 我们需要填充的字典。\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # 日志记录。\n",
    "    print(f\"正在后处理 {len(examples)} 个示例的预测，这些预测分散在 {len(features)} 个特征中。\")\n",
    "\n",
    "    # 遍历所有示例！\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # 这些是与当前示例关联的特征的索引。\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # 仅在squad_v2为True时使用。\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # 遍历与当前示例关联的所有特征。\n",
    "        for feature_index in feature_indices:\n",
    "            # 我们获取模型对这个特征的预测。\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # 这将允许我们将logits中的某些位置映射到原始上下文中的文本跨度。\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # 更新最小空预测。\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # 浏览所有的最佳开始和结束logits，为 `n_best_size` 个最佳选择。\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # 在极少数情况下我们没有一个非空预测，我们创建一个假预测以避免失败。\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # 选择我们的最终答案：最佳答案或空答案（仅适用于squad_v2）\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在原始结果上应用后处理问答结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T17:28:41.823104Z",
     "iopub.status.busy": "2024-01-08T17:28:41.822640Z",
     "iopub.status.idle": "2024-01-08T17:29:20.183250Z",
     "shell.execute_reply": "2024-01-08T17:29:20.182047Z",
     "shell.execute_reply.started": "2024-01-08T17:28:41.823060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 11873 个示例的预测，这些预测分散在 12134 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b923ae9cbe4e0d857c0b7797bc82f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `datasets.load_metric` 中加载 `SQuAD v2` 的评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T17:29:20.185318Z",
     "iopub.status.busy": "2024-01-08T17:29:20.185028Z",
     "iopub.status.idle": "2024-01-08T17:29:24.501866Z",
     "shell.execute_reply": "2024-01-08T17:29:24.500983Z",
     "shell.execute_reply.started": "2024-01-08T17:29:20.185299Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2517/2330875496.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba28f464eb3f446c8949a81dc586362f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca98ffbfbad0465bad9e757fff3874a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们可以调用上面定义的函数进行评估。\n",
    "\n",
    "只需稍微调整一下预测和标签的格式，因为它期望的是一系列字典而不是一个大字典。\n",
    "\n",
    "在使用`squad_v2`数据集时，我们还需要设置`no_answer_probability`参数（我们在这里将其设置为0.0，因为如果我们选择了答案，我们已经将答案设置为空）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-08T17:30:31.149492Z",
     "iopub.status.busy": "2024-01-08T17:30:31.148994Z",
     "iopub.status.idle": "2024-01-08T17:30:33.577711Z",
     "shell.execute_reply": "2024-01-08T17:30:33.576779Z",
     "shell.execute_reply.started": "2024-01-08T17:30:31.149424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 57.99713635980797,\n",
       " 'f1': 61.8106436924254,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 57.827260458839405,\n",
       " 'HasAns_f1': 65.46521129557436,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 58.16652649285113,\n",
       " 'NoAns_f1': 58.16652649285113,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 58.064516129032256,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 61.819066163578626,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework：加载本地保存的模型，进行评估和再训练更高的 F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T04:58:20.817005Z",
     "iopub.status.busy": "2024-01-09T04:58:20.816558Z",
     "iopub.status.idle": "2024-01-09T04:58:20.828447Z",
     "shell.execute_reply": "2024-01-09T04:58:20.827388Z",
     "shell.execute_reply.started": "2024-01-09T04:58:20.816958Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "\n",
    "squad_v2 = True\n",
    "model_checkpoint = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T04:58:23.556602Z",
     "iopub.status.busy": "2024-01-09T04:58:23.556194Z",
     "iopub.status.idle": "2024-01-09T04:58:54.195681Z",
     "shell.execute_reply": "2024-01-09T04:58:54.194970Z",
     "shell.execute_reply.started": "2024-01-09T04:58:23.556561Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "\n",
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:02:03.583703Z",
     "iopub.status.busy": "2024-01-09T05:02:03.583024Z",
     "iopub.status.idle": "2024-01-09T05:02:04.118711Z",
     "shell.execute_reply": "2024-01-09T05:02:04.117822Z",
     "shell.execute_reply.started": "2024-01-09T05:02:03.583654Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建 tokenizer 实例\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:02:09.792724Z",
     "iopub.status.busy": "2024-01-09T05:02:09.792263Z",
     "iopub.status.idle": "2024-01-09T05:02:09.798559Z",
     "shell.execute_reply": "2024-01-09T05:02:09.797494Z",
     "shell.execute_reply.started": "2024-01-09T05:02:09.792679Z"
    }
   },
   "outputs": [],
   "source": [
    "# 配置预处理参数\n",
    "\n",
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128 \n",
    "# 填充策略\n",
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:02:13.548440Z",
     "iopub.status.busy": "2024-01-09T05:02:13.547970Z",
     "iopub.status.idle": "2024-01-09T05:02:13.564309Z",
     "shell.execute_reply": "2024-01-09T05:02:13.563457Z",
     "shell.execute_reply.started": "2024-01-09T05:02:13.548394Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练集预处理函数\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # 一些问题的左侧可能有很多空白字符，这对我们没有用，而且会导致上下文的截断失败\n",
    "    # （标记化的问题将占用大量空间）。因此，我们删除左侧的空白字符。\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    #print(f\"examples=\\n{examples}\")\n",
    "\n",
    "    # 使用截断和填充对我们的示例进行标记化，但保留溢出部分，使用步幅（stride）。\n",
    "    # 当上下文很长时，这会导致一个示例可能提供多个特征，其中每个特征的上下文都与前一个特征的上下文有一些重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    #print(f\"tokenized_examples=\\n{tokenized_examples}\")\n",
    "\n",
    "    # 由于一个示例可能给我们提供多个特征（如果它具有很长的上下文），我们需要一个从特征到其对应示例的映射。这个键就提供了这个映射关系。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # 偏移映射将为我们提供从令牌到原始上下文中的字符位置的映射。这将帮助我们计算开始位置和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 让我们为这些示例进行标记！\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 我们将使用CLS令牌的索引来标记不可能的答案。\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 获取与该示例对应的序列（以了解上下文和问题是什么）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 一个示例可以提供多个跨度，这是包含此文本跨度的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        #print(f\"sample_mapping=\\n{sample_mapping}\")\n",
    "        #print(f\"answers=\\n{answers}\")\n",
    "        \n",
    "        # 如果没有给出答案，则将cls_index设置为答案。\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案在文本中的开始和结束字符索引。\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # 当前跨度在文本中的开始令牌索引。\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 当前跨度在文本中的结束令牌索引。\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 检测答案是否超出跨度（在这种情况下，该特征的标签将使用CLS索引）。\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 否则，将token_start_index和token_end_index移到答案的两端。\n",
    "                # 注意：如果答案是最后一个单词（边缘情况），我们可以在最后一个偏移之后继续。\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:02:21.010099Z",
     "iopub.status.busy": "2024-01-09T05:02:21.009606Z",
     "iopub.status.idle": "2024-01-09T05:02:21.045481Z",
     "shell.execute_reply": "2024-01-09T05:02:21.044463Z",
     "shell.execute_reply.started": "2024-01-09T05:02:21.010053Z"
    }
   },
   "outputs": [],
   "source": [
    "# 预处理训练数据\n",
    "\n",
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:02:40.870726Z",
     "iopub.status.busy": "2024-01-09T05:02:40.870294Z",
     "iopub.status.idle": "2024-01-09T05:02:44.595653Z",
     "shell.execute_reply": "2024-01-09T05:02:44.594843Z",
     "shell.execute_reply.started": "2024-01-09T05:02:40.870684Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 13:02:41.504900: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-09 13:02:41.504952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-09 13:02:41.506050: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-09 13:02:41.513102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-09 13:02:42.413073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# 数据整理器将训练数据整理为批次数据，用于模型训练时的批次处理，本教程使用默认的 default_data_collator。\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:03:30.569354Z",
     "iopub.status.busy": "2024-01-09T05:03:30.568871Z",
     "iopub.status.idle": "2024-01-09T05:03:34.517889Z",
     "shell.execute_reply": "2024-01-09T05:03:34.516965Z",
     "shell.execute_reply.started": "2024-01-09T05:03:30.569310Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载本地模型\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "trained_model_path = f\"{model_dir}/{model_name}-finetuned-squad-trained\"\n",
    "\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:03:36.206513Z",
     "iopub.status.busy": "2024-01-09T05:03:36.205971Z",
     "iopub.status.idle": "2024-01-09T05:03:36.293657Z",
     "shell.execute_reply": "2024-01-09T05:03:36.292789Z",
     "shell.execute_reply.started": "2024-01-09T05:03:36.206464Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置训练超参数\n",
    "\n",
    "batch_size=64\n",
    "model_dir = \"models\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_dir}/{model_name}-finetuned-squad\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:03:39.510819Z",
     "iopub.status.busy": "2024-01-09T05:03:39.510335Z",
     "iopub.status.idle": "2024-01-09T05:03:39.775146Z",
     "shell.execute_reply": "2024-01-09T05:03:39.774438Z",
     "shell.execute_reply.started": "2024-01-09T05:03:39.510774Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 实例化训练器\n",
    "\n",
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:04:27.849768Z",
     "iopub.status.busy": "2024-01-09T05:04:27.849292Z",
     "iopub.status.idle": "2024-01-09T05:50:48.840069Z",
     "shell.execute_reply": "2024-01-09T05:50:48.838653Z",
     "shell.execute_reply.started": "2024-01-09T05:04:27.849721Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3090' max='3090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3090/3090 46:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.093500</td>\n",
       "      <td>1.355893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>1.405757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.860700</td>\n",
       "      <td>1.410320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3090, training_loss=0.9639389531897882, metrics={'train_runtime': 2780.6743, 'train_samples_per_second': 142.146, 'train_steps_per_second': 1.111, 'total_flos': 3.873165421863629e+16, 'train_loss': 0.9639389531897882, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 执行训练\n",
    "\n",
    "trained_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:50:48.843289Z",
     "iopub.status.busy": "2024-01-09T05:50:48.842837Z",
     "iopub.status.idle": "2024-01-09T05:50:49.369782Z",
     "shell.execute_reply": "2024-01-09T05:50:49.368924Z",
     "shell.execute_reply.started": "2024-01-09T05:50:48.843248Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "\n",
    "model_to_save = trained_trainer.save_model(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:50:49.371446Z",
     "iopub.status.busy": "2024-01-09T05:50:49.371216Z",
     "iopub.status.idle": "2024-01-09T05:50:49.378095Z",
     "shell.execute_reply": "2024-01-09T05:50:49.377292Z",
     "shell.execute_reply.started": "2024-01-09T05:50:49.371426Z"
    }
   },
   "outputs": [],
   "source": [
    "# 验证集预处理函数\n",
    "\n",
    "def prepare_validation_features(examples):\n",
    "    # 一些问题的左侧有很多空白，这些空白并不有用且会导致上下文截断失败（分词后的问题会占用很多空间）。\n",
    "    # 因此我们移除这些左侧空白\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和可能的填充对我们的示例进行分词，但使用步长保留溢出的令牌。这导致一个长上下文的示例可能产生\n",
    "    # 几个特征，每个特征的上下文都会稍微与前一个特征的上下文重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例在上下文很长时可能会产生几个特征，我们需要一个从特征映射到其对应示例的映射。这个键就是为了这个目的。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # 我们保留产生这个特征的示例ID，并且会存储偏移映射。\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # 获取与该示例对应的序列（以了解哪些是上下文，哪些是问题）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # 一个示例可以产生几个文本段，这里是包含该文本段的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        #print(f\"sample_index={sample_index}\")\n",
    "        #print(f\"example_id={examples['id'][sample_index]}\")\n",
    "\n",
    "        # 将不属于上下文的偏移映射设置为None，以便容易确定一个令牌位置是否属于上下文。\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:50:49.379618Z",
     "iopub.status.busy": "2024-01-09T05:50:49.379418Z",
     "iopub.status.idle": "2024-01-09T05:51:25.835191Z",
     "shell.execute_reply": "2024-01-09T05:51:25.834335Z",
     "shell.execute_reply.started": "2024-01-09T05:50:49.379599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e7bf1270c44ec095b10a4971f6beb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 将 prepare_validation_features 应用到整个验证集\n",
    "\n",
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "raw_predictions = trained_trainer.predict(validation_features)\n",
    "\n",
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:51:25.836548Z",
     "iopub.status.busy": "2024-01-09T05:51:25.836347Z",
     "iopub.status.idle": "2024-01-09T05:51:25.847508Z",
     "shell.execute_reply": "2024-01-09T05:51:25.846792Z",
     "shell.execute_reply.started": "2024-01-09T05:51:25.836529Z"
    }
   },
   "outputs": [],
   "source": [
    "# 聚合验证集最佳预测答案\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # 构建一个从示例到其对应特征的映射。\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # 我们需要填充的字典。\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # 日志记录。\n",
    "    print(f\"正在后处理 {len(examples)} 个示例的预测，这些预测分散在 {len(features)} 个特征中。\")\n",
    "\n",
    "    # 遍历所有示例！\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # 这些是与当前示例关联的特征的索引。\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # 仅在squad_v2为True时使用。\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # 遍历与当前示例关联的所有特征。\n",
    "        for feature_index in feature_indices:\n",
    "            # 我们获取模型对这个特征的预测。\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # 这将允许我们将logits中的某些位置映射到原始上下文中的文本跨度。\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # 更新最小空预测。\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # 浏览所有的最佳开始和结束logits，为 `n_best_size` 个最佳选择。\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # 在极少数情况下我们没有一个非空预测，我们创建一个假预测以避免失败。\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # 选择我们的最终答案：最佳答案或空答案（仅适用于squad_v2）\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:51:25.848433Z",
     "iopub.status.busy": "2024-01-09T05:51:25.848232Z",
     "iopub.status.idle": "2024-01-09T05:52:04.355701Z",
     "shell.execute_reply": "2024-01-09T05:52:04.354834Z",
     "shell.execute_reply.started": "2024-01-09T05:51:25.848416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 11873 个示例的预测，这些预测分散在 12134 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82305ec64b04dcb9927b3363476be0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 在原始结果上应用后处理问答结果\n",
    "\n",
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:52:04.357246Z",
     "iopub.status.busy": "2024-01-09T05:52:04.357019Z",
     "iopub.status.idle": "2024-01-09T05:52:07.329644Z",
     "shell.execute_reply": "2024-01-09T05:52:07.328876Z",
     "shell.execute_reply.started": "2024-01-09T05:52:04.357225Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21286/1671546096.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n"
     ]
    }
   ],
   "source": [
    "# 使用 datasets.load_metric 中加载 SQuAD v2 的评估指标\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T05:52:42.519775Z",
     "iopub.status.busy": "2024-01-09T05:52:42.519309Z",
     "iopub.status.idle": "2024-01-09T05:52:44.656389Z",
     "shell.execute_reply": "2024-01-09T05:52:44.655705Z",
     "shell.execute_reply.started": "2024-01-09T05:52:42.519730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 61.40823717678767,\n",
       " 'f1': 65.33826003975071,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 65.60391363022941,\n",
       " 'HasAns_f1': 73.47522966463536,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 57.22455845248108,\n",
       " 'NoAns_f1': 57.22455845248108,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 61.40823717678767,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 65.3382600397509,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算评估结果\n",
    "\n",
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question Answering on SQUAD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
