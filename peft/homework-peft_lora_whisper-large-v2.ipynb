{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219841f-493c-40f9-a6c9-3700f0c525d0",
   "metadata": {},
   "source": [
    "# PEFT 库 LoRA 实战 - OpenAI Whisper-large-v2\n",
    "\n",
    "本教程使用 LoRA 在`OpenAI Whisper-large-v2`模型上实现`语音识别(ASR)`任务的微调训练。\n",
    "\n",
    "我们还结合了`int8` 量化进一步降低训练过程资源开销，同时保证了精度几乎不受影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a1e23-ea71-45d6-82d6-453077cf2d29",
   "metadata": {},
   "source": [
    "## 全局参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd00402-d821-485e-8703-fb16bcb56a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:00:21.278873Z",
     "iopub.status.busy": "2024-01-16T13:00:21.278360Z",
     "iopub.status.idle": "2024-01-16T13:00:21.291114Z",
     "shell.execute_reply": "2024-01-16T13:00:21.290118Z",
     "shell.execute_reply.started": "2024-01-16T13:00:21.278825Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffa1df-e51e-4026-9817-1cebddf0061a",
   "metadata": {},
   "source": [
    "## 下载数据集 Common Voice\n",
    "\n",
    "Common Voice 11.0 数据集包含许多不同语言的录音，总时长达数小时。\n",
    "\n",
    "本教程以中文数据为例，展示如何使用 LoRA 在 Whisper-large-v2 上进行微调训练。\n",
    "\n",
    "首先，初始化一个DatasetDict结构，并将训练集（将训练+验证拆分为训练集）和测试集拆分好，按照中文数据集构建配置加载到内存中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ff42f4-f3ec-46d3-b0c0-dd9ffbf7b50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:00:24.945998Z",
     "iopub.status.busy": "2024-01-16T13:00:24.945498Z",
     "iopub.status.idle": "2024-01-16T13:35:13.091035Z",
     "shell.execute_reply": "2024-01-16T13:35:13.090471Z",
     "shell.execute_reply.started": "2024-01-16T13:00:24.945955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa007b2438fd4aa3ac63ba5841620036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4341db1d58a14b7dbd45d948786df566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ffcfdee2904f33a71cd40651313f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7483ebe696e345f2ad1ac021478a1189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f79d5ed05974ff18819c68c14c0c0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cde1eabf93248fc86ad8a73fe790ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7693a208994ca684d553de75f9af55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e0a4a2efb244c5be9d3262d4e28b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d9303fc9724f0284b4d4e4b793ef84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc181fe3bc74c68a0ab90f1b35196c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c113dbf37474e9a8316fb115ffac88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5b450267be4794a5b598a6ab2b16be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea824c74891f487faf29f6b22cae0888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d96fdcc148b427191400a761695d74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3236262b04ec4363926f95f4c6913620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/998M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad318bea75747c9862b7ccbd81c19a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67e485424174bf1be55382ec067ad1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f319d525a9b94f7eae36f2e411cb1226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4713acd7ac2b433994fb973eaf8ce94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b74d713fd44ae2b59d695f396ec681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd6a625f43a4769b75188cb8a0ff3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e4cd3ca407465b9394a78d4fe74edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a600958e484556af3cc9917056916e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5040a02623945178a79e7635c2207be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fc43be84f8493abbebddf10a3202aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc4070f2a1f4544bfd1f6df7c410932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/549M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3c8e3c85a34dab9c15fe6027cfe1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/610M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5115a04eb4d4607b10f8b5bd74e9254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e34b7970e74eaaab8323dbfe8e21c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a174b1d150b4492b86e8b2f7ce694d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb23e3cc75c4c06bdef48dcdffb8d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/156M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babbcc0b13194c29b0db8c7a860660d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.98M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4bd6991d0f44599f0afa598fc917a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 0it [00:00, ?it/s]\u001b[A\n",
      "Reading metadata...: 13626it [00:00, 136253.63it/s]\u001b[A\n",
      "Reading metadata...: 29056it [00:00, 132145.03it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf4535a3a254ca48d8a6958517def47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 10581it [00:00, 179360.68it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e784c69b97649408fcec0243c1c9bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 10581it [00:00, 195665.77it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3942a07c885419fbd34697278213cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating other split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 0it [00:00, ?it/s]\u001b[A\n",
      "Reading metadata...: 14049it [00:00, 140438.24it/s]\u001b[A\n",
      "Reading metadata...: 30310it [00:00, 153472.96it/s]\u001b[A\n",
      "Reading metadata...: 49092it [00:00, 169150.41it/s]\u001b[A\n",
      "Reading metadata...: 68299it [00:00, 178183.41it/s]\u001b[A\n",
      "Reading metadata...: 87192it [00:00, 182047.26it/s]\u001b[A\n",
      "Reading metadata...: 105582it [00:00, 182674.52it/s]\u001b[A\n",
      "Reading metadata...: 124606it [00:00, 185144.53it/s]\u001b[A\n",
      "Reading metadata...: 143518it [00:00, 186407.42it/s]\u001b[A\n",
      "Reading metadata...: 162159it [00:00, 180953.48it/s]\u001b[A\n",
      "Reading metadata...: 180288it [00:01, 179225.02it/s]\u001b[A\n",
      "Reading metadata...: 199630it [00:01, 183473.30it/s]\u001b[A\n",
      "Reading metadata...: 218501it [00:01, 185039.50it/s]\u001b[A\n",
      "Reading metadata...: 237241it [00:01, 185744.24it/s]\u001b[A\n",
      "Reading metadata...: 256133it [00:01, 186694.63it/s]\u001b[A\n",
      "Reading metadata...: 275037it [00:01, 187391.32it/s]\u001b[A\n",
      "Reading metadata...: 293886it [00:01, 187718.87it/s]\u001b[A\n",
      "Reading metadata...: 312765it [00:01, 188037.37it/s]\u001b[A\n",
      "Reading metadata...: 331573it [00:01, 187814.35it/s]\u001b[A\n",
      "Reading metadata...: 350358it [00:01, 178711.92it/s]\u001b[A\n",
      "Reading metadata...: 369488it [00:02, 182354.16it/s]\u001b[A\n",
      "Reading metadata...: 388392it [00:02, 184307.13it/s]\u001b[A\n",
      "Reading metadata...: 407264it [00:02, 185605.72it/s]\u001b[A\n",
      "Reading metadata...: 426151it [00:02, 186563.47it/s]\u001b[A\n",
      "Reading metadata...: 444995it [00:02, 187119.05it/s]\u001b[A\n",
      "Reading metadata...: 463743it [00:02, 187224.86it/s]\u001b[A\n",
      "Reading metadata...: 482482it [00:02, 187107.28it/s]\u001b[A\n",
      "Reading metadata...: 501466it [00:02, 187921.89it/s]\u001b[A\n",
      "Reading metadata...: 520358it [00:02, 188217.39it/s]\u001b[A\n",
      "Reading metadata...: 539186it [00:02, 187065.95it/s]\u001b[A\n",
      "Reading metadata...: 557899it [00:03, 186649.55it/s]\u001b[A\n",
      "Reading metadata...: 576568it [00:03, 186573.56it/s]\u001b[A\n",
      "Reading metadata...: 595229it [00:03, 183537.27it/s]\u001b[A\n",
      "Reading metadata...: 613595it [00:03, 181675.89it/s]\u001b[A\n",
      "Reading metadata...: 631773it [00:03, 179956.90it/s]\u001b[A\n",
      "Reading metadata...: 649777it [00:03, 178890.30it/s]\u001b[A\n",
      "Reading metadata...: 667672it [00:03, 177712.35it/s]\u001b[A\n",
      "Reading metadata...: 698486it [00:03, 182491.92it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e65adc538f40df8f6f938317fd1134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating invalidated split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 0it [00:00, ?it/s]\u001b[A\n",
      "Reading metadata...: 21302it [00:00, 190220.34it/s]\u001b[A\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'client_id': '95368aab163e0387e4fd4991b4f2d8ccfbd4364bf656c860230501fd27dcedf087773e4695a6cf5de9c4f1d406d582283190d065cdfa36b0e2b060cffaca977e',\n",
       " 'path': '/home/yuxiang/.cache/huggingface/datasets/downloads/extracted/68855bbd878ec603e04e5baebb8217d4b60e177bbe23157096f27ede1a5cff9c/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       " 'audio': {'path': '/home/yuxiang/.cache/huggingface/datasets/downloads/extracted/68855bbd878ec603e04e5baebb8217d4b60e177bbe23157096f27ede1a5cff9c/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([-6.82121026e-13, -2.27373675e-12, -2.27373675e-12, ...,\n",
       "          1.21667399e-05,  3.23003678e-06, -2.43066324e-07]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。',\n",
       " 'up_votes': 2,\n",
       " 'down_votes': 0,\n",
       " 'age': '',\n",
       " 'gender': '',\n",
       " 'accent': '',\n",
       " 'locale': 'zh-CN',\n",
       " 'segment': ''}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\")\n",
    "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\")\n",
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81faa4-d8fe-4cc7-afe6-4c2615b9050f",
   "metadata": {},
   "source": [
    "## 预处理训练数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5822025f-7f8e-4141-8bfe-d8822d0da20f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:45:11.092925Z",
     "iopub.status.busy": "2024-01-16T13:45:11.092015Z",
     "iopub.status.idle": "2024-01-16T13:45:33.337554Z",
     "shell.execute_reply": "2024-01-16T13:45:33.336741Z",
     "shell.execute_reply.started": "2024-01-16T13:45:11.092873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a8985c341c44ba86cac9ebbc17869d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b69ef59be284d6c9d080cf5753f277b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7103100e8f4096a0639fc9a412a8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bcda2121f74440afdfe4e48787e868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d122347ec31f4ffe8e824e0c92d5724c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0db9120c6f4452588c08882c74e04fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41aed139457042eba6f30d9dd8d6a0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60cd2a42cab2479f93a9f4fa96f6eec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, language=language, task=task)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394e5cd-23b8-413e-8bde-88c3542b84fa",
   "metadata": {},
   "source": [
    "#### 移除数据集中不必要的字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1690dc5a-c1f7-4556-9be3-d31ad888e52e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:46:11.747262Z",
     "iopub.status.busy": "2024-01-16T13:46:11.746388Z",
     "iopub.status.idle": "2024-01-16T13:46:11.762614Z",
     "shell.execute_reply": "2024-01-16T13:46:11.761884Z",
     "shell.execute_reply.started": "2024-01-16T13:46:11.747216Z"
    }
   },
   "outputs": [],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309aff16-ea26-4474-af54-7ef244783999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:46:14.664762Z",
     "iopub.status.busy": "2024-01-16T13:46:14.664258Z",
     "iopub.status.idle": "2024-01-16T13:46:14.688289Z",
     "shell.execute_reply": "2024-01-16T13:46:14.687454Z",
     "shell.execute_reply.started": "2024-01-16T13:46:14.664716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/yuxiang/.cache/huggingface/datasets/downloads/extracted/68855bbd878ec603e04e5baebb8217d4b60e177bbe23157096f27ede1a5cff9c/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([-6.82121026e-13, -2.27373675e-12, -2.27373675e-12, ...,\n",
       "          1.21667399e-05,  3.23003678e-06, -2.43066324e-07]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881546ab-72e4-4bcf-852f-a8be736164b7",
   "metadata": {},
   "source": [
    "#### 降采样音频数据\n",
    "\n",
    "查看`common_voice` 数据集介绍，你会发现其音频是以48kHz的采样率进行采样的.\n",
    "\n",
    "而`Whisper`模型是在16kHZ的音频输入上预训练的，因此我们需要将音频输入降采样以匹配模型预训练时使用的采样率。\n",
    "\n",
    "通过在音频列上使用`cast_column`方法，并将`sampling_rate`设置为16kHz来对音频进行降采样。\n",
    "\n",
    "下次调用时，音频输入将实时重新取样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc451cc-e21e-473c-a702-d7d6ed098f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:46:19.680102Z",
     "iopub.status.busy": "2024-01-16T13:46:19.679577Z",
     "iopub.status.idle": "2024-01-16T13:46:19.698910Z",
     "shell.execute_reply": "2024-01-16T13:46:19.698165Z",
     "shell.execute_reply.started": "2024-01-16T13:46:19.680059Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc3d7fcc-7c34-41c8-9857-5a6e883f6115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:46:25.116026Z",
     "iopub.status.busy": "2024-01-16T13:46:25.115475Z",
     "iopub.status.idle": "2024-01-16T13:46:25.134817Z",
     "shell.execute_reply": "2024-01-16T13:46:25.134102Z",
     "shell.execute_reply.started": "2024-01-16T13:46:25.115978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/yuxiang/.cache/huggingface/datasets/downloads/extracted/68855bbd878ec603e04e5baebb8217d4b60e177bbe23157096f27ede1a5cff9c/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([ 5.09317033e-11, -7.27595761e-12, -6.54836185e-11, ...,\n",
       "         -5.96661994e-06,  2.71382887e-05,  1.29687978e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55908f-3ea3-4aee-8062-6f8d3a6573b9",
   "metadata": {},
   "source": [
    "### 整合以上数据处理为一个函数\n",
    "\n",
    "该数据预处理函数应该包括：\n",
    "- 通过加载音频列将音频输入重新采样为16kHZ。\n",
    "- 使用特征提取器从音频数组计算输入特征。\n",
    "- 将句子列标记化为输入标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58f42c35-35ba-4d6b-9d15-095963cec67c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:46:28.435207Z",
     "iopub.status.busy": "2024-01-16T13:46:28.434675Z",
     "iopub.status.idle": "2024-01-16T13:46:28.441979Z",
     "shell.execute_reply": "2024-01-16T13:46:28.440982Z",
     "shell.execute_reply.started": "2024-01-16T13:46:28.435161Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392f7856-a720-40a7-af7e-40e185fc315b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T13:46:32.766067Z",
     "iopub.status.busy": "2024-01-16T13:46:32.765589Z",
     "iopub.status.idle": "2024-01-16T15:11:36.411351Z",
     "shell.execute_reply": "2024-01-16T15:11:36.410177Z",
     "shell.execute_reply.started": "2024-01-16T13:46:32.766022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afcbe27a36445dc89483795ac0c620b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39637 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c329685e92504916ad42cd2f72390ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10581 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ec184e-d840-40b6-99af-d11392273442",
   "metadata": {},
   "source": [
    "创建一个`DataCollator`类来将每个批次中的`attention_mask`填充到最大长度，并用`-100`替换填充值，以便在损失函数中被忽略。\n",
    "\n",
    "然后初始化数据收集器的实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c89ffcf-c805-48c2-b7d3-ae01b687178c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:22:39.179052Z",
     "iopub.status.busy": "2024-01-16T15:22:39.178518Z",
     "iopub.status.idle": "2024-01-16T15:22:39.192927Z",
     "shell.execute_reply": "2024-01-16T15:22:39.192306Z",
     "shell.execute_reply.started": "2024-01-16T15:22:39.179006Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ecd4bc-01fd-4286-afe5-fe2639ae15a1",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9fcb121-fa5c-4c30-8bdc-9ab08ab75427",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:22:43.504249Z",
     "iopub.status.busy": "2024-01-16T15:22:43.503789Z",
     "iopub.status.idle": "2024-01-16T15:28:20.658326Z",
     "shell.execute_reply": "2024-01-16T15:28:20.657661Z",
     "shell.execute_reply.started": "2024-01-16T15:22:43.504216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e8773506584ec8ad8ba6bfad39d0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d06b89debf486bab4aaf54897128df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ddb105f9564dacbc1dbdc2f562379b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb016f1-e6e9-4fd8-9c8b-72fd23be92d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:30:23.715212Z",
     "iopub.status.busy": "2024-01-16T15:30:23.714716Z",
     "iopub.status.idle": "2024-01-16T15:30:23.721247Z",
     "shell.execute_reply": "2024-01-16T15:30:23.719985Z",
     "shell.execute_reply.started": "2024-01-16T15:30:23.715165Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba1fa0-ea15-48d9-8c16-70df9f0b60b1",
   "metadata": {},
   "source": [
    "为了准备模型进行int8量化，使用 `prepare_model_for_int8_training` 函数来处理模型：\n",
    "- 将所有非int8模块转换为完全精度（fp32）以保持稳定性\n",
    "- 在输入嵌入层上添加前向钩子，计算输入隐藏状态的梯度\n",
    "- 启用渐变检查点以进行更高效的内存训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee34359-fe1b-48f1-827c-6a8ec4a53af7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:30:27.523365Z",
     "iopub.status.busy": "2024-01-16T15:30:27.522907Z",
     "iopub.status.idle": "2024-01-16T15:30:27.643905Z",
     "shell.execute_reply": "2024-01-16T15:30:27.642963Z",
     "shell.execute_reply.started": "2024-01-16T15:30:27.523323Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6f8a2-867f-4ed5-bad5-15ca9fd9547c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdf6bc9c-6d2c-4dbf-b09e-a89cb1041c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:30:41.224783Z",
     "iopub.status.busy": "2024-01-16T15:30:41.224315Z",
     "iopub.status.idle": "2024-01-16T15:30:41.231373Z",
     "shell.execute_reply": "2024-01-16T15:30:41.230169Z",
     "shell.execute_reply.started": "2024-01-16T15:30:41.224741Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b74c7508-e6f4-42d8-8aaf-fe83c5977c35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:30:44.738331Z",
     "iopub.status.busy": "2024-01-16T15:30:44.737877Z",
     "iopub.status.idle": "2024-01-16T15:30:45.014169Z",
     "shell.execute_reply": "2024-01-16T15:30:45.013382Z",
     "shell.execute_reply.started": "2024-01-16T15:30:44.738291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,932,160 || all params: 1,547,237,120 || trainable%: 0.25414074863974306\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6b26a-3e54-4a46-9b36-a048b40a37d7",
   "metadata": {},
   "source": [
    "### 演示需要，只训练了100 steps。建议同学改为默认的 3个 epochs 完整训练一个中文语音识别模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11f259c8-dbcf-4a7f-bbb5-821ab104efee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:30:57.809531Z",
     "iopub.status.busy": "2024-01-16T15:30:57.809055Z",
     "iopub.status.idle": "2024-01-16T15:30:57.842746Z",
     "shell.execute_reply": "2024-01-16T15:30:57.841880Z",
     "shell.execute_reply.started": "2024-01-16T15:30:57.809487Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# 设置序列到序列模型训练的参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/whisper-large-v2-asr-int8\",  # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=batch_size,  # 每个设备上的训练批量大小\n",
    "    gradient_accumulation_steps=1,  # 梯度累积步数，在每次优化器步骤之前累积的更新步数\n",
    "    learning_rate=1e-3,  # 学习率\n",
    "    warmup_steps=50,  # 在训练初期增加学习率的步数，有助于稳定训练\n",
    "    max_steps=100, # 训练总步数\n",
    "    # num_train_epochs=3,  # 训练的总轮数\n",
    "    # evaluation_strategy=\"epoch\",  # 设置评估策略，这里是在每个epoch结束时进行评估\n",
    "    fp16=True,  # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    per_device_eval_batch_size=batch_size,  # 每个设备上的评估批量大小\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,  # 生成任务的最大长度\n",
    "    logging_steps=25,  # 指定日志记录的步骤，用于跟踪训练进度\n",
    "    remove_unused_columns=False,  # 是否删除不使用的列，以减少数据处理开销\n",
    "    label_names=[\"labels\"],  # 指定标签列的名称，用于训练过程中\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ee183-b16f-4313-97f6-0df6c0f5f467",
   "metadata": {},
   "source": [
    "#### 训练过程保存状态的回调，长时期训练建议使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ce443d9-f309-4c03-bd74-c6842292b713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:31:01.568234Z",
     "iopub.status.busy": "2024-01-16T15:31:01.567649Z",
     "iopub.status.idle": "2024-01-16T15:31:01.651875Z",
     "shell.execute_reply": "2024-01-16T15:31:01.651189Z",
     "shell.execute_reply.started": "2024-01-16T15:31:01.568190Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers import Seq2SeqTrainer, TrainerCallback, Seq2SeqTrainingArguments, TrainerState, TrainerControl\n",
    "\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: Seq2SeqTrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8a52ed7-cae0-4aba-818e-87717430d908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:31:07.398068Z",
     "iopub.status.busy": "2024-01-16T15:31:07.397138Z",
     "iopub.status.idle": "2024-01-16T15:31:07.411118Z",
     "shell.execute_reply": "2024-01-16T15:31:07.410087Z",
     "shell.execute_reply.started": "2024-01-16T15:31:07.398022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6973bed7-8f53-4d55-966c-f037941e5ef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T15:31:12.968865Z",
     "iopub.status.busy": "2024-01-16T15:31:12.968398Z",
     "iopub.status.idle": "2024-01-16T16:12:10.189652Z",
     "shell.execute_reply": "2024-01-16T16:12:10.188723Z",
     "shell.execute_reply.started": "2024-01-16T15:31:12.968822Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 40:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.407300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.391600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.9084253120422363, metrics={'train_runtime': 2456.9063, 'train_samples_per_second': 2.605, 'train_steps_per_second': 0.041, 'total_flos': 1.362453331968e+19, 'train_loss': 0.9084253120422363, 'epoch': 0.16})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620992c3-64f5-48f9-8e66-fdc5f6a27427",
   "metadata": {},
   "source": [
    "### 保存 LoRA 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53310565-7313-46a7-acf1-215970fd4f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T16:15:34.383087Z",
     "iopub.status.busy": "2024-01-16T16:15:34.382549Z",
     "iopub.status.idle": "2024-01-16T16:15:34.558890Z",
     "shell.execute_reply": "2024-01-16T16:15:34.558162Z",
     "shell.execute_reply.started": "2024-01-16T16:15:34.383038Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/whisper-large-v2-asr-int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe9611-eee5-462f-8cb8-fed86eec76e0",
   "metadata": {},
   "source": [
    "### 使用 Pipiline 加载 LoRA 模型，实现自动语音识别任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18181692-a143-44ee-b56c-e754d308e0ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T16:15:40.899785Z",
     "iopub.status.busy": "2024-01-16T16:15:40.899000Z",
     "iopub.status.idle": "2024-01-16T16:15:40.905302Z",
     "shell.execute_reply": "2024-01-16T16:15:40.904106Z",
     "shell.execute_reply.started": "2024-01-16T16:15:40.899737Z"
    }
   },
   "outputs": [],
   "source": [
    "test_audio = \"data/audio/test_zh.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d494647-082c-4e48-9486-7945618ae679",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T16:15:49.937997Z",
     "iopub.status.busy": "2024-01-16T16:15:49.937531Z",
     "iopub.status.idle": "2024-01-16T16:15:49.979347Z",
     "shell.execute_reply": "2024-01-16T16:15:49.978440Z",
     "shell.execute_reply.started": "2024-01-16T16:15:49.937954Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModel' is not supported for . Supported models are ['Pop2PianoForConditionalGeneration', 'SeamlessM4TForSpeechToText', 'SeamlessM4Tv2ForSpeechToText', 'SpeechEncoderDecoderModel', 'Speech2TextForConditionalGeneration', 'SpeechT5ForSpeechToText', 'WhisperForConditionalGeneration', 'Data2VecAudioForCTC', 'HubertForCTC', 'MCTCTForCTC', 'SEWForCTC', 'SEWDForCTC', 'UniSpeechForCTC', 'UniSpeechSatForCTC', 'Wav2Vec2ForCTC', 'Wav2Vec2ConformerForCTC', 'WavLMForCTC'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "pipeline = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"chinese\", task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3eac486-169f-41ad-b9c3-69f2c27c3e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T16:15:54.664426Z",
     "iopub.status.busy": "2024-01-16T16:15:54.663959Z",
     "iopub.status.idle": "2024-01-16T16:16:09.717493Z",
     "shell.execute_reply": "2024-01-16T16:16:09.716235Z",
     "shell.execute_reply.started": "2024-01-16T16:15:54.664383Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.amp.autocast():\n",
    "    text = pipeline(test_audio, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids}, max_new_tokens=255)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc4b24b2-c65b-4e63-8f16-26c72039a38d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-16T16:16:09.719105Z",
     "iopub.status.busy": "2024-01-16T16:16:09.718893Z",
     "iopub.status.idle": "2024-01-16T16:16:09.724008Z",
     "shell.execute_reply": "2024-01-16T16:16:09.723164Z",
     "shell.execute_reply.started": "2024-01-16T16:16:09.719088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这是一段测试用于WhisperLarge V2模型的自动语音识别测试。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4859d84-7dfb-437d-862b-593ee120932a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T13:51:57.435335Z",
     "iopub.status.busy": "2024-01-17T13:51:57.434844Z",
     "iopub.status.idle": "2024-01-17T13:52:12.644384Z",
     "shell.execute_reply": "2024-01-17T13:52:12.643696Z",
     "shell.execute_reply.started": "2024-01-17T13:51:57.435288Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载训练后的模型\n",
    "\n",
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model_path = \"models/whisper-large-v2-asr-int8\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bef8e8c-babd-4299-8da5-6345bbca9e66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T13:52:12.645849Z",
     "iopub.status.busy": "2024-01-17T13:52:12.645547Z",
     "iopub.status.idle": "2024-01-17T13:53:47.263082Z",
     "shell.execute_reply": "2024-01-17T13:53:47.261700Z",
     "shell.execute_reply.started": "2024-01-17T13:52:12.645830Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c9f6f3528548cea7542457543769ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=28):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "# 加载测试集\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "# common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\")\n",
    "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\").select(range(1000))\n",
    "\n",
    "# 准备特征提取器和分词器\n",
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, language=language, task=task)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_name_or_path, language=language, task=task)\n",
    "\n",
    "# 移除数据集中不必要的字段\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "# 降采样音频数据\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# 预处理函数\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# 执行预处理操作\n",
    "common_voice = common_voice.map(prepare_dataset, num_proc=28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a81ec0-3f89-430f-8f05-05d251ef22c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T13:53:47.266158Z",
     "iopub.status.busy": "2024-01-17T13:53:47.265169Z",
     "iopub.status.idle": "2024-01-17T13:53:47.275800Z",
     "shell.execute_reply": "2024-01-17T13:53:47.275162Z",
     "shell.execute_reply.started": "2024-01-17T13:53:47.266107Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化数据收集器的实例\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24fccce-fec3-48a3-b43c-b9077788521d",
   "metadata": {},
   "source": [
    "## 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b021c6a8-645d-44f7-970b-f410180787a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T13:53:47.278056Z",
     "iopub.status.busy": "2024-01-17T13:53:47.277743Z",
     "iopub.status.idle": "2024-01-17T13:54:01.354365Z",
     "shell.execute_reply": "2024-01-17T13:54:01.353237Z",
     "shell.execute_reply.started": "2024-01-17T13:53:47.278030Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# 词错误率（WER）是评估ASR模型常用的指标。从 Evaluate加载 WER 指标\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5156ce16-0e04-4e41-b308-52c6c9b2a20d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T13:54:01.356157Z",
     "iopub.status.busy": "2024-01-17T13:54:01.355492Z",
     "iopub.status.idle": "2024-01-17T13:54:01.393007Z",
     "shell.execute_reply": "2024-01-17T13:54:01.392416Z",
     "shell.execute_reply.started": "2024-01-17T13:54:01.356127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): lora.Linear8bitLt(\n",
       "              (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear8bitLt(\n",
       "              (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): lora.Linear8bitLt(\n",
       "              (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear8bitLt(\n",
       "              (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): lora.Linear8bitLt(\n",
       "              (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear8bitLt(\n",
       "              (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "            )\n",
       "            (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=12, collate_fn=data_collator)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9120279e-b10a-44ea-9275-2152ec204fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T13:54:01.394314Z",
     "iopub.status.busy": "2024-01-17T13:54:01.394073Z",
     "iopub.status.idle": "2024-01-17T14:20:13.422933Z",
     "shell.execute_reply": "2024-01-17T14:20:13.422345Z",
     "shell.execute_reply.started": "2024-01-17T13:54:01.394291Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|██████████| 84/84 [26:12<00:00, 18.71s/it] \n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    decoder_input_ids=batch[\"labels\"][:, :4].to(\"cuda\"),\n",
    "                    max_new_tokens=255,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            metric.add_batch(\n",
    "                predictions=decoded_preds,\n",
    "                references=decoded_labels,\n",
    "            )\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad4d7e8-ee0d-484a-9ae7-490c8b9898a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-17T14:20:13.424116Z",
     "iopub.status.busy": "2024-01-17T14:20:13.423906Z",
     "iopub.status.idle": "2024-01-17T14:20:13.456659Z",
     "shell.execute_reply": "2024-01-17T14:20:13.456004Z",
     "shell.execute_reply.started": "2024-01-17T14:20:13.424095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer=66.3\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute()\n",
    "print(f\"{wer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f49787-6ab4-4bc1-91b8-a1c104c9feaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0285dd19-229e-4241-b680-71e25ab51dde",
   "metadata": {},
   "source": [
    "#### Homework 1: 为中文语料的训练过程增加过程评估，观察 Train Loss 和 Validation Loss 变化；\n",
    "#### Homework 2: LoRA 模型训练完成后，使用测试集进行完整的模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c90ad4c-70eb-43d1-96ec-cc74c4bae345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:35:09.044443Z",
     "iopub.status.busy": "2024-01-18T00:35:09.043969Z",
     "iopub.status.idle": "2024-01-18T00:35:09.056785Z",
     "shell.execute_reply": "2024-01-18T00:35:09.055696Z",
     "shell.execute_reply.started": "2024-01-18T00:35:09.044395Z"
    }
   },
   "outputs": [],
   "source": [
    "# 全局参数设置\n",
    "\n",
    "model_id = \"openai/whisper-large-v2\"\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "batch_size=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de38c0c6-c9b7-4dcd-8dc9-367f7f320cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:35:09.753514Z",
     "iopub.status.busy": "2024-01-18T00:35:09.753258Z",
     "iopub.status.idle": "2024-01-18T00:36:49.717723Z",
     "shell.execute_reply": "2024-01-18T00:36:49.717170Z",
     "shell.execute_reply.started": "2024-01-18T00:35:09.753489Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'client_id': '95368aab163e0387e4fd4991b4f2d8ccfbd4364bf656c860230501fd27dcedf087773e4695a6cf5de9c4f1d406d582283190d065cdfa36b0e2b060cffaca977e',\n",
       " 'path': '/home/yuxiang/.cache/huggingface/datasets/downloads/extracted/68855bbd878ec603e04e5baebb8217d4b60e177bbe23157096f27ede1a5cff9c/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       " 'audio': {'path': '/home/yuxiang/.cache/huggingface/datasets/downloads/extracted/68855bbd878ec603e04e5baebb8217d4b60e177bbe23157096f27ede1a5cff9c/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([-6.82121026e-13, -2.27373675e-12, -2.27373675e-12, ...,\n",
       "          1.21667399e-05,  3.23003678e-06, -2.43066324e-07]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。',\n",
       " 'up_votes': 2,\n",
       " 'down_votes': 0,\n",
       " 'age': '',\n",
       " 'gender': '',\n",
       " 'accent': '',\n",
       " 'locale': 'zh-CN',\n",
       " 'segment': ''}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据集（训练集+验证集，测试集）\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\")\n",
    "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\").select(range(1000))\n",
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913d57b0-d4cc-4402-a948-50770d356710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:36:49.719363Z",
     "iopub.status.busy": "2024-01-18T00:36:49.719031Z",
     "iopub.status.idle": "2024-01-18T00:36:55.574503Z",
     "shell.execute_reply": "2024-01-18T00:36:55.573455Z",
     "shell.execute_reply.started": "2024-01-18T00:36:49.719343Z"
    }
   },
   "outputs": [],
   "source": [
    "# 准备特征提前器和分词器\n",
    "\n",
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, language=language, task=task)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b56379b2-55d2-4bc9-b1bc-5129915a38c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:36:55.576136Z",
     "iopub.status.busy": "2024-01-18T00:36:55.575683Z",
     "iopub.status.idle": "2024-01-18T00:36:55.590274Z",
     "shell.execute_reply": "2024-01-18T00:36:55.589746Z",
     "shell.execute_reply.started": "2024-01-18T00:36:55.576115Z"
    }
   },
   "outputs": [],
   "source": [
    "# 移除数据集中不必要的字段\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "# 降采样音频数据\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef11249e-c774-4fa5-96ea-5c82b8c2016c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:36:55.592183Z",
     "iopub.status.busy": "2024-01-18T00:36:55.591963Z",
     "iopub.status.idle": "2024-01-18T00:36:55.603106Z",
     "shell.execute_reply": "2024-01-18T00:36:55.602469Z",
     "shell.execute_reply.started": "2024-01-18T00:36:55.592164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/yuxiang/.cache/huggingface/datasets/downloads/extracted/68855bbd878ec603e04e5baebb8217d4b60e177bbe23157096f27ede1a5cff9c/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([ 5.09317033e-11, -7.27595761e-12, -6.54836185e-11, ...,\n",
       "         -5.96661994e-06,  2.71382887e-05,  1.29687978e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525f0e0c-2819-4729-91e9-06edb8780afe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:36:55.604002Z",
     "iopub.status.busy": "2024-01-18T00:36:55.603778Z",
     "iopub.status.idle": "2024-01-18T00:36:55.804727Z",
     "shell.execute_reply": "2024-01-18T00:36:55.803910Z",
     "shell.execute_reply.started": "2024-01-18T00:36:55.603984Z"
    }
   },
   "outputs": [],
   "source": [
    "# 预处理函数\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# 执行数据集预处理\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8001f66d-f9fa-48d5-9862-95bcb326e180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:36:55.806010Z",
     "iopub.status.busy": "2024-01-18T00:36:55.805679Z",
     "iopub.status.idle": "2024-01-18T00:36:55.812259Z",
     "shell.execute_reply": "2024-01-18T00:36:55.811687Z",
     "shell.execute_reply.started": "2024-01-18T00:36:55.805991Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化数据收集器的实例\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c55bef-0b74-4b9f-8efc-3039c0672b0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:36:55.813254Z",
     "iopub.status.busy": "2024-01-18T00:36:55.813061Z",
     "iopub.status.idle": "2024-01-18T00:37:05.667918Z",
     "shell.execute_reply": "2024-01-18T00:37:05.667032Z",
     "shell.execute_reply.started": "2024-01-18T00:36:55.813234Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "\n",
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f356d6-7c05-436e-88a6-575288a018df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:05.669264Z",
     "iopub.status.busy": "2024-01-18T00:37:05.669020Z",
     "iopub.status.idle": "2024-01-18T00:37:05.674226Z",
     "shell.execute_reply": "2024-01-18T00:37:05.673519Z",
     "shell.execute_reply.started": "2024-01-18T00:37:05.669240Z"
    }
   },
   "outputs": [],
   "source": [
    "# 原始 Whisper 模型在自回归生成开始之前强制添加了若干前缀词元 ID (forced_decoder_ids)。这些词元 ID 主要用于在零样本 ASR 任务中标识语种和任务。\n",
    "# 因为我们现在是对已知语种 (中文) 和任务 (转录) 进行微调，所以我们要将 forced_decoder_ids 设置为 None。\n",
    "# 另外，模型还抑制了一些词元 (suppress_tokens)，这些词元的对数概率被强置为 -inf，以保证它们永远不会被采样到。我们会用一个空列表覆盖 suppress_tokens，即我们不抑制任何词元。\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "808bc138-a41e-4ed6-93ed-71b03e606622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:05.675335Z",
     "iopub.status.busy": "2024-01-18T00:37:05.675065Z",
     "iopub.status.idle": "2024-01-18T00:37:05.684429Z",
     "shell.execute_reply": "2024-01-18T00:37:05.683797Z",
     "shell.execute_reply.started": "2024-01-18T00:37:05.675314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 1543.62 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory footprint: {model.get_memory_footprint() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09686b8b-2b41-4822-8eec-f5b0b3812dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:05.687422Z",
     "iopub.status.busy": "2024-01-18T00:37:05.687191Z",
     "iopub.status.idle": "2024-01-18T00:37:05.750490Z",
     "shell.execute_reply": "2024-01-18T00:37:05.749812Z",
     "shell.execute_reply.started": "2024-01-18T00:37:05.687400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 为了准备模型进行int8量化，使用 `prepare_model_for_int8_training` 函数来处理模型\n",
    "# prepare_model_for_int8_training() function:\n",
    "# 1) casts all the non int8 modules to full precision (fp32) for stability\n",
    "# 2) adds a forward hook to the input embedding layer to calculate the gradients of the input hidden states\n",
    "# 3) enables gradient checkpointing for more memory-efficient training\n",
    "\n",
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f8fd25a-3b17-4070-bf67-a2f127846134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:05.751381Z",
     "iopub.status.busy": "2024-01-18T00:37:05.751196Z",
     "iopub.status.idle": "2024-01-18T00:37:05.758871Z",
     "shell.execute_reply": "2024-01-18T00:37:05.758309Z",
     "shell.execute_reply.started": "2024-01-18T00:37:05.751364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 1687.24 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory footprint: {model.get_memory_footprint() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "950cc5ed-2fa0-4c8c-88ef-627b4395cb65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:05.759762Z",
     "iopub.status.busy": "2024-01-18T00:37:05.759576Z",
     "iopub.status.idle": "2024-01-18T00:37:05.763752Z",
     "shell.execute_reply": "2024-01-18T00:37:05.763115Z",
     "shell.execute_reply.started": "2024-01-18T00:37:05.759746Z"
    }
   },
   "outputs": [],
   "source": [
    "# 配置 LoRA 参数\n",
    "\n",
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "115c281e-6ab2-4af7-ae1f-2e1849662d3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:05.764770Z",
     "iopub.status.busy": "2024-01-18T00:37:05.764524Z",
     "iopub.status.idle": "2024-01-18T00:37:05.969801Z",
     "shell.execute_reply": "2024-01-18T00:37:05.969050Z",
     "shell.execute_reply.started": "2024-01-18T00:37:05.764753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,932,160 || all params: 1,547,237,120 || trainable%: 0.25414074863974306\n"
     ]
    }
   ],
   "source": [
    "# 获取 PEFT 模型\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc3325e2-1c45-406e-9bb0-e5b4cdcc89b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:05.970864Z",
     "iopub.status.busy": "2024-01-18T00:37:05.970668Z",
     "iopub.status.idle": "2024-01-18T00:37:05.990801Z",
     "shell.execute_reply": "2024-01-18T00:37:05.990129Z",
     "shell.execute_reply.started": "2024-01-18T00:37:05.970847Z"
    }
   },
   "outputs": [],
   "source": [
    "# 配置训练超参数（TrainingArguments）\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# 设置序列到序列模型训练的参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/whisper-large-v2-asr-int8\",  # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=batch_size,  # 每个设备上的训练批量大小\n",
    "    gradient_accumulation_steps=1,  # 梯度累积步数，在每次优化器步骤之前累积的更新步数\n",
    "    learning_rate=1e-3,  # 学习率\n",
    "    warmup_steps=50,  # 在训练初期增加学习率的步数，有助于稳定训练\n",
    "    # max_steps=5000, # 训练总步数\n",
    "    num_train_epochs=3,  # 训练的总轮数\n",
    "    evaluation_strategy=\"epoch\",  # 设置评估策略，可选值：no, steps, epoch，这里是在每个epoch结束时进行评估\n",
    "    # eval_steps=500,\n",
    "    fp16=True,  # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    per_device_eval_batch_size=12,  # 每个设备上的评估批量大小\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,  # 生成任务的最大长度\n",
    "    logging_steps=100,  # 指定日志记录的步骤，用于跟踪训练进度\n",
    "    remove_unused_columns=False,  # 是否删除不使用的列，以减少数据处理开销\n",
    "    label_names=[\"labels\"],  # 指定标签列的名称，用于训练过程中\n",
    "    dataloader_num_workers=24, # Number of subprocesses to use for data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0791cb0-8c38-4faa-8c3c-e4ce724f6fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:05.991690Z",
     "iopub.status.busy": "2024-01-18T00:37:05.991505Z",
     "iopub.status.idle": "2024-01-18T00:37:06.045728Z",
     "shell.execute_reply": "2024-01-18T00:37:06.045088Z",
     "shell.execute_reply.started": "2024-01-18T00:37:05.991673Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练过程保存状态的回调\n",
    "\n",
    "import os\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers import Seq2SeqTrainer, TrainerCallback, Seq2SeqTrainingArguments, TrainerState, TrainerControl\n",
    "\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: Seq2SeqTrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "897b78ef-ccc7-43e2-bde4-fccbcce162b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:06.048134Z",
     "iopub.status.busy": "2024-01-18T00:37:06.047481Z",
     "iopub.status.idle": "2024-01-18T00:37:19.662676Z",
     "shell.execute_reply": "2024-01-18T00:37:19.661461Z",
     "shell.execute_reply.started": "2024-01-18T00:37:06.048116Z"
    }
   },
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# 词错误率（WER）是评估ASR模型常用的指标。从 Evaluate加载 WER 指标\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    # replace -100 with the pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    wer = 100 * metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4d06bcf-80b5-4df4-a3e7-8bb4251140e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:19.663961Z",
     "iopub.status.busy": "2024-01-18T00:37:19.663698Z",
     "iopub.status.idle": "2024-01-18T00:37:19.672947Z",
     "shell.execute_reply": "2024-01-18T00:37:19.672157Z",
     "shell.execute_reply.started": "2024-01-18T00:37:19.663936Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 实例化训练器（Trainer）\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b655f45-6aa7-4dd1-bfd6-dbc06ac4e5f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T00:37:19.674342Z",
     "iopub.status.busy": "2024-01-18T00:37:19.674086Z",
     "iopub.status.idle": "2024-01-18T08:17:41.604950Z",
     "shell.execute_reply": "2024-01-18T08:17:41.604171Z",
     "shell.execute_reply.started": "2024-01-18T00:37:19.674319Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1488' max='1488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1488/1488 7:39:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.426020</td>\n",
       "      <td>67.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.287200</td>\n",
       "      <td>0.407857</td>\n",
       "      <td>65.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.227400</td>\n",
       "      <td>0.410588</td>\n",
       "      <td>65.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1488, training_loss=0.3278871031217678, metrics={'train_runtime': 27621.6572, 'train_samples_per_second': 4.305, 'train_steps_per_second': 0.054, 'total_flos': 2.531417002463232e+20, 'train_loss': 0.3278871031217678, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdb48a0b-4dae-49e6-8698-6e0de5685d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T08:17:41.606511Z",
     "iopub.status.busy": "2024-01-18T08:17:41.606266Z",
     "iopub.status.idle": "2024-01-18T08:17:41.689938Z",
     "shell.execute_reply": "2024-01-18T08:17:41.689242Z",
     "shell.execute_reply.started": "2024-01-18T08:17:41.606488Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存 LoRA 模型\n",
    "\n",
    "model.save_pretrained(\"models/whisper-large-v2-asr-int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99b58a13-dfa1-4ee3-9eb5-8741191b4f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T08:17:41.691076Z",
     "iopub.status.busy": "2024-01-18T08:17:41.690883Z",
     "iopub.status.idle": "2024-01-18T08:37:19.090050Z",
     "shell.execute_reply": "2024-01-18T08:37:19.089324Z",
     "shell.execute_reply.started": "2024-01-18T08:17:41.691059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 19:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4105878472328186,\n",
       " 'eval_wer': 65.5,\n",
       " 'eval_runtime': 1177.3944,\n",
       " 'eval_samples_per_second': 0.849,\n",
       " 'eval_steps_per_second': 0.071,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估模型最终准确率（方法一）\n",
    "\n",
    "trainer.evaluate(common_voice[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d6d390a-545a-4159-af09-98aba7756b63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T08:37:19.091364Z",
     "iopub.status.busy": "2024-01-18T08:37:19.091172Z",
     "iopub.status.idle": "2024-01-18T08:37:32.692243Z",
     "shell.execute_reply": "2024-01-18T08:37:32.691047Z",
     "shell.execute_reply.started": "2024-01-18T08:37:19.091344Z"
    }
   },
   "outputs": [],
   "source": [
    "# 评估模型最终准确率（方法二）\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# 词错误率（WER）是评估ASR模型常用的指标。从 Evaluate加载 WER 指标\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78f68629-368c-4ed2-8aa2-9341765721cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T08:37:32.694039Z",
     "iopub.status.busy": "2024-01-18T08:37:32.693437Z",
     "iopub.status.idle": "2024-01-18T08:37:32.734574Z",
     "shell.execute_reply": "2024-01-18T08:37:32.733685Z",
     "shell.execute_reply.started": "2024-01-18T08:37:32.694011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=12, collate_fn=data_collator)\n",
    "\n",
    "# model.train()\n",
    "# 在使用 pytorch 构建神经网络的时候，训练过程中会在程序上方添加一句model.train()，作用是 启用 batch normalization 和 dropout。\n",
    "# 如果模型中有BN层（Batch Normalization）和 Dropout ，需要在训练时添加 model.train()。\n",
    "# model.train() 是保证 BN 层能够用到每一批数据的均值和方差。对于 Dropout，model.train() 是随机取一部分网络连接来训练更新参数。\n",
    "\n",
    "# model.eval()\n",
    "# model.eval()的作用是不启用 Batch Normalization 和 Dropout。\n",
    "# 如果模型中有 BN 层（Batch Normalization）和 Dropout，在测试时添加model.eval()。\n",
    "# model.eval() 是保证 BN 层能够用全部训练数据的均值和方差，即测试过程中要保证 BN 层的均值和方差不变。对于 Dropout，model.eval() 是利用到了所有网络连接，即不进行随机舍弃神经元。\n",
    "\n",
    "# 为什么测试时要用 model.eval() ？\n",
    "# 训练完 train 样本后，生成的模型 model 要用来测试样本了。在 model(test) 之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是 model 中含有 BN 层和 Dropout 所带来的的性质。\n",
    "# model.eval() 时，pytorch 会自动把 BN 和 DropOut 固定住，不会取平均，而是用训练好的值。\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "873a9472-8c1f-4827-b72f-1e38ca8a30e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T08:37:32.736622Z",
     "iopub.status.busy": "2024-01-18T08:37:32.736296Z",
     "iopub.status.idle": "2024-01-18T08:55:25.291324Z",
     "shell.execute_reply": "2024-01-18T08:55:25.290760Z",
     "shell.execute_reply.started": "2024-01-18T08:37:32.736591Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [17:52<00:00, 12.77s/it]\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    decoder_input_ids=batch[\"labels\"][:, :4].to(\"cuda\"),\n",
    "                    max_new_tokens=255,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            metric.add_batch(\n",
    "                predictions=decoded_preds,\n",
    "                references=decoded_labels,\n",
    "            )\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb746cde-23ac-4516-a5da-cc605fb1bcab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T08:55:25.292634Z",
     "iopub.status.busy": "2024-01-18T08:55:25.292463Z",
     "iopub.status.idle": "2024-01-18T08:55:25.324093Z",
     "shell.execute_reply": "2024-01-18T08:55:25.323244Z",
     "shell.execute_reply.started": "2024-01-18T08:55:25.292617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer=59.4\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute()\n",
    "print(f\"{wer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "814fd43c-4087-4539-8252-ead3a22d62ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T08:55:25.324917Z",
     "iopub.status.busy": "2024-01-18T08:55:25.324739Z",
     "iopub.status.idle": "2024-01-18T08:55:32.908865Z",
     "shell.execute_reply": "2024-01-18T08:55:32.908338Z",
     "shell.execute_reply.started": "2024-01-18T08:55:25.324902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModel' is not supported for . Supported models are ['Pop2PianoForConditionalGeneration', 'SeamlessM4TForSpeechToText', 'SeamlessM4Tv2ForSpeechToText', 'SpeechEncoderDecoderModel', 'Speech2TextForConditionalGeneration', 'SpeechT5ForSpeechToText', 'WhisperForConditionalGeneration', 'Data2VecAudioForCTC', 'HubertForCTC', 'MCTCTForCTC', 'SEWForCTC', 'SEWDForCTC', 'UniSpeechForCTC', 'UniSpeechSatForCTC', 'Wav2Vec2ForCTC', 'Wav2Vec2ConformerForCTC', 'WavLMForCTC'].\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'这是一段测试用于Whisperler Large V2模型的自动语音识别测试。'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试模型\n",
    "test_audio = \"data/audio/test_zh.flac\"\n",
    "\n",
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "pipeline = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"chinese\", task=task)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    text = pipeline(test_audio, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids}, max_new_tokens=255)[\"text\"]\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa65229-4a2f-4afe-a075-8ac11468aaf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-20T15:21:21.886898Z",
     "iopub.status.busy": "2024-01-20T15:21:21.886445Z",
     "iopub.status.idle": "2024-01-20T15:21:21.892186Z",
     "shell.execute_reply": "2024-01-20T15:21:21.891231Z",
     "shell.execute_reply.started": "2024-01-20T15:21:21.886854Z"
    }
   },
   "source": [
    "#### 重新加载保存后的模型，继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c8eb1b-c55d-4956-994e-20a00453817b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:01:52.899694Z",
     "iopub.status.busy": "2024-01-21T08:01:52.899253Z",
     "iopub.status.idle": "2024-01-21T08:01:52.911905Z",
     "shell.execute_reply": "2024-01-21T08:01:52.910820Z",
     "shell.execute_reply.started": "2024-01-21T08:01:52.899649Z"
    }
   },
   "outputs": [],
   "source": [
    "# 全局参数设置\n",
    "\n",
    "# 模型训练后的保存路径\n",
    "model_path = \"models/whisper-large-v2-asr-int8\"\n",
    "# 原有模型的 model_id\n",
    "model_id = \"openai/whisper-large-v2\"\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "batch_size=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e00cf4-4683-45bb-8170-7d43c36e20a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:01:53.444148Z",
     "iopub.status.busy": "2024-01-21T08:01:53.443881Z",
     "iopub.status.idle": "2024-01-21T08:03:32.806978Z",
     "shell.execute_reply": "2024-01-21T08:03:32.806016Z",
     "shell.execute_reply.started": "2024-01-21T08:01:53.444122Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/datasets/load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_11_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_11_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集（训练集+验证集，测试集）\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\")\n",
    "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\").select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e15c2b6-008f-40ae-9156-32c267ae0f61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:32.809097Z",
     "iopub.status.busy": "2024-01-21T08:03:32.808716Z",
     "iopub.status.idle": "2024-01-21T08:03:38.496261Z",
     "shell.execute_reply": "2024-01-21T08:03:38.495514Z",
     "shell.execute_reply.started": "2024-01-21T08:03:32.809069Z"
    }
   },
   "outputs": [],
   "source": [
    "# 准备特征提前器和分词器\n",
    "\n",
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, language=language, task=task)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca0070d8-90f1-46d6-8811-9d4bd4b40257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:38.497731Z",
     "iopub.status.busy": "2024-01-21T08:03:38.497116Z",
     "iopub.status.idle": "2024-01-21T08:03:38.510182Z",
     "shell.execute_reply": "2024-01-21T08:03:38.509368Z",
     "shell.execute_reply.started": "2024-01-21T08:03:38.497710Z"
    }
   },
   "outputs": [],
   "source": [
    "# 移除数据集中不必要的字段\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "\n",
    "from datasets import Audio\n",
    "\n",
    "# 降采样音频数据\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7cf06b-5c14-408d-888e-db8e318a9e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:38.512103Z",
     "iopub.status.busy": "2024-01-21T08:03:38.511871Z",
     "iopub.status.idle": "2024-01-21T08:03:38.705321Z",
     "shell.execute_reply": "2024-01-21T08:03:38.704474Z",
     "shell.execute_reply.started": "2024-01-21T08:03:38.512085Z"
    }
   },
   "outputs": [],
   "source": [
    "# 预处理函数\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# 执行数据集预处理\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e32eb3-4b4e-4afc-bcdd-16285cbec86b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:38.706467Z",
     "iopub.status.busy": "2024-01-21T08:03:38.706171Z",
     "iopub.status.idle": "2024-01-21T08:03:38.712393Z",
     "shell.execute_reply": "2024-01-21T08:03:38.711843Z",
     "shell.execute_reply.started": "2024-01-21T08:03:38.706449Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化数据收集器的实例\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab122ac-e4e2-4e9f-9487-ac45c10a33b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:38.713440Z",
     "iopub.status.busy": "2024-01-21T08:03:38.713231Z",
     "iopub.status.idle": "2024-01-21T08:03:47.913684Z",
     "shell.execute_reply": "2024-01-21T08:03:47.912984Z",
     "shell.execute_reply.started": "2024-01-21T08:03:38.713422Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载训练后的模型\n",
    "\n",
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "# 加载基础模型\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_path, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df9d1d6-00ff-4d2b-850a-3abbd81338eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:47.914912Z",
     "iopub.status.busy": "2024-01-21T08:03:47.914649Z",
     "iopub.status.idle": "2024-01-21T08:03:47.918107Z",
     "shell.execute_reply": "2024-01-21T08:03:47.917557Z",
     "shell.execute_reply.started": "2024-01-21T08:03:47.914889Z"
    }
   },
   "outputs": [],
   "source": [
    "# 原始 Whisper 模型在自回归生成开始之前强制添加了若干前缀词元 ID (forced_decoder_ids)。这些词元 ID 主要用于在零样本 ASR 任务中标识语种和任务。\n",
    "# 因为我们现在是对已知语种 (中文) 和任务 (转录) 进行微调，所以我们要将 forced_decoder_ids 设置为 None。\n",
    "# 另外，模型还抑制了一些词元 (suppress_tokens)，这些词元的对数概率被强置为 -inf，以保证它们永远不会被采样到。我们会用一个空列表覆盖 suppress_tokens，即我们不抑制任何词元。\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5186b7d5-501b-4d53-a4c4-52603a9df2d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:47.918979Z",
     "iopub.status.busy": "2024-01-21T08:03:47.918759Z",
     "iopub.status.idle": "2024-01-21T08:03:47.933796Z",
     "shell.execute_reply": "2024-01-21T08:03:47.933302Z",
     "shell.execute_reply.started": "2024-01-21T08:03:47.918960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 1558.62 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory footprint: {model.get_memory_footprint() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea828a04-e86a-4d4e-8fe1-cdb96e99a51e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:47.934804Z",
     "iopub.status.busy": "2024-01-21T08:03:47.934615Z",
     "iopub.status.idle": "2024-01-21T08:03:47.973084Z",
     "shell.execute_reply": "2024-01-21T08:03:47.972526Z",
     "shell.execute_reply.started": "2024-01-21T08:03:47.934786Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 为了准备模型进行int8量化，使用 `prepare_model_for_int8_training` 函数来处理模型\n",
    "# prepare_model_for_int8_training() function:\n",
    "# 1) casts all the non int8 modules to full precision (fp32) for stability\n",
    "# 2) adds a forward hook to the input embedding layer to calculate the gradients of the input hidden states\n",
    "# 3) enables gradient checkpointing for more memory-efficient training\n",
    "\n",
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74782f80-0c65-457f-b34c-9db65f10850c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:47.975355Z",
     "iopub.status.busy": "2024-01-21T08:03:47.975139Z",
     "iopub.status.idle": "2024-01-21T08:03:47.987184Z",
     "shell.execute_reply": "2024-01-21T08:03:47.986672Z",
     "shell.execute_reply.started": "2024-01-21T08:03:47.975337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 1702.24 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory footprint: {model.get_memory_footprint() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d579074-d455-4acb-bad6-d5b2f0b66bb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:47.988084Z",
     "iopub.status.busy": "2024-01-21T08:03:47.987841Z",
     "iopub.status.idle": "2024-01-21T08:03:48.222815Z",
     "shell.execute_reply": "2024-01-21T08:03:48.222137Z",
     "shell.execute_reply.started": "2024-01-21T08:03:47.988052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# 加载经过 PEFT 微调的模型\n",
    "model = PeftModel.from_pretrained(model, model_path, is_trainable=True, device_map=\"auto\")\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88f79489-a5c2-4e8c-a0aa-67242265ed39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:48.224144Z",
     "iopub.status.busy": "2024-01-21T08:03:48.223785Z",
     "iopub.status.idle": "2024-01-21T08:03:48.236141Z",
     "shell.execute_reply": "2024-01-21T08:03:48.235471Z",
     "shell.execute_reply.started": "2024-01-21T08:03:48.224109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,932,160 || all params: 1,547,237,120 || trainable%: 0.25414074863974306\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84c36bee-ba1c-48a3-bfdd-2a67614165ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:48.237448Z",
     "iopub.status.busy": "2024-01-21T08:03:48.237096Z",
     "iopub.status.idle": "2024-01-21T08:03:48.262438Z",
     "shell.execute_reply": "2024-01-21T08:03:48.261859Z",
     "shell.execute_reply.started": "2024-01-21T08:03:48.237415Z"
    }
   },
   "outputs": [],
   "source": [
    "# 配置训练超参数（TrainingArguments）\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# 设置序列到序列模型训练的参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/whisper-large-v2-asr-int8\",  # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=batch_size,  # 每个设备上的训练批量大小\n",
    "    gradient_accumulation_steps=1,  # 梯度累积步数，在每次优化器步骤之前累积的更新步数\n",
    "    learning_rate=1e-3,  # 学习率\n",
    "    warmup_steps=50,  # 在训练初期增加学习率的步数，有助于稳定训练\n",
    "    # max_steps=5000, # 训练总步数\n",
    "    num_train_epochs=3,  # 训练的总轮数\n",
    "    evaluation_strategy=\"epoch\",  # 设置评估策略，可选值：no, steps, epoch，这里是在每个epoch结束时进行评估\n",
    "    # eval_steps=500,\n",
    "    fp16=True,  # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    per_device_eval_batch_size=12,  # 每个设备上的评估批量大小\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,  # 生成任务的最大长度\n",
    "    logging_steps=100,  # 指定日志记录的步骤，用于跟踪训练进度\n",
    "    remove_unused_columns=False,  # 是否删除不使用的列，以减少数据处理开销\n",
    "    label_names=[\"labels\"],  # 指定标签列的名称，用于训练过程中\n",
    "    dataloader_num_workers=24, # Number of subprocesses to use for data loading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51961cf-a7ef-49d8-828b-92352252c61f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:48.263781Z",
     "iopub.status.busy": "2024-01-21T08:03:48.263435Z",
     "iopub.status.idle": "2024-01-21T08:03:48.616874Z",
     "shell.execute_reply": "2024-01-21T08:03:48.616035Z",
     "shell.execute_reply.started": "2024-01-21T08:03:48.263748Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练过程保存状态的回调\n",
    "\n",
    "import os\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers import Seq2SeqTrainer, TrainerCallback, Seq2SeqTrainingArguments, TrainerState, TrainerControl\n",
    "\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: Seq2SeqTrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c91a263c-454d-4773-82cc-e9eb074b42cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:03:48.618615Z",
     "iopub.status.busy": "2024-01-21T08:03:48.618035Z",
     "iopub.status.idle": "2024-01-21T08:04:02.491730Z",
     "shell.execute_reply": "2024-01-21T08:04:02.490576Z",
     "shell.execute_reply.started": "2024-01-21T08:03:48.618586Z"
    }
   },
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# 词错误率（WER）是评估ASR模型常用的指标。从 Evaluate加载 WER 指标\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    # replace -100 with the pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    wer = 100 * metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b0ce273-a449-4a4e-b8f0-f463ff91bc12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:04:02.493437Z",
     "iopub.status.busy": "2024-01-21T08:04:02.493021Z",
     "iopub.status.idle": "2024-01-21T08:04:02.503631Z",
     "shell.execute_reply": "2024-01-21T08:04:02.502870Z",
     "shell.execute_reply.started": "2024-01-21T08:04:02.493397Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# 实例化训练器（Trainer）\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2933ab8e-43b3-4af6-aa2c-fedc1b534b98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T08:04:02.505349Z",
     "iopub.status.busy": "2024-01-21T08:04:02.504960Z",
     "iopub.status.idle": "2024-01-21T15:47:44.967238Z",
     "shell.execute_reply": "2024-01-21T15:47:44.966444Z",
     "shell.execute_reply.started": "2024-01-21T08:04:02.505313Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1488' max='1488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1488/1488 7:42:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.262800</td>\n",
       "      <td>0.437628</td>\n",
       "      <td>67.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>0.433860</td>\n",
       "      <td>66.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.443800</td>\n",
       "      <td>66.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1488, training_loss=0.20217879741422592, metrics={'train_runtime': 27822.1897, 'train_samples_per_second': 4.274, 'train_steps_per_second': 0.053, 'total_flos': 2.531417002463232e+20, 'train_loss': 0.20217879741422592, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 由最后一个 checkpoint 开始重新训练\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cde5f105-26c7-4f1b-a487-5ae4f74bc2b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T15:47:44.968728Z",
     "iopub.status.busy": "2024-01-21T15:47:44.968492Z",
     "iopub.status.idle": "2024-01-21T15:47:45.052550Z",
     "shell.execute_reply": "2024-01-21T15:47:45.051705Z",
     "shell.execute_reply.started": "2024-01-21T15:47:44.968705Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存 LoRA 模型\n",
    "\n",
    "model.save_pretrained(\"models/whisper-large-v2-asr-int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e160e0be-e7cf-44cd-b42f-bccc283e957e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T15:47:45.053980Z",
     "iopub.status.busy": "2024-01-21T15:47:45.053702Z",
     "iopub.status.idle": "2024-01-21T16:08:39.220958Z",
     "shell.execute_reply": "2024-01-21T16:08:39.220192Z",
     "shell.execute_reply.started": "2024-01-21T15:47:45.053952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 20:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4437997043132782,\n",
       " 'eval_wer': 66.5,\n",
       " 'eval_runtime': 1254.1623,\n",
       " 'eval_samples_per_second': 0.797,\n",
       " 'eval_steps_per_second': 0.067,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估模型最终准确率（方法一）\n",
    "\n",
    "trainer.evaluate(common_voice[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1b7033b-b6f4-4c8d-8f33-855884f4c7fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T16:08:39.222285Z",
     "iopub.status.busy": "2024-01-21T16:08:39.222076Z",
     "iopub.status.idle": "2024-01-21T16:08:52.813178Z",
     "shell.execute_reply": "2024-01-21T16:08:52.812147Z",
     "shell.execute_reply.started": "2024-01-21T16:08:39.222264Z"
    }
   },
   "outputs": [],
   "source": [
    "# 评估模型最终准确率（方法二）\n",
    "\n",
    "import evaluate\n",
    "\n",
    "# 词错误率（WER）是评估ASR模型常用的指标。从 Evaluate加载 WER 指标\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc3dec81-cde9-4cbc-af7a-062aa88a9c00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T16:08:52.814522Z",
     "iopub.status.busy": "2024-01-21T16:08:52.814248Z",
     "iopub.status.idle": "2024-01-21T16:08:52.844778Z",
     "shell.execute_reply": "2024-01-21T16:08:52.844195Z",
     "shell.execute_reply.started": "2024-01-21T16:08:52.814497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=12, collate_fn=data_collator)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2eca899b-7da0-4e36-bdc9-fbe83e82f7a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T16:08:52.846210Z",
     "iopub.status.busy": "2024-01-21T16:08:52.845877Z",
     "iopub.status.idle": "2024-01-21T16:25:49.925254Z",
     "shell.execute_reply": "2024-01-21T16:25:49.924691Z",
     "shell.execute_reply.started": "2024-01-21T16:08:52.846186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [16:57<00:00, 12.11s/it]\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    decoder_input_ids=batch[\"labels\"][:, :4].to(\"cuda\"),\n",
    "                    max_new_tokens=255,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            metric.add_batch(\n",
    "                predictions=decoded_preds,\n",
    "                references=decoded_labels,\n",
    "            )\n",
    "    del generated_tokens, labels, batch\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89bf2960-3bf1-4ffe-8470-ecf5c98af3f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T16:25:49.926689Z",
     "iopub.status.busy": "2024-01-21T16:25:49.926408Z",
     "iopub.status.idle": "2024-01-21T16:25:49.958065Z",
     "shell.execute_reply": "2024-01-21T16:25:49.957527Z",
     "shell.execute_reply.started": "2024-01-21T16:25:49.926670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer=61.4\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute()\n",
    "print(f\"{wer=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d37ad3be-539d-4ce3-bab7-10cf9e7630fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T16:25:49.958928Z",
     "iopub.status.busy": "2024-01-21T16:25:49.958726Z",
     "iopub.status.idle": "2024-01-21T16:25:57.434417Z",
     "shell.execute_reply": "2024-01-21T16:25:57.433203Z",
     "shell.execute_reply.started": "2024-01-21T16:25:49.958911Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModel' is not supported for . Supported models are ['Pop2PianoForConditionalGeneration', 'SeamlessM4TForSpeechToText', 'SeamlessM4Tv2ForSpeechToText', 'SpeechEncoderDecoderModel', 'Speech2TextForConditionalGeneration', 'SpeechT5ForSpeechToText', 'WhisperForConditionalGeneration', 'Data2VecAudioForCTC', 'HubertForCTC', 'MCTCTForCTC', 'SEWForCTC', 'SEWDForCTC', 'UniSpeechForCTC', 'UniSpeechSatForCTC', 'Wav2Vec2ForCTC', 'Wav2Vec2ConformerForCTC', 'WavLMForCTC'].\n",
      "/home/yuxiang/miniconda3/envs/py311_llm_dev/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'这是一段测试用于Whisperer Large V2模型的自动语音识别测试。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试模型\n",
    "test_audio = \"data/audio/test_zh.flac\"\n",
    "\n",
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "pipeline = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"chinese\", task=task)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    text = pipeline(test_audio, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids}, max_new_tokens=255)[\"text\"]\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84f627-0299-4fb9-bd37-8e0651e49317",
   "metadata": {},
   "source": [
    "## Use Adapters PEFT after training\n",
    "<https://huggingface.co/docs/trl/main/en/use_model>\n",
    "\n",
    "from peft import PeftConfig, PeftModel  \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  \n",
    "\n",
    "#path/to/your/model/or/name/on/hub  \n",
    "base_model_name = \"kashif/stack-llama-2\"  \n",
    "adapter_model_name = \"path/to/my/adapter\"  \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)  \n",
    "model = PeftModel.from_pretrained(model, adapter_model_name)  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)  \n",
    "\n",
    "You can also merge the adapters into the base model so you can use the model like a normal transformers model, however the checkpoint will be significantly bigger:  \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)  \n",
    "model = PeftModel.from_pretrained(model, adapter_model_name)  \n",
    "\n",
    "model = model.merge_and_unload()  \n",
    "model.save_pretrained(\"merged_adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f85be-d3f5-4828-a0c9-685ce95ab967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
